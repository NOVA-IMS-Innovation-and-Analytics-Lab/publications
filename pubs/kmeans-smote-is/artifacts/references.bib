% This file was created with Citavi 5.7.0.0

@proceedings{.1988,
 year = {1988},
 title = {Neural Computers},
 keywords = {FernandoGeorgiosSources}
}


@proceedings{.1996,
 year = {1996},
 title = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 keywords = {FernandoGeorgiosSources},
 isbn = {3540615105}
}


@proceedings{.1996b,
 year = {1996},
 title = {WCNN'96},
 keywords = {FernandoGeorgiosSources}
}


@proceedings{.1998,
 year = {1998},
 title = {Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection}
}


@proceedings{.1999,
 year = {1999},
 title = {Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining},
 keywords = {FernandoGeorgiosSources}
}


@book{.2000,
 year = {2000},
 title = {Proceedings of the AAAI'2000 workshop on imbalanced data sets}
}


@proceedings{.2000b,
 year = {2000},
 title = {Learning from imbalanced data sets: A comparison of various strategies},
 publisher = {{Menlo Park, CA}}
}


@book{.2001,
 year = {2001},
 title = {Springer Series in Information Sciences},
 keywords = {FernandoGeorgiosSources},
 isbn = {9783540679219}
}


@proceedings{.2001b,
 year = {2001},
 title = {AISTATS}
}


@proceedings{.2006,
 year = {2006},
 title = {IEEE International Conference on Granular Computing, 2006},
 keywords = {Ebooks},
 publisher = {IEEE},
 isbn = {1-4244-0134-8}
}


@proceedings{.2008,
 year = {2008},
 title = {Proceedings - IEEE International Conference on Data Mining, ICDM},
 keywords = {FernandoGeorgiosSources},
 isbn = {9780769535029}
}


@proceedings{.2009,
 year = {2009},
 title = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 keywords = {FernandoGeorgiosSources},
 isbn = {3642013066}
}


@proceedings{.2010,
 year = {2010},
 title = {Proceedings of the 19th international conference on World wide web},
 address = {1772690},
 publisher = {ACM},
 isbn = {978-1-60558-799-8}
}


@proceedings{ACM.1999,
 year = {1999},
 title = {Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
 institution = {ACM}
}


@article{Aggarwal.2001,
 abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
 author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
 year = {2001},
 title = {On the surprising behavior of distance metrics in high dimensional space},
 keywords = {FernandoGeorgiosSources},
 pages = {420--434},
 issn = {0956-7925},
 journal = {Database Theory -- ICDT 2001},
 doi = {10.1007/3-540-44503-X_27}
}


@article{Akbani.2004,
 abstract = {Support Vector Machines (SVM) have been extensively studied and have shown remarkable success in many applications. However the success of SVM is very limited when it is applied to the problem of learning from imbal- anced datasets in which negative instances heavily outnumber the positive in- stances (e.g. in gene profiling and detecting credit card fraud). This paper dis- cusses the factors behind this failure and explains why the common strategy of undersampling the training data may not be the best choice for SVM. We then propose an algorithm for overcoming these problems which is based on a vari- ant of the SMOTE algorithm by Chawla et al, combined with Veropoulos et als different error costs algorithm. We compare the performance of our algorithm against these two algorithms, along with undersampling and regular SVM and show that our algorithm outperforms all of them.},
 author = {Akbani, Rehan and Kwek, Stephen and Japkowicz, Nathalie},
 year = {2004},
 title = {Applying Support Vector Machine to Imbalanced Datasets},
 keywords = {FernandoGeorgiosSources},
 pages = {39--50},
 journal = {Machine Learning: ECML 2004}
}


@article{Alcala-fdez.2011,
	title = {Keel data-mining software tool: data set repository, integration of algorithms and experimental analysis framework},
	volume = {17},
	journal = {Journal of Multiple-Valued Logic \& Soft Computing},
	author = {Alcalá-Fdez, Jesús and Fernández, Alberto and Luengo, Julián and Derrac, Joaquín and García, Salvador and Sánchez, Luciano and Herrera, Francisco},
	year = {2011}
}


@article{Babar.2015,
 author = {Babar, Varsha S. and Ade, Roshani},
 year = {2015},
 title = {Article: A Review on Imbalanced Learning Methods},
 pages = {23--27},
 volume = {NCAC 2015},
 number = {2},
 journal = {IJCA Proceedings on National Conference on Advances in Computing}
}


@article{Barua.2014,
 abstract = {Imbalanced learning problems contain an unequal distribution of data samples among different classes and pose a challenge to any classifier as it becomes hard to learn the minority class samples. Synthetic oversampling methods address this problem by generating the synthetic minority class samples to balance the distribution between the samples of the majority and minority classes. This paper identifies that most of the existing oversampling methods may generate the wrong synthetic minority samples in some scenarios and make learning tasks harder. To this end, a new method, called Majority Weighted Minority Oversampling TEchnique (MWMOTE), is presented for efficiently handling imbalanced learning problems.MWMOTEfirst identifies the hard-to-learn informative minority class samples and assigns them weights according to their euclidean distance from the nearest majority class samples. It then generates the synthetic samples from the weighted informative minority class samples using a clustering approach. This is done in such a way that all the generated samples lie inside some minority class cluster. MWMOTE has been evaluated extensively on four artificial and 20 real-world data sets. The simulation results show that our method is better than or comparable with some other existing methods in terms of various assessment metrics, such as geometric mean (G-mean) and area under the receiver operating curve (ROC), usually known as area under curve (AUC).},
 author = {Barua, Sukarna and Islam, Md Monirul and Yao, Xin and Murase, Kazuyuki},
 year = {2014},
 title = {MWMOTE - Majority weighted minority oversampling technique for imbalanced data set learning},
 keywords = {Class Imbalanced Problem;clustering;FernandoGeorgiosSources;K-Means SMOTE;oversampling;synthetic sample generation;undersampling},
 pages = {405--425},
 volume = {26},
 number = {2},
 issn = {10414347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 doi = {10.1109/TKDE.2012.232}
}


@article{Batista.2004,
 abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
 author = {Batista, Gustavo E. A. P. A. and Prati, Ronaldo C. and Monard, Maria Carolina},
 year = {2004},
 title = {A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data},
 keywords = {FernandoGeorgiosSources},
 pages = {20--29},
 volume = {6},
 number = {1},
 issn = {1931-0145},
 journal = {ACM SIGKDD Explorations Newsletter},
 doi = {10.1145/1007730.1007735}
}


@article{Bellman.1961,
 abstract = {Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.},
 author = {Bellman, R. E.},
 year = {1961},
 title = {Adaptive control processes: A guided tour},
 keywords = {FernandoGeorgiosSources},
 pages = {1--19},
 volume = {28},
 journal = {Princeton University Press}
}


@article{Bengio.2015,
 abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens givensome input, as exemplified by recent results in machine translation and imagecaptioning. The current approach to training them consists of maximizing thelikelihood of each token in the sequence given the current (recurrent) stateand the previous token. At inference, the unknown previous token is thenreplaced by a token generated by the model itself. This discrepancy betweentraining and inference can yield errors that can accumulate quickly along thegenerated sequence. We propose a curriculum learning strategy to gently changethe training process from a fully guided scheme using the true previous token,towards a less guided scheme which mostly uses the generated token instead.Experiments on several sequence prediction tasks show that this approach yieldssignificant improvements. Moreover, it was used successfully in our winningentry to the MSCOCO image captioning challenge, 2015.},
 author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
 year = {2015},
 title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
 keywords = {kdnuggets},
 journal = {arXiv}
}


@article{Beyer.1999,
 abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions. These results should not be interpreted to mean that high-dimensional indexing is never meaningful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10--15) dimensionality!},
 author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
 year = {1999},
 title = {When is ``nearest neighbor'' meaningful?},
 keywords = {FernandoGeorgiosSources},
 pages = {217--235},
 issn = {3540654526},
 journal = {Database Theory---ICDT'99},
 doi = {10.1007/3-540-49257-7_15}
}


@book{Bishop.2006,
 author = {Bishop, C. M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 doi = {10.1016/c2009-0-22409-3}
}


@article{Brahma.2016,
 abstract = {Deep hierarchical representations of the data have been found out to provide better informative features for several machine learning applications. In addition, multilayer neural networks surprisingly tend to achieve better performance when they are subject to an unsupervised pretraining. The booming of deep learning motivates researchers to identify the factors that contribute to its success. One possible reason identified is the flattening of manifold-shaped data in higher layers of neural networks. However, it is not clear how to measure the flattening of such manifold-shaped data and what amount of flattening a deep neural network can achieve. For the first time, this paper provides quantitative evidence to validate the flattening hypothesis. To achieve this, we propose a few quantities for measuring manifold entanglement under certain assumptions and conduct experiments with both synthetic and real-world data. Our experimental results validate the proposition and lead to new insights on deep learning.},
 author = {Brahma, P. P. and Wu, D. and She, Y.},
 year = {2016},
 title = {Why Deep Learning Works: A Manifold Disentanglement Perspective},
 pages = {1997--2008},
 volume = {27},
 number = {10},
 issn = {2162-237X},
 journal = {IEEE transactions on neural networks and learning systems},
 doi = {10.1109/TNNLS.2015.2496947}
}


@inproceedings{Bunkhumpornpat.2009,
 abstract = {The class imbalanced problem occurs in various disciplines when one of target classes has a tiny number of instances comparing to other classes. A typical classifier normally ignores or neglects to detect a minority class due to the small number of class instances. SMOTE is one of over-sampling techniques that remedies this situation. It generates minority instances within the overlapping regions. However, SMOTE randomly synthesizes the minority instances along a line joining a minority instance and its selected nearest neighbours, ignoring nearby majority instances. Our technique called Safe-Level-SMOTE carefully samples minority instances along the same line with different weight degree, called safe level. The safe level computes by using nearest neighbour minority instances. By synthesizing the minority instances more around larger safe level, we achieve a better accuracy performance than SMOTE and Borderline-SMOTE.},
 author = {Bunkhumpornpat, Chumphol and Sinapiromsaran, Krung and Lursinsap, Chidchanok},
 title = {Safe-level-SMOTE: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem},
 keywords = {Class Imbalanced Problem;FernandoGeorgiosSources;oversampling;Safe Level;SMOTE},
 pages = {475--482},
 volume = {5476 LNAI},
 isbn = {3642013066},
 booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 year = {2009},
 doi = {10.1007/978-3-642-01307-2_43}
}


@article{Bunkhumpornpat.2012,
 abstract = {A dataset exhibits the class imbalance problem when a target class has a very small number of instances relative to other classes. A trivial classifier typically fails to detect a minority class due to its extremely low incidence rate. In this paper, a new over-sampling technique called DBSMOTE is proposed. Our technique relies on a density-based notion of clusters and is designed to over-sample an arbitrarily shaped cluster discovered by DB-SCAN. DBSMOTE generates synthetic instances along a shortest path from each positive instance to a pseudo-centroid of a minority-class cluster. Consequently, these synthetic instances are dense near this centroid and are sparse far from this centroid. Our experimental results show that DBSMOTE improves precision, F-value, and AUC more effectively than SMOTE, Borderline-SMOTE, and Safe-Level-SMOTE for imbalanced datasets. $\backslash$textcopyright 2011 Springer-Verlag.},
 author = {Bunkhumpornpat, Chumphol and Sinapiromsaran, Krung and Lursinsap, Chidchanok},
 year = {2012},
 title = {DBSMOTE: Density-based synthetic minority over-sampling technique},
 keywords = {Class Imbalanced Problem;Classification;Density-based;FernandoGeorgiosSources;oversampling},
 pages = {664--684},
 volume = {36},
 number = {3},
 issn = {0924669X},
 journal = {Applied Intelligence},
 doi = {10.1007/s10489-011-0287-y}
}


@article{Cai.2014,
 abstract = {In this paper, a hybrid learning model of imbalanced evolving self-organizing maps (IESOMs) is proposed to address the imbalanced learning problems. In our approach, we propose to modify the classic SOM learning rule to search the winner neuron based on energy function by minimally reducing local error in the competitive learning phase. The advantage of IESOM is that it can improve the classification performance through obtaining useful knowledge from the limited and underrepresented minority class data. The positive and negative SOMs are employed to train the minority and majority class, respectively. Based on the original minority class, the positive SOM evolves into a new stage that might discover novel knowledge. The purpose of convergent evolution is to recurrently search the fitness value via minimal mean quantization error in the feature space, which can motivate the offspring individuals to move toward the center of positive SOM so as to form more explicit boundary. The iterative learning procedure is used to adaptively update the incremental feature maps and create more minority instances to facilitate learning from imbalanced data. The effectiveness of the proposed algorithm is compared with several existing methods under various assessment metrics.},
 author = {Cai, Qiao and He, Haibo and Man, Hong},
 year = {2014},
 title = {Imbalanced evolving self-organizing learning},
 keywords = {Genetic Algorithm;Imbalanced learning;Self-organizing map},
 pages = {258--270},
 volume = {133},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2013.11.010}
}


@article{Carvalho.2004,
 author = {Carvalho, Deborah R. and Freitas, Alex A.},
 year = {2004},
 title = {A hybrid decision tree/genetic algorithm method for data mining},
 keywords = {Decision trees},
 pages = {13--35},
 volume = {163},
 number = {1},
 journal = {Information Sciences}
}


@article{Chawla.2002,
 abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep- resented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting)example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor- mal)class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal)class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)than only under-sampling the majority class. This paper also shows that a combination of ourmethod of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy},
 author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
 year = {2002},
 title = {SMOTE: Synthetic minority over-sampling technique},
 keywords = {Class Imbalanced Problem;FernandoGeorgiosSources},
 pages = {321--357},
 volume = {16},
 issn = {10769757},
 journal = {Journal of Artificial Intelligence Research},
 doi = {10.1613/jair.953}
}


@article{Chawla.2004,
 abstract = {methods for bal- ancing machine training . digital text categorization from docu- ments Toward scalable with non-uniform class},
 author = {Chawla, Nitesh V. and Japkowicz, Nathalie and Drive, Prentice},
 year = {2004},
 title = {Editorial: Special Issue on Learning from Imbalanced Data Sets},
 keywords = {FernandoGeorgiosSources},
 pages = {1--6},
 volume = {6},
 number = {1},
 issn = {1931-0145},
 journal = {ACM SIGKDD Explorations Newsletter},
 doi = {10.1145/1007730.1007733}
}


@incollection{Chawla.2005,
 author = {Chawla, Nitesh V.},
 title = {Data Mining for Imbalanced Datasets: An Overview},
 keywords = {FernandoGeorgiosSources},
 pages = {853--867},
 publisher = {Springer},
 isbn = {978-0-387-25465-4},
 editor = {Maimon, Oded Z. and Rokach, Lior},
 booktitle = {Data mining and knowledge discovery handbook},
 year = {2005},
 address = {Ramat-Aviv and Great Britain},
 doi = {10.1007/0-387-25465-X_40}
}


@article{Chawla.2008,
	title = {Automatically countering imbalance and its empirical relationship to cost},
	volume = {17},
	issn = {1384-5810, 1573-756X},
	doi = {10.1007/s10618-008-0087-0},
	language = {en},
	number = {2},
	urldate = {2017-12-09},
	journal = {Data Mining and Knowledge Discovery},
	author = {Chawla, Nitesh V. and Cieslak, David A. and Hall, Lawrence O. and Joshi, Ajay},
	month = oct,
	year = {2008},
	pages = {225--252}
}


@inproceedings{Cieslak.2006,
 author = {Cieslak, D. A. and Chawla, N. V. and Striegel, A.},
 title = {Combating imbalance in network intrusion datasets},
 keywords = {K-Means SMOTE;SMOTE},
 pages = {732--737},
 publisher = {IEEE},
 isbn = {1-4244-0134-8},
 booktitle = {IEEE International Conference on Granular Computing, 2006},
 year = {2006},
 doi = {10.1109/GRC.2006.1635905}
}


@inproceedings{Cieslak.2008,
 abstract = {Class imbalance is a ubiquitous problem in supervised learning and has gained wide-scale attention in the literature. Perhaps the most prevalent solution is to apply sampling to training data in order improve classifier performance. The typical approach will apply uniform levels of sampling globally. However, we believe that data is typically multi-modal, which suggests sampling should be treated locally rather than globally. It is the purpose of this paper to propose a framework which first identifies meaningful regions of data and then proceeds to find optimal sampling levels within each. This paper demonstrates that a global classifier trained on data locally sampled produces superior rank-orderings on a wide range of real-world and artificial datasets as compared to contemporary global sampling methods.},
 author = {Cieslak, David A. and Chawla, Nitesh V.},
 title = {Start globally, optimize locally, predict globally: Improving performance on imbalanced data},
 keywords = {FernandoGeorgiosSources},
 pages = {143--152},
 isbn = {9780769535029},
 booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
 year = {2008},
 doi = {10.1109/ICDM.2008.87}
}


@article{Cieslak.2012,
	title = {Hellinger distance decision trees are robust and skew-insensitive},
	volume = {24},
	number = {1},
	journal = {Data Mining and Knowledge Discovery},
	author = {Cieslak, David A and Hoens, T Ryan and Chawla, Nitesh V and Kegelmeyer, W Philip},
	year = {2012},
	pages = {136--158},
	doi = {10.1007/s10618-011-0222-1}
}


@proceedings{Citeseer.1989,
 year = {1989},
 title = {IJCAI},
 institution = {Citeseer}
}


@article{Clearwater.1991,
 abstract = {We have applied a rule-learning program to the problem of event classification in high energy physics. The program searches for event classifications, i.e. rules, and effectively allows an exploration of many more possible classifications than is practical by a physicist. The program, RL4, is particularly useful because it can easily explore multi-dimensional rules as well as rules that may seem non-intuitive at first to the physicist. RL4 is also contrasted with other learning programs. ?? 1991.},
 author = {Clearwater, S. H. and Stern, E. G.},
 year = {1991},
 title = {A rule-learning program in high energy physics event classification},
 keywords = {FernandoGeorgiosSources},
 pages = {159--182},
 volume = {67},
 number = {2},
 issn = {00104655},
 journal = {Computer Physics Communications},
 doi = {10.1016/0010-4655(91)90014-C}
}


@proceedings{Cohen.2006,
 year = {2006},
 title = {Proceedings of the 23rd international conference on machine learning: ICML 2006 / Edited by William W. Cohen and Andrew Moore},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {1595933832},
 editor = {Cohen, William W. and Moore, Andrew W.}
}


@article{Cottrell.1998,
 abstract = {The SOM algorithm is very astonishing. On the one hand, it is very simple to write down and to simulate, its practical properties are clear and easy to observe. However, on the other hand, its theoretical properties still remain without proof in the general case, despite the tremendous efforts of several authors. In this paper, we briefly review the previous results and provide some conjectures for future work.},
 author = {Cottrell, M. and Fort, J. C. and Pag{\`e}s, G.},
 year = {1998},
 title = {Theoretical aspects of the SOM algorithm},
 keywords = {Convergence of stochastic processes;FernandoGeorgiosSources;Kohonen algorithm;Self-organization;Vectorial quantization},
 pages = {119--138},
 volume = {21},
 number = {1-3},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/S0925-2312(98)00034-4}
}


@inproceedings{DalPozzolo.2015,
 author = {{Dal Pozzolo}, Andrea and Caelen, Olivier and Johnson, Reid A. and Bontempi, Gianluca},
 title = {Calibrating Probability with Undersampling for Unbalanced Classification},
 keywords = {datasets},
 pages = {159--166},
 booktitle = {IEEE Symposium Series on Computational Intelligence, 2015},
 year = {2015}
}


@inproceedings{Davis.2006,
 author = {Davis, Jesse and Goadrich, Mark},
 title = {The relationship between Precision-Recall and ROC curves},
 publisher = {ACM},
 isbn = {1595933832},
 editor = {Cohen, William W. and Moore, Andrew W.},
 booktitle = {Proceedings of the 23rd international conference on machine learning},
 year = {2006},
 address = {New York, NY, USA}
}


@article{Demsar.2006,
	title = {Statistical comparisons of classifiers over multiple data sets},
	volume = {7},
	number = {Jan},
	journal = {Journal of Machine learning research},
	author = {Demšar, Janez},
	year = {2006},
	pages = {1--30}
}

@article{Diez-pastor.2015,
	title = {Diversity techniques improve the performance of the best imbalance learning ensembles},
	volume = {325},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2015.07.025},
	abstract = {Many real-life problems can be described as unbalanced, where the number of instances belonging to one of the classes is much larger than the numbers in other classes. Examples are spam detection, credit card fraud detection or medical diagnosis. Ensembles of classifiers have acquired popularity in this kind of problems for their ability to obtain better results than individual classifiers. The most commonly used techniques by those ensembles especially designed to deal with imbalanced problems are for example Re-weighting, Oversampling and Undersampling. Other techniques, originally intended to increase the ensemble diversity, have not been systematically studied for their effect on imbalanced problems. Among these are Random Oracles, Disturbing Neighbors, Random Feature Weights or Rotation Forest. This paper presents an overview and an experimental study of various ensemble-based methods for imbalanced problems, the methods have been tested in its original form and in conjunction with several diversity-increasing techniques, using 84 imbalanced data sets from two well known repositories. This paper shows that these diversity-increasing techniques significantly improve the performance of ensemble methods for imbalanced problems and provides some ideas about when it is more convenient to use these diversifying techniques.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Díez-Pastor, José F. and Rodríguez, Juan J. and García-Osorio, César I. and Kuncheva, Ludmila I.},
	month = dec,
	year = {2015},
	keywords = {SMOTE, Classifier ensembles, Diversity, Imbalanced data sets, Rotation forest, Undersampling},
	pages = {98--117},
	file = {Díez-Pastor et al. - 2015 - Diversity techniques improve the performance of th.pdf:/Users/felix/Zotero/storage/2KQRK5X5/Díez-Pastor et al. - 2015 - Diversity techniques improve the performance of th.pdf:application/pdf;ScienceDirect Snapshot:/Users/felix/Zotero/storage/BHASGWPN/S0020025515005186.html:text/html}
}

@inproceedings{Domingos.1999,
 abstract = {Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD problems. Individually making each classification learner cost-sensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classi- fier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.},
 author = {Domingos, Pedro},
 title = {MetaCost: A General Method for Making Classifiers Cost-Sensitive},
 keywords = {Class Imbalanced Problem;FernandoGeorgiosSources},
 pages = {155--164},
 booktitle = {Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining},
 year = {1999},
 doi = {10.1145/312129.312220}
}


@article{Douzas.2017,
 author = {Douzas, Georgios and Bacao, Fernando},
 year = {2017},
 title = {Self-Organizing Map Oversampling (SOMO) for imbalanced data set learning},
 pages = {40--52},
 volume = {82},
 issn = {09574174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2017.03.073}
}


@article{Dunne.1997,
 abstract = {Abstract It is suggested in the literature 2, 1] that there is a natural pairing between the softmax activation function and the cross entropy penalty function. We clarify a reason for this pairing and give an improved derivation of the softmax activation function. In addition,  ...},
 author = {Dunne, R. A. and Campbell, N. A.},
 year = {1997},
 title = {On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function},
 journal = {Proc. 8th Aust. Conf. on the Neural Networks}
}


@article{E.Merenyi.2009,
 author = {{E. Merenyi}, K. Tasdemir and L, Zhang},
 year = {2009},
 title = {Learning Highly Structured Manifolds: Harnessing the Power of SOMs. Springer},
 keywords = {FernandoGeorgiosSources},
 pages = {138--168},
 journal = {Similarity-Based Clustering, LNAI 5400}
}


@article{Erren.2007,
 abstract = {Thomas C. Erren is with the Institute and Policlinic for Occupational and Social Medicine, School of Medicine and Dentistry, University of Cologne, K{\"o}ln, Lindenthal, Germany. Paul Cullen is with the Medizinisches Versorgungszentrum f{\"u}r Laboratoriumsmedizin Dr. L{\"o}er, Dr.Treder, ...},
 author = {Erren, Thomas C.},
 year = {2007},
 title = {Ten simple rules for doing your best research, according to Hamming},
 keywords = {meta},
 pages = {1839},
 volume = {3},
 number = {10},
 journal = {PLoS computational biology},
 doi = {10.1371/journal.pcbi.0030213}
}


@article{Estabrooks.2004,
 author = {Estabrooks, Andrew and Jo, Taeho and Japkowicz, Nathalie},
 year = {2004},
 title = {A multiple resampling method for learning from imbalanced data sets},
 pages = {18--36},
 volume = {20},
 number = {1},
 issn = {1467-8640},
 journal = {Computational intelligence},
 doi = {10.1111/j.0824-7935.2004.t01-1-00228.x}
}


@article{Fernandez.2013,
 abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository. ?? 2013 Elsevier B.V. All rights reserved.},
 author = {Fern{\'a}ndez, Alberto and L{\'o}pez, Victoria and Galar, Mikel and {Del Jesus}, Mar{\'i}a Jos{\'e} and Herrera, Francisco},
 year = {2013},
 title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
 keywords = {Class Imbalanced Problem;Cost-sensitive learning;FernandoGeorgiosSources;Multi-classification;Pairwise learning;Preprocessing},
 pages = {97--110},
 volume = {42},
 issn = {09507051},
 journal = {Knowledge-Based Systems},
 doi = {10.1016/j.knosys.2013.01.018}
}


@misc{Fix.1951,
 author = {Fix, Evelyn and {Hodges Jr.}, Joseph},
 year = {1951},
 title = {Discriminatory analysis - nonparametric discrimination: Consistency properties},
 number = {ADA800276}
}


@misc{FjodorvanVeen.2016,
 author = {{Fjodor van Veen}},
 year = {2016},
 title = {The Neural~Network Zoo},
 urldate = {19.10.2016}
}


@article{Friedman.1937,
 author = {Friedman, Milton},
 year = {1937},
 title = {The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance},
 pages = {675},
 volume = {32},
 number = {200},
 issn = {01621459},
 journal = {Journal of the American Statistical Association},
 doi = {10.2307/2279372}
}


@article{Friedman.2001,
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent {\dq}boosting{\dq} paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such {\dq}TreeBoost{\dq} models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for ruining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Friedman, Jerome H.},
 year = {2001},
 title = {Greedy function approximation: A gradient boosting machine},
 keywords = {Boosting;Decision trees;FernandoGeorgiosSources;Function estimation;Robust nonparametric regression},
 pages = {1189--1232},
 volume = {29},
 number = {5},
 issn = {00905364},
 journal = {Annals of Statistics},
 doi = {10.1214/aos/1013203451}
}


@misc{Galar.2012,
 abstract = {Classifier learningwith data-sets that suffer fromim- balanced class distributions is a challenging problem in data min- ing community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention fromresearchers. Inmachine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxon- omy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemblemethodology in which it is based. In addition,we develop a thorough empirical comparison by the consideration of themost significant published approaches, within the families of the taxon- omy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest ap- proaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Further- more, our results showempirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocess- ing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of},
 author = {Galar, Mikel and Fernandez, Alberto and Barrenechea, Edurne and Bustince, Humberto and Herrera, Francisco},
 year = {2012},
 title = {A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches},
 keywords = {Bagging;Boosting;Class Imbalanced Problem;Classification;ensembles;FernandoGeorgiosSources;multiple classifier systems},
 volume = {42},
 number = {4},
 isbn = {1094-6977},
 doi = {10.1109/TSMCC.2011.2161285}
}


@article{Galar.2016,
	title = {Ordering-based pruning for improving the performance of ensembles of classifiers in the framework of imbalanced datasets},
	volume = {354},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2016.02.056},
	abstract = {The scenario of classification with imbalanced datasets has gained a notorious significance in the last years. This is due to the fact that a large number of problems where classes are highly skewed may be found, affecting the global performance of the system. A great number of approaches have been developed to address this problem. These techniques have been traditionally proposed under three different perspectives: data treatment, adaptation of algorithms, and cost-sensitive learning. Ensemble-based models for classifiers are an extension over the former solutions. They consider a pool of classifiers, and they can in turn integrate any of these proposals. The quality and performance of this type of methodology over baseline solutions have been shown in several studies of the specialized literature. The goal of this work is to improve the capabilities of tree-based ensemble-based solutions that were specifically designed for imbalanced classification, focusing on the best behaving bagging- and boosting-based ensembles in this scenario. In order to do so, this paper proposes several new metrics for ordering-based pruning, which are properly adapted to address the skewed-class distribution. From our experimental study we show two main results: on the one hand, the use of the new metrics allows pruning to become a very successful approach in this scenario; on the other hand, the behavior of Under-Bagging model excels, achieving the highest gain with the usage of pruning, since the random undersampled sets that best complement each other can be selected. Accordingly, this scheme is capable of outperforming previous ensemble models selected from the state-of-the-art.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Galar, Mikel and Fernández, Alberto and Barrenechea, Edurne and Bustince, Humberto and Herrera, Francisco},
	month = aug,
	year = {2016},
	keywords = {Boosting, Bagging, Imbalanced datasets, Ordering-based pruning, Tree-based ensembles},
	pages = {178--196},
	file = {2016-INS-Galar-Ensemble_Pruning_imbalanced.pdf:/Users/felix/Zotero/storage/UNB7NGI3/2016-INS-Galar-Ensemble_Pruning_imbalanced.pdf:application/pdf;ScienceDirect Snapshot:/Users/felix/Zotero/storage/2VKNJG5P/S0020025516301384.html:text/html}
}


@article{Golik.2013,
 abstract = {Abstract In this paper we investigate the error criteria that are optimized during the training of artificial neural networks (ANN). We compare the bounds of the squared error (SE) and the crossentropy (CE) criteria being the most popular choices in stateof-the art  ...},
 author = {Golik, P. and Doetsch, P. and Ney, H.},
 year = {2013},
 title = {Cross-entropy vs. squared error training: a theoretical and experimental comparison},
 journal = {INTERSPEECH}
}


@misc{Goodfellow.01.01.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 date = {01.01.2016},
 title = {Deep Learning},
}


@article{GorbanA..2008,
 author = {{Gorban A.}, Kegl B. Wunsch D. and A, Zinovyen},
 year = {2008},
 title = {Principal Manifolds for Data Visualization and Dimension Reduction},
 keywords = {FernandoGeorgiosSources},
 journal = {Springer}
}


@misc{Gosavi.,
 author = {Gosavi, Abhijit},
 title = {Neural Networks and Reinforcement Learning},
 institution = {{Missouri University of Science and Technology}}
}


@article{Greff.2015,
 abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random searchand their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs (about 15 years ofCPU time), which makes our study the largest of its kind on LSTM networks. Ourresults show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.},
 author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn$\backslash$'$\backslash$ik, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
 year = {2015},
 title = {LSTM: A Search Space Odyssey},
 keywords = {kdnuggets},
 journal = {arXiv}
}


@misc{Guyon.2003,
 author = {Guyon, Isabelle},
 year = {2003},
 title = {Design of experiments of the {NIPS} 2003 variable selection benchmark}
}


@article{Haixiang.2016,
 author = {Haixiang, Guo and Yijing, Li and Shang, Jennifer and Mingyun, Gu and Yuanyue, Huang and Bing, Gong},
 year = {2016},
 title = {Learning from class-imbalanced data: Review of methods and applications},
 keywords = {Imbalanced learning},
 issn = {09574174},
 journal = {Expert Systems with Applications}
}


@article{Hall.2014,
 abstract = {In the era of social media there are now many different ways that a scientist can build their public profile; the publication of high-quality scientific papers being just one. While social media is a valuable tool for outreach and the sharing of ideas, there is a danger that this form of communication is gaining too high a value and that we are losing sight of key metrics of scientific value, such as citation indices. To help quantify this, I propose the 'Kardashian Index', a measure of discrepancy between a scientist's social media profile and publication record based on the direct comparison of numbers of citations and Twitter followers.},
 author = {Hall, Neil},
 year = {2014},
 title = {The Kardashian index: a measure of discrepant social media profile for scientists},
 pages = {424},
 volume = {15},
 number = {7},
 journal = {Genome biology},
 doi = {10.1186/s13059-014-0424-0}
}


@article{Han.2005,
 abstract = {In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique (SMOTE) is one of the over-sampling methods addressing this problem. Based on SMOTE method, this paper presents two new minority over-sampling methods, borderline-SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods.},
 author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
 year = {2005},
 title = {Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning},
 keywords = {Class Imbalanced Problem;Classification;FernandoGeorgiosSources;High-dimensional data;Multidimensional projection;oversampling;SMOTE;Visual data mining},
 pages = {878--887},
 volume = {17},
 number = {12},
 issn = {1941-0506},
 journal = {Advances in intelligent computing},
 doi = {10.1007/11538059_91}
}


@article{Hart.1968,
 author = {Hart, P.},
 year = {1968},
 title = {The condensed nearest neighbor rule},
 keywords = {FernandoGeorgiosSources},
 pages = {515--516},
 volume = {14},
 journal = {IEEE Trans. Inform. Theory}
}


@article{Hawkins.2004,
 author = {Hawkins, Douglas M.},
 year = {2004},
 title = {The problem of overfitting},
 pages = {1--12},
 volume = {44},
 number = {1},
 journal = {Journal of chemical information and computer sciences},
 doi = {10.1002/chin.200419274}
}


@article{He.2008,
 author = {He, H. Bai Y. Garcia E. {\&}. Li S.},
 year = {2008},
 title = {ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In IEEE International Joint Conference on Neural Networks, 2008},
 keywords = {FernandoGeorgiosSources},
 pages = {1322--1328},
 journal = {IJCNN 2008.(IEEE World Congress on Computational Intelligence) (pp. 1322-- 1328)}
}


@article{He.2009,
 abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
 author = {He, Haibo and Garcia, Edwardo A.},
 year = {2009},
 title = {Learning from imbalanced data},
 keywords = {Active learning;Assessment metrics;Class Imbalanced Problem;Classification;Cost-sensitive learning;FernandoGeorgiosSources;Kernel-based learning},
 pages = {1263--1284},
 volume = {21},
 number = {9},
 issn = {10414347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 doi = {10.1109/TKDE.2008.239}
}


@book{He.2013,
 year = {2013},
 title = {Imbalanced learning: Foundations, algorithms, and applications},
 publisher = {{John Wiley {\&} Sons}},
 editor = {He, Haibo and Ma, Yunqian}
}


@article{Holm.1979,
 ISSN = {03036898, 14679469},
 author = {Holm, Sture},
 journal = {Scandinavian Journal of Statistics},
 number = {2},
 pages = {65-70},
 publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
 title = {A Simple Sequentially Rejective Multiple Test Procedure},
 volume = {6},
 year = {1979}
}


@inproceedings{Holte.1989,
 author = {Holte, Robert C. and Acker, Liane and Porter, Bruce W. and others},
 title = {Concept Learning and the Problem of Small Disjuncts},
 keywords = {Small Disjuncts Problem},
 pages = {813--818},
 volume = {89},
 booktitle = {IJCAI},
 year = {1989}
}


@proceedings{IEEE.2015,
 year = {2015},
 title = {IEEE Symposium Series on Computational Intelligence, 2015},
 institution = {IEEE}
}


@proceedings{IEEE.2016,
 year = {2016},
 title = {IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), 2016},
 institution = {IEEE}
}


@article{Japkowicz.2000,
 author = {Japkowicz, N.},
 year = {2000},
 title = {Learning from Imbalanced Data Sets. Proc. Am. Assoc},
 keywords = {FernandoGeorgiosSources},
 journal = {Artificial Intelligence (AAAI) Workshop}
}


@article{Japkowicz.2002,
 author = {Japkowicz, Nathalie and Stephen, Shaju},
 year = {2002},
 title = {The class imbalance problem: A systematic study},
 keywords = {C5.0;class imbalances;concept learning;misclassification costs;Multi-Layer Perceptrons;re-sampling;Support Vector Machines},
 pages = {429--449},
 volume = {6},
 number = {5},
 issn = {1088-467X},
 journal = {Intelligent Data Analysis}
}


@incollection{Japkowicz.2013,
 author = {Japkowicz, Nathalie},
 title = {Assessment Metrics for Imbalanced Learning},
 pages = {187--206},
 publisher = {{John Wiley {\&} Sons}},
 editor = {He, Haibo and Ma, Yunqian},
 booktitle = {Imbalanced learning},
 year = {2013},
 doi = {10.1002/9781118646106.ch8}
}


@article{Jo.2004,
 author = {Jo, Taeho and Japkowicz, Nathalie},
 year = {2004},
 title = {Class imbalances versus small disjuncts},
 keywords = {Class Imbalanced Problem;FernandoGeorgiosSources;K-Means SMOTE;related approaches;Small Disjuncts Problem},
 pages = {40--49},
 volume = {6},
 number = {1},
 issn = {1931-0145},
 journal = {ACM SIGKDD Explorations Newsletter}
}


@inproceedings{Kaski.1996,
 abstract = {In exploratory analysis of high-dimensional data the self- organizing map can be used to illustrate relations between the data items. We have developed two measures for comparing how di erent maps rep- resent these relations. The other combines an index of discontinuities in the mapping from the input data set to the map grid with an index of the accuracy with which the map represents the data set. This measure can be used for determining the goodness of single maps. The other measure has been used to directly compare how similarly two maps represent re- lations between data items. Such a measure of the dissimilarity of maps is useful, e.g., for analyzing the sensitivity of maps to variations in their inputs or in the learning process. Also the similarity of two data sets can be compared indirectly by comparing the maps that represent them.},
 author = {Kaski, Samuel and Lagus, Krista},
 title = {Comparing self-organizing maps},
 keywords = {FernandoGeorgiosSources},
 pages = {809--814},
 volume = {1112 LNCS},
 isbn = {3540615105},
 booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 year = {1996},
 doi = {10.1007/3-540-61510-5_136}
}


@article{Kaski.1998,
 author = {Kaski, S. and Kohonen, T.},
 year = {1998},
 title = {Methods for interpreting a self-organized map in data analysis},
 keywords = {FernandoGeorgiosSources},
 pages = {185--190},
 journal = {ESANN '98, 6th European Symposium on Artificial Neural Neural Networks, 22--24 April 1998, Brussels, Belgium (Brussels: D-Facto)}
}


@article{Kohonen.1982,
 abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
 author = {Kohonen, Teuvo},
 year = {1982},
 title = {Self-organized formation of topologically correct feature maps},
 keywords = {FernandoGeorgiosSources},
 pages = {59--69},
 volume = {43},
 number = {1},
 issn = {0340-1200},
 journal = {Biological Cybernetics},
 doi = {10.1007/BF00337288}
}


@incollection{Kohonen.2001,
 abstract = {The Self-Organizing Map (SOM), with its variants, is the most popular artificial neural network algorithm in the unsupervised learning category. About 4000 research articles on it have appeared in the open literature, and many industrial projects use the SOM as a tool for solving hard real-world problems. Many fields of science have adopted the SOM as a standard analytical tool: in statistics, signal processing, control theory, financial analyses, experimental physics, chemistry and medicine. The SOM solves difficult high-dimensional and nonlinear problems such as feature extraction and classification of images and acoustic patterns, adaptive control of robots, and equalization, demodulation, and error-tolerant transmission of signals in telecommunications. A new area is organization of very large document collections. Last but not least, it may be mentioned that the SOM is one of the most realistic models of the biological brain function. This new edition includes a survey of over 2000 contemporary studies to cover the newest results; case examples were provided with detailed formulae, illustrations, and tables; a new chapter on Software Tools for SOM was written, other chapters were extended or reorganized.},
 author = {Kohonen, Teuvo},
 title = {Self-Organizing Maps},
 keywords = {FernandoGeorgiosSources},
 pages = {501},
 volume = {30},
 isbn = {9783540679219},
 booktitle = {Springer Series in Information Sciences},
 year = {2001},
 doi = {10.1007/978-3-642-56927-2}
}


@article{Kotsiantis.2006,
 abstract = {Learning classifiers from imbalanced or skewed datasets is an important topic, arising very often in practice in classification problems. In such problems, almost all the instances are labelled as one class, while far fewer instances are labelled as the other class, usually the more important class. It is obvious that traditional classifiers seeking an accurate performance over a full range of instances are not suitable to deal with imbalanced learning tasks, since they tend to classify all the data into the majority class, which is usually the less important class. This paper describes various techniques for handling imbalance dataset problems. Of course, a single article cannot be a complete review of all the methods and algorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
 author = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris and Pintelas, Panayiotis},
 year = {2006},
 title = {Handling imbalanced datasets: A review},
 keywords = {Class Imbalanced Problem;FernandoGeorgiosSources},
 pages = {25--36},
 volume = {30},
 number = {1},
 issn = {14337851},
 journal = {Science},
 doi = {10.1007/978-0-387-09823-4_45}
}


@misc{Kotsiantis.2007,
 author = {Kotsiantis, S. and Pintelas, P. and Anyfantis, D. and Karagiannopoulos, M.},
 year = {2007},
 title = {Robustness of learning techniques in handling class noise in imbalanced datasets},
 keywords = {Class Imbalanced Problem},
 publisher = {{IFIP International Federation for Information Processing}},
 doi = {10.1007/978-0-387-74161-1_3}
}


@article{Kubat.1997,
 abstract = {Adding examples of the majority class to the training set can have a detrimental effect on the learner's behaviour: noisy or otherwise unreliable examples from the majority class can overwhelm the minority class. The paper discusses criteria to evaluate the utility of classifiers induced from such imblanced training sets, gives explanation of thepoor behavior of some learners under these circumstances, and suggests as a solution a simple technique called one-sided selection of examples.},
 author = {Kubat, Miroslav and Matwin, Stan},
 year = {1997},
 title = {Addressing the Curse of Imbalanced Training Sets: One Sided Selection},
 keywords = {FernandoGeorgiosSources},
 pages = {179--186},
 volume = {97},
 issn = {0717-6163},
 journal = {ICML},
 doi = {10.1007/s13398-014-0173-7.2}
}


@inproceedings{Landgrebe.2006,
 author = {Landgrebe, T.C.W. and Paclik, P. and Duin, R.P.W.},
 title = {Precision-recall operating characteristic (P-ROC) curves in imprecise environments},
 pages = {123--127},
 publisher = {{IEEE Computer Society}},
 isbn = {1051-4651},
 editor = {Tang, Y. Y. and Wang, S. P. and Lorette, G. and Yeung, D. S. and Yan, H.},
 booktitle = {18th International Conference on Pattern Recognition},
 year = {2006},
 address = {Hong Kong},
 doi = {10.1109/ICPR.2006.941}
}


@book{Le.2011,
 author = {Le, Quoc and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew},
 year = {2011},
 title = {On Optimization Methods for Deep Learning},
 urldate = {17.10.2016},
 publisher = {{Proceedings of the 28th International Conference on Machine Learning}}
}


@article{Lemaitre.2017,
	title = {Imbalanced-learn: {A} {Python} {Toolbox} to {Tackle} the {Curse} of {Imbalanced} {Datasets} in {Machine} {Learning}},
	volume = {18},
	shorttitle = {Imbalanced-learn},
	number = {17},
	urldate = {2017-10-31},
	journal = {Journal of Machine Learning Research},
	author = {Lemaître, Guillaume and Nogueira, Fernando and Aridas, Christos K.},
	year = {2017},
	pages = {1--5},
	file = {Full Text PDF:/Users/felix/Zotero/storage/XXUU6IBD/Lemaître et al. - 2017 - Imbalanced-learn A Python Toolbox to Tackle the C.pdf:application/pdf;Snapshot:/Users/felix/Zotero/storage/3SR92RSB/16-365.html:text/html}
}


@misc{Lichman.2013,
 author = {Lichman, M.},
 year = {2013},
 title = {UCI Machine Learning Repository},
 keywords = {datasets},
 institution = {{University of California, Irvine, School of Information and Computer Sciences{\dq}}}
}


@article{Lin.2017,
	title = {Clustering-based undersampling in class-imbalanced data},
	volume = {409-410},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2017.05.008},
	abstract = {Class imbalance is often a problem in various real-world data sets, where one class (i.e. the minority class) contains a small number of data points and the other (i.e. the majority class) contains a large number of data points. It is notably difficult to develop an effective model using current data mining and machine learning algorithms without considering data preprocessing to balance the imbalanced data sets. Random undersampling and oversampling have been used in numerous studies to ensure that the different classes contain the same number of data points. A classifier ensemble (i.e. a structure containing several classifiers) can be trained on several different balanced data sets for later classification purposes. In this paper, we introduce two undersampling strategies in which a clustering technique is used during the data preprocessing step. Specifically, the number of clusters in the majority class is set to be equal to the number of data points in the minority class. The first strategy uses the cluster centers to represent the majority class, whereas the second strategy uses the nearest neighbors of the cluster centers. A further study was conducted to examine the effect on performance of the addition or deletion of 5 to 10 cluster centers in the majority class. The experimental results obtained using 44 small-scale and 2 large-scale data sets revealed that the clustering-based undersampling approach with the second strategy outperformed five state-of-the-art approaches. Specifically, this approach combined with a single multilayer perceptron classifier and C4.5 decision tree classifier ensembles delivered optimal performance over both small- and large-scale data sets.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Lin, Wei-Chao and Tsai, Chih-Fong and Hu, Ya-Han and Jhang, Jing-Shang},
	month = oct,
	year = {2017},
	keywords = {Class imbalance, Classifier ensembles, Clustering, Imbalanced data, Machine learning},
	pages = {17--26},
	file = {lin2017.pdf:/Users/felix/Zotero/storage/6N848BQH/lin2017.pdf:application/pdf;ScienceDirect Snapshot:/Users/felix/Zotero/storage/I7R99QA6/S0020025517307235.html:text/html}
}


@article{Ma.2017,
 abstract = {BACKGROUND

The random forests algorithm is a type of classifier with prominent universality, a wide application range, and robustness for avoiding overfitting. But there are still some drawbacks to random forests. Therefore, to improve the performance of random forests, this paper seeks to improve imbalanced data processing, feature selection and parameter optimization.

RESULTS

We propose the CURE-SMOTE algorithm for the imbalanced data classification problem. Experiments on imbalanced UCI data reveal that the combination of Clustering Using Representatives (CURE) enhances the original synthetic minority oversampling technique (SMOTE) algorithms effectively compared with the classification results on the original data using random sampling, Borderline-SMOTE1, safe-level SMOTE, C-SMOTE, and k-means-SMOTE. Additionally, the hybrid RF (random forests) algorithm has been proposed for feature selection and parameter optimization, which uses the minimum out of bag (OOB) data error as its objective function. Simulation results on binary and higher-dimensional data indicate that the proposed hybrid RF algorithms, hybrid genetic-random forests algorithm, hybrid particle swarm-random forests algorithm and hybrid fish swarm-random forests algorithm can achieve the minimum OOB error and show the best generalization ability.

CONCLUSION

The training set produced from the proposed CURE-SMOTE algorithm is closer to the original data distribution because it contains minimal noise. Thus, better classification results are produced from this feasible and effective algorithm. Moreover, the hybrid algorithm's F-value, G-mean, AUC and OOB scores demonstrate that they surpass the performance of the original RF algorithm. Hence, this hybrid algorithm provides a new way to perform feature selection and parameter optimization.},
 author = {Ma, Li and Fan, Suohai},
 year = {2017},
 title = {CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests},
 keywords = {K-Means SMOTE},
 pages = {169},
 volume = {18},
 number = {1},
 issn = {1471-2105},
 journal = {BMC bioinformatics},
 doi = {10.1186/s12859-017-1578-z}
}


@inproceedings{MacQueen.1967,
 author = {MacQueen, J.},
 title = {Some methods for classification and analysis of multivariate observations},
 pages = {281--297},
 volume = {1},
 booktitle = {Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
 year = {1967}
}


@book{Maimon.2005,
 year = {2005},
 title = {Data mining and knowledge discovery handbook},
 price = {No price},
 address = {Ramat-Aviv and Great Britain},
 publisher = {Springer},
 isbn = {978-0-387-25465-4},
 editor = {Maimon, Oded Z. and Rokach, Lior}
}


@article{McCullagh.1984,
 author = {McCullagh, Peter},
 year = {1984},
 title = {Generalized linear models},
 pages = {285--292},
 volume = {16},
 number = {3},
 journal = {European Journal of Operational Research},
 doi = {10.1016/0377-2217(84)90282-0}
}


@article{Merenyi.2007,
 abstract = {In this paper, we examine the scope of validity of the explicit self-organizing map (SOM) magnification control scheme of Bauer et al. (1996) on data for which the theory does not guarantee success, namely data that are n-dimensional, n {\textgreater} or =2, and whose components in the different dimensions are not statistically independent. The Bauer et al. algorithm is very attractive for the possibility of faithful representation of the probability density function (pdf) of a data manifold, or for discovery of rare events, among other properties. Since theoretically unsupported data of higher dimensionality and higher complexity would benefit most from the power of explicit magnification control, we conduct systematic simulations on {\dq}forbidden{\dq} data. For the unsupported n=2 cases that we investigate, the simulations show that even though the magnification exponent alpha achieved achieved by magnification control is not the same as the desired alpha desired, alpha achieved systematically follows alpha desired with a slowly increasing positive offset. We show that for simple synthetic higher dimensional data information, theoretically optimum pdf matching (alpha achieved = 1) can be achieved, and that negative magnification has the desired effect of improving the detectability of rare classes. In addition, we further study theoretically unsupported cases with real data.},
 author = {Meŕenyi, Erzs{\'e}bet and Jain, Abha and Villmann, Thomas},
 year = {2007},
 title = {Explicit magnification control of self-organizing maps for {\dq}Forbidden{\dq} data},
 keywords = {Data mining;FernandoGeorgiosSources;High-dimensional data;Map magnification;Self-organizing maps (SOMs)},
 pages = {786--797},
 volume = {18},
 number = {3},
 issn = {10459227},
 journal = {IEEE Transactions on Neural Networks},
 doi = {10.1109/TNN.2007.895833}
}


@misc{Mnih.20131219,
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
 year = {2013},
 title = {Playing Atari with Deep Reinforcement Learning},
 keywords = {convolution;reinforcement learning}
}


@article{Mnih.2015,
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
 year = {2015},
 title = {Human-level control through deep reinforcement learning},
 pages = {529--533},
 volume = {518},
 number = {7540},
 journal = {Nature},
 doi = {10.1038/nature14236}
}


@article{N.V.Chawla.2003,
 author = {{] N.V. Chawla}, N. Japkowicz and Kolcz, A.},
 year = {2003},
 title = {Workshop Learning from Imbalanced Data Sets II},
 keywords = {FernandoGeorgiosSources},
 journal = {Proc. Int'l Conf. Machine Learning}
}


@article{Nair.2015,
 abstract = {We present the first massively distributed architecture for deepreinforcement learning. This architecture uses four main components: parallelactors that generate new behaviour; parallel learners that are trained fromstored experience; a distributed neural network to represent the value functionor behaviour policy; and a distributed store of experience. We used ourarchitecture to implement the Deep Q-Network algorithm (DQN). Our distributedalgorithm was applied to 49 games from Atari 2600 games from the ArcadeLearning Environment, using identical hyperparameters. Our performancesurpassed non-distributed DQN in 41 of the 49 games and also reduced thewall-time required to achieve these results by an order of magnitude on mostgames.},
 author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and de Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
 year = {2015},
 title = {Massively Parallel Methods for Deep Reinforcement Learning},
 journal = {arXiv}
}


@article{Narasimhan.2015,
 abstract = {In this paper, we consider the task of learning control policies fortext-based games. In these games, all interactions in the virtual world arethrough text and the underlying state is not observed. The resulting languagebarrier makes such environments challenging for automatic game players. Weemploy a deep reinforcement learning framework to jointly learn staterepresentations and action policies using game rewards as feedback. Thisframework enables us to map text descriptions into vector representations thatcapture the semantics of the game states. We evaluate our approach on two gameworlds, comparing against baselines using bag-of-words and bag-of-bigrams forstate representations. Our algorithm outperforms the baselines on both worldsdemonstrating the importance of learning expressive representations.},
 author = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
 year = {2015},
 title = {Language Understanding for Text-based Games Using Deep Reinforcement Learning},
 journal = {arXiv}
}


@article{Nekooeimehr.2016,
 abstract = {In many applications, the dataset for classification may be highly imbalanced where most of the instances in the training set may belong to one of the classes (majority class), while only a few instances are from the other class (minority class). Conventional classifiers will strongly favor the majority class and ignore the minority instances. In this paper, we present a new oversampling method called Adaptive Semi-Unsupervised Weighted Oversampling (A-SUWO) for imbalanced binary dataset classification. The proposed method clusters the minority instances using a semi-unsupervised hierarchical clustering approach and adaptively determines the size to oversample each sub-cluster using its classification complexity and cross validation. Then, the minority instances are oversampled depending on their Euclidean distance to the majority class. A-SUWO aims to identify hard-to-learn instances by considering minority instances from each sub-cluster that are closer to the borderline. It also avoids generating synthetic minority instances that overlap with the majority class by considering the majority class in the clustering and oversampling stages. Results demonstrate that the proposed method achieves significantly better results in most datasets compared with other sampling methods.},
 author = {Nekooeimehr, Iman and Lai-Yuen, Susana K.},
 year = {2016},
 title = {Adaptive semi-unsupervised weighted oversampling ({A}-{SUWO}) for imbalanced datasets},
 keywords = {Class Imbalanced Problem;Classification;clustering;FernandoGeorgiosSources;K-Means SMOTE;oversampling},
 pages = {405--416},
 volume = {46},
 issn = {09574174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2015.10.031}
}


@inproceedings{Nickerson.2001,
 author = {Nickerson, Adam and Japkowicz, Nathalie and Milios, Evangelos E.},
 title = {Using Unsupervised Learning to Guide Resampling in Imbalanced Data Sets},
 booktitle = {AISTATS},
 year = {2001},
 pages = {261--265}
}


@book{Nielsen.2015,
 author = {Nielsen, Michael A.},
 year = {2015},
 title = {Neural Networks and Deep Learning},
 publisher = {{Determination Press}}
}


@proceedings{OaklandCAUSA.1967,
 year = {1967},
 title = {Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
 institution = {{Oakland, CA, USA}}
}


@article{Ollivier.2015,
 abstract = {We introduce the {\dq}NoBackTrack{\dq} algorithm to train the parameters of dynamicalsystems such as recurrent neural networks. This algorithm works in an online,memoryless setting, thus requiring no backpropagation through time, and isscalable, avoiding the large computational and memory cost of maintaining thefull gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search directionin parameter space. The evolution of this search direction is partly stochasticand is constructed in such a way to provide, at every time, an unbiased randomestimate of the gradient of the loss function with respect to the parameters.Because the gradient estimate is unbiased, on average over time the parameteris updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-likefilter to yield an improved algorithm. For recurrent neural networks, theresulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing thatthe stochastic approximation of the gradient introduced in the algorithm is notdetrimental to learning. In particular, the Kalman-like version of NoBackTrackis superior to backpropagation through time (BPTT) when the time span ofdependencies in the data is longer than the truncation span for BPTT.},
 author = {Ollivier, Yann and Tallec, Corentin and Charpiat, Guillaume},
 year = {2015},
 title = {Training recurrent networks online without backtracking},
 keywords = {kdnuggets},
 journal = {arXiv}
}


@article{Pautasso.2013,
 abstract = {Literature reviews are in great demand in most scientific fields. Their need stems from the ever-increasing output of scientific publications [1]. For example, compared to 1991, in 2008 three, eight, and forty times more papers were indexed in Web of Science on malaria, obesity, and biodiversity, respectively [2]. Given such mountains of papers, scientists cannot be expected to examine in detail every single new paper relevant to their interests [3]. Thus, it is both advantageous and necessary to rely on regular summaries of the recent literature. Although recognition for scientists mainly comes from primary research, timely literature reviews can lead to new synthetic insights and are often widely read [4]. For such summaries to be useful, however, they need to be compiled in a professional way [5]. When starting from scratch, reviewing the literature can require a titanic amount of work. That is why researchers who have spent their career working on a certain research issue are in a perfect position to review that literature. Some graduate schools are now offering courses in reviewing the literature, given that most research students start their project by producing an overview of what has already been done on their research issue [6]. However, it is likely that most scientists have not thought in detail about how to approach and carry out a literature review. Reviewing the literature requires the ability to juggle multiple tasks, from finding and evaluating relevant material to synthesising information from various sources, from critical thinking to paraphrasing, evaluating, and citation skills [7]. In this contribution, I share ten simple rules I learned working on about 25 literature reviews as a PhD and postdoctoral student. Ideas and insights also come from discussions with coauthors and colleagues, as well as feedback from reviewers and editors.},
 author = {Pautasso, Marco},
 year = {2013},
 title = {Ten Simple Rules for Writing a Literature Review},
 keywords = {meta},
 volume = {9},
 number = {7},
 journal = {PLoS computational biology},
 doi = {10.1371/journal.pcbi.1003149}
}


@article{Pedregosa.2011,
 author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 year = {2011},
 title = {Scikit-learn: Machine Learning in Python},
 pages = {2825--2830},
 volume = {12},
 journal = {Journal of Machine learning research}
}


@article{Peng.2015,
 author = {Peng, Baolin and Lu, Zhengdong and Li, Hang and Wong, Kam-Fai},
 year = {2015},
 title = {Towards Neural Network-based Reasoning},
 keywords = {kdnuggets},
 pages = {188},
 journal = {Neural Networks and Soft Computing},
 doi = {10.1007/978-3-7908-1902-1_25}
}


@article{Peng.2015b,
 author = {Peng, Baolin and Lu, Zhengdong and Li, Hang and Wong, Kam-Fai},
 year = {2015},
 title = {Towards Neural Network-based Reasoning},
 pages = {188},
 journal = {Neural Networks and Soft Computing},
 doi = {10.1007/978-3-7908-1902-1_25}
}


@article{Phadtare.2009,
 abstract = {Writing plays a central role in the communication of scientific ideas and is therefore a key aspect in researcher education, ultimately determining the success and long-term sustainability of their careers. Despite the growing popularity of e-learning, we are not aware of any existing study comparing on-line vs. traditional classroom-based methods for teaching scientific writing.},
 author = {Phadtare, Amruta and Bahmani, Anu and Shah, Anand and Pietrobon, Ricardo},
 year = {2009},
 title = {Scientific writing: a randomized controlled trial comparing standard and on-line instruction},
 pages = {27},
 volume = {9},
 number = {1},
 journal = {BMC Medical Education},
 doi = {10.1186/1472-6920-9-27}
}


@inproceedings{Prati.2004,
 author = {Prati, Ronaldo C. and Batista, GEAPA and Monard, Maria Carolina},
 title = {Learning with class skews and small disjuncts},
 pages = {296--306},
 booktitle = {SBIA},
 year = {2004},
 doi = {10.1007/978-3-540-28645-5_30}
}


@article{Prati.2014,
 author = {Prati, R. Batista G.E.C. and Silva, D. F.},
 year = {2014},
 title = {Class imbalance revisited: A new experimental setup to assess the performance of treatment methods},
 keywords = {FernandoGeorgiosSources},
 pages = {1--24},
 journal = {Knowledge and Information Systems}
}


@incollection{Provost.2000,
 author = {Provost, Foster},
 title = {Machine learning from imbalanced data sets 101},
 keywords = {Class Imbalanced Problem},
 booktitle = {Proceedings of the AAAI'2000 workshop on imbalanced data sets},
 volume={68},
 pages={1--3},
 year = {2000},
 publisher = {AAAI Press}
}


@article{Rasmus.2015,
 abstract = {We combine supervised learning with unsupervised learning in deep neuralnetworks. The proposed model is trained to simultaneously minimize the sum ofsupervised and unsupervised cost functions by backpropagation, avoiding theneed for layer-wise pre-training. Our work builds on the Ladder networkproposed by Valpola (2015), which we extend by combining the model withsupervision. We show that the resulting model reaches state-of-the-artperformance in semi-supervised MNIST and CIFAR-10 classification, in additionto permutation-invariant MNIST classification with all labels.},
 author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
 year = {2015},
 title = {Semi-Supervised Learning with Ladder Networks},
 keywords = {kdnuggets},
 journal = {arXiv}
}


@inproceedings{Ritter.1988,
 abstract = {Rapid limb movements are known to be initiated by a brief torque pulse at the joints and to proceed freely thereafter (ballistic movements). To initiate such movements with a desired starting velocity u requires knowledge of the relation between torque pulse and desired velocity of the limb. We show for a planar two-link arm model that this relationship can be learnt with the aid of a self-organizing mapping of the type proposed earlier by Kohonen. To this end we extend Kohonen's algorithm by a suitable learning rule for the individual units and show that this approach results in a significant improvement in the convergency properties of the learning rule used.},
 author = {Ritter, H. and Schulten, K.},
 title = {Extending Kohonen's Self-Organizing Mapping Algorithm to Learn Ballistic Movements},
 keywords = {FernandoGeorgiosSources},
 pages = {393--406},
 booktitle = {Neural Computers},
 year = {1988}
}


@article{Rivera.2017,
	title = {Noise {Reduction} {A} {Priori} {Synthetic} {Over}-{Sampling} for class imbalanced data sets},
	volume = {408},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2017.04.046},
	abstract = {In real world data set the underlying data distribution may be highly skewed. Building accurate classifiers for predicting group membership is made difficult because the classifier has a tendency to be biased towards the over represented or majority group as a result. This problem is referred to as a class imbalance problem. Re-sampling techniques that produce new samples by means of over-sampling aim to combat class imbalance by increasing the number of members that belong to the minority group. This paper introduces a new over-sampling technique that focuses on noise reduction and selective sampling of the minority group which results in improvement for prediction of minority group membership. Experiments are conducted across a wide range of data sets, learners and over sampling methods. The results for this new method show improvement for Sensitivity and Gmean measures over the compared approaches.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Rivera, William A.},
	month = oct,
	year = {2017},
	keywords = {Classification, SMOTE, Class imbalance, NRAS, OUPS},
	pages = {146--161},
	file = {rivera2017.pdf:/Users/felix/Zotero/storage/ILHADWLI/rivera2017.pdf:application/pdf;ScienceDirect Snapshot:/Users/felix/Zotero/storage/WR46IIF4/S0020025517307089.html:text/html}
}


@article{Rosenblatt.1958,
 author = {Rosenblatt, Frank},
 year = {1958},
 title = {The perceptron: a probabilistic model for information storage and organization in the brain},
 pages = {386},
 volume = {65},
 number = {6},
 issn = {1939-1471},
 journal = {Psychological review}
}


@article{Saez.2015,
	title = {{SMOTE}–{IPF}: {Addressing} the noisy and borderline examples problem in imbalanced classification by a re-sampling method with filtering},
	volume = {291},
	issn = {0020-0255},
	shorttitle = {{SMOTE}–{IPF}},
	doi = {10.1016/j.ins.2014.08.051},
	abstract = {Classification datasets often have an unequal class distribution among their examples. This problem is known as imbalanced classification. The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most well-know data pre-processing methods to cope with it and to balance the different number of examples of each class. However, as recent works claim, class imbalance is not a problem in itself and performance degradation is also associated with other factors related to the distribution of the data. One of these is the presence of noisy and borderline examples, the latter lying in the areas surrounding class boundaries. Certain intrinsic limitations of SMOTE can aggravate the problem produced by these types of examples and current generalizations of SMOTE are not correctly adapted to their treatment. This paper proposes the extension of SMOTE through a new element, an iterative ensemble-based noise filter called Iterative-Partitioning Filter (IPF), which can overcome the problems produced by noisy and borderline examples in imbalanced datasets. This extension results in SMOTE–IPF. The properties of this proposal are discussed in a comprehensive experimental study. It is compared against a basic SMOTE and its most well-known generalizations. The experiments are carried out both on a set of synthetic datasets with different levels of noise and shapes of borderline examples as well as real-world datasets. Furthermore, the impact of introducing additional different types and levels of noise into these real-world data is studied. The results show that the new proposal performs better than existing SMOTE generalizations for all these different scenarios. The analysis of these results also helps to identify the characteristics of IPF which differentiate it from other filtering approaches.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Sáez, José A. and Luengo, Julián and Stefanowski, Jerzy and Herrera, Francisco},
	month = jan,
	year = {2015},
	keywords = {SMOTE, Borderline examples, Imbalanced classification, Noise filters, Noisy data},
	pages = {184--203},
	file = {Sáez et al. - 2015 - SMOTE–IPF Addressing the noisy and borderline exa.pdf:/Users/felix/Zotero/storage/ZL5M7WIM/Sáez et al. - 2015 - SMOTE–IPF Addressing the noisy and borderline exa.pdf:application/pdf;ScienceDirect Snapshot:/Users/felix/Zotero/storage/FIG3GT2X/S0020025514008561.html:text/html}
}


@article{Santos.2015,
 abstract = {Liver cancer is the sixth most frequently diagnosed cancer and, particularly, Hepatocellular Carcinoma (HCC) represents more than 90{\%} of primary liver cancers. Clinicians assess each patient's treatment on the basis of evidence-based medicine, which may not always apply to a specific patient, given the biological variability among individuals. Over the years, and for the particular case of Hepatocellular Carcinoma, some research studies have been developing strategies for assisting clinicians in decision making, using computational methods (e.g. machine learning techniques) to extract knowledge from the clinical data. However, these studies have some limitations that have not yet been addressed: some do not focus entirely on Hepatocellular Carcinoma patients, others have strict application boundaries, and none considers the heterogeneity between patients nor the presence of missing data, a common drawback in healthcare contexts. In this work, a real complex Hepatocellular Carcinoma database composed of heterogeneous clinical features is studied. We propose a new cluster-based oversampling approach robust to small and imbalanced datasets, which accounts for the heterogeneity of patients with Hepatocellular Carcinoma. The preprocessing procedures of this work are based on data imputation considering appropriate distance metrics for both heterogeneous and missing data (HEOM) and clustering studies to assess the underlying patient groups in the studied dataset (K-means). The final approach is applied in order to diminish the impact of underlying patient profiles with reduced sizes on survival prediction. It is based on K-means clustering and the SMOTE algorithm to build a representative dataset and use it as training example for different machine learning procedures (logistic regression and neural networks). The results are evaluated in terms of survival prediction and compared across baseline approaches that do not consider clustering and/or oversampling using the Friedman rank test. Our proposed methodology coupled with neural networks outperformed all others, suggesting an improvement over the classical approaches currently used in Hepatocellular Carcinoma prediction models.},
 author = {Santos, Miriam Seoane and Abreu, Pedro Henriques and Garc{\'i}a-Laencina, Pedro J. and Sim{\~a}o, Ad{\'e}lia and Carvalho, Armando},
 year = {2015},
 title = {A new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients},
 keywords = {K-Means SMOTE},
 pages = {49--59},
 volume = {58},
 issn = {1532-0480},
 journal = {Journal of biomedical informatics},
 doi = {10.1016/j.jbi.2015.09.012}
}


@inproceedings{Sarajedini.1996,
 abstract = {Several probability density estimator neural networks have been developed recently. However, these have limited pratical application because of their computational complexity. In pratical circumstances, one often requires the probability of a certain random variable being in a certain interval or less than (or greater than) a particular value.With density estimators, this requires integration over the input space which is often done numerically thus requiring a large number of computations. Neural network density estimators also suffer from the computational complexity of numerical integration during training. Although Monte Carlo or quasi-Monte Carlo techniques may be used to reduce the amount of computation in multidimensional integrals, there remains a large computational burden nevertheless. We propose a cumulative distribution estimating neural network. This avoids the need for integration in training and in application. We also present application of this neural network to some simple problems.+},
 author = {Sarajedini, By A. and Chau, P. M.},
 title = {Cumulative Distribution Estimation With Neural Networks},
 keywords = {FernandoGeorgiosSources},
 pages = {876--880},
 booktitle = {WCNN'96},
 year = {1996}
}


@article{Schmidhuber.2014,
 abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
 author = {Schmidhuber, J{\"u}rgen},
 year = {2014},
 title = {Deep learning in neural networks: An overview},
 keywords = {kdnuggets},
 pages = {85--117},
 volume = {61},
 issn = {08936080},
 journal = {Neural Networks},
 doi = {10.1016/j.neunet.2014.09.003}
}


@inproceedings{Sculley.2010,
 author = {Sculley, David},
 title = {Web-scale k-means clustering},
 pages = {1177--1178},
 publisher = {ACM},
 isbn = {978-1-60558-799-8},
 booktitle = {Proceedings of the 19th international conference on World wide web},
 year = {2010},
 address = {1772690},
 doi = {10.1145/1772690.1772862}
}

@inproceedings{Seabold.2010,
	title = {Statsmodels: {Econometric} and statistical modeling with python},
	volume = {57},
	booktitle = {Proceedings of the 9th {Python} in {Science} {Conference}},
	author = {Seabold, Skipper and Perktold, Josef},
	year = {2010},
	pages = {61}
}

@inproceedings{Song.2016,
 author = {Song, Jia and Huang, Xianglin and Qin, Sijun and Song, Qing},
 title = {A bi-directional sampling based on K-means method for imbalance text classification},
 keywords = {K-Means SMOTE},
 pages = {1--5},
 booktitle = {International Conference on Computer and Information Science (ICIS), 2016 IEEE/ACIS 15th},
 year = {2016},
 doi = {10.1109/icis.2016.7550920}
}


@proceedings{Springer.2004,
 year = {2004},
 title = {SBIA},
 institution = {Springer}
}


@article{T.2004,
 author = {T, Jo and N, Japkowicz},
 year = {2004},
 title = {Class imbalances versus small disjuncts},
 keywords = {FernandoGeorgiosSources},
 pages = {40--49},
 volume = {6},
 issn = {1931-0145},
 journal = {ACM SIGKDD Explorations Newsletter},
 doi = {10.1145/1007730.1007737}
}


@proceedings{Tang.2006,
 year = {2006},
 title = {18th International Conference on Pattern Recognition},
 address = {Hong Kong},
 volume = {4},
 publisher = {{IEEE Computer Society}},
 isbn = {1051-4651},
 editor = {Tang, Y. Y. and Wang, S. P. and Lorette, G. and Yeung, D. S. and Yan, H.},
 doi = {10.1109/ICPR.2006.5}
}


@article{Ting.2002,
 abstract = {We introduce an instance-weighting method to induce cost-sensitive trees. It is a generalization of the standard tree induction process where only the initial instance weights determine the type of tree to be induced-minimum error trees or minimum high cost error trees. We demonstrate that it can be easily adapted to an existing tree learning algorithm. Previous research provides insufficient evidence to support the idea that the greedy divide-and-conquer algorithm can effectively induce a truly cost-sensitive tree directly from the training data. We provide this empirical evidence in this paper. The algorithm incorporating the instance-weighting method is found to be better than the original algorithm in in of total misclassification costs, the number of high cost errors, and tree size two-class data sets. The instance-weighting method is simpler and more effective in implementation than a previous method based on altered priors},
 author = {Ting, Kai Ming},
 year = {2002},
 title = {An instance-weighting method to induce cost-sensitive trees},
 keywords = {Cost-sensitive;Decision trees;FernandoGeorgiosSources;Greedy divide-and-conquer algorithm;Induction;Instance weighting},
 pages = {659--665},
 volume = {14},
 number = {3},
 issn = {10414347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 doi = {10.1109/TKDE.2002.1000348}
}


@article{Tomek.1976,
 abstract = {The condensed nearest-neighbor (CNN) method chooses samples randomly. This results in a) retention of unnecessary samples and b) occasional retention of internal rather than boundary samples. Two modifications of CNN are presented which remove these dis- advantages by considering only points close to the boundary. Per- formance is illustrated by an example.},
 author = {Tomek, Ivan},
 year = {1976},
 title = {Two Modifications of CNN},
 keywords = {FernandoGeorgiosSources},
 pages = {769--772},
 issn = {0018-9472},
 journal = {IEEE Transactions on Systems, Man and Cybernetics 6},
 doi = {10.1109/TSMC.1976.4309452}
}


@article{vandenBergh.2000,
 abstract = {This paper presents a method to employ particle swarms optimizers in a cooperative configuration. This is achieved by splitting the input vector into several sub-vectors, each which is optimized cooperatively in its own swarm. The application of this technique to neural network training is investigated, with promising results. Keywords: Particle swarms, cooperative learning, optimization Computing Review Categories: G.1.6, I.2.6 1 Introduction Particle Swarm Optimizers (PSOs) have previously been used to train neural networks[6, 10] and generally met with success. The advantage of the PSO over many of the other optimization algorithms is its relative simplicity. This paper aims to improve the performance of the basic PSO by partitioning the input vector into several subvectors. Each sub-vector is then allocated its own swarm. In Section 2, a brief overview of PSOs is presented, followed by a discussion of how cooperative behavior can be implemented through a splitting technique i...},
 author = {{van den Bergh}, Frans and Engelbrecht, Andries P. and Engelbrecht, A. P.},
 year = {2000},
 title = {Cooperative Learning in Neural Networks using Particle Swarm Optimizers},
 keywords = {optimization;particle swarm},
 pages = {84--90},
 volume = {26},
 journal = {South African Computer Journal}
}


@article{Vanhoeyveld.2017,
	title = {Imbalanced classification in sparse and large behaviour datasets},
	issn = {1384-5810, 1573-756X},
	doi = {10.1007/s10618-017-0517-y},
	language = {en},
	urldate = {2017-12-07},
	journal = {Data Mining and Knowledge Discovery},
	author = {Vanhoeyveld, Jellis and Martens, David},
	month = jun,
	year = {2017},
	pages = {1--58}
}


@article{Weiss.2007,
 author = {Weiss, Gary M. and McCarthy, Kate and Zabar, Bibi},
 year = {2007},
 title = {Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs?},
 pages = {35--41},
 volume = {7},
 journal = {DMIN}
}


@article{Wilson.1972,
 abstract = {The convergence properties of a nearest neighbor rule that uses an editing procedure to reduce the number of preclassified samples and to improve the performance of the rule are developed. Editing of the preclassified samples using the three-nearest neighbor rule followed by classification using the single-nearest neighbor rule with the remaining preclassified samples appears to produce a decision procedure whose risk approaches the Bayes' risk quite closely in many problems with only a few preclassified samples. The asymptotic risk of the nearest neighbor rules and the nearest neighbor rules using edited preclassified samples is calculated for several problems.},
 author = {Wilson, Dennis L.},
 year = {1972},
 title = {Asymptotic Properties of Nearest Neighbor Rules Using Edited Data},
 keywords = {FernandoGeorgiosSources},
 pages = {408--421},
 volume = {2},
 number = {3},
 issn = {21682909},
 journal = {IEEE Transactions on Systems, Man and Cybernetics},
 doi = {10.1109/TSMC.1972.4309137}
}


@article{Wu.2010,
	title = {{COG}: local decomposition for rare class analysis},
	volume = {20},
	number = {2},
	journal = {Data Mining and Knowledge Discovery},
	author = {Wu, Junjie and Xiong, Hui and Chen, Jian},
	year = {2010},
	pages = {191--220},
	doi = {10.1007/s10618-009-0146-1}
}


@article{Zhang.2007,
 abstract = {The particle swarm optimization algorithm was showed to converge rapidly during the initial stages of a global search, but around global optimum, the search process will become very slow. On the contrary, the gradient descending method can achieve faster convergent speed around global optimum, and at the same time, the convergent accuracy can be higher. So in this paper, a hybrid algorithm combining particle swarm optimization (PSO) algorithm with back-propagation (BP) algorithm, also referred to as PSO--BP algorithm, is proposed to train the weights of feedforward neural network (FNN), the hybrid algorithm can make use of not only strong global searching ability of the PSOA, but also strong local searching ability of the BP algorithm. In this paper, a novel selection strategy of the inertial weight is introduced to the PSO algorithm. In the proposed PSO--BP algorithm, we adopt a heuristic way to give a transition from particle swarm search to gradient descending search. In this paper, we also give three kind of encoding strategy of particles, and give the different problem area in which every encoding strategy is used. The experimental results show that the proposed hybrid PSO--BP algorithm is better than the Adaptive Particle swarm optimization algorithm (APSOA) and BP algorithm in convergent speed and convergent accuracy.},
 author = {Zhang, Jing-Ru and Zhang, Jun and Lok, Tat-Ming and Lyu, Michael R.},
 year = {2007},
 title = {A hybrid particle swarm optimization--back-propagation algorithm for feedforward neural network training},
 keywords = {optimization;particle swarm},
 pages = {1026--1037},
 volume = {185},
 number = {2},
 issn = {00963003},
 journal = {Applied Mathematics and Computation},
 doi = {10.1016/j.amc.2006.07.025}
}


@article{Zhu.2017,
	title = {An empirical comparison of techniques for the class imbalance problem in churn prediction},
	volume = {408},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2017.04.015},
	abstract = {Class imbalance brings significant challenges to customer churn prediction. Many solutions have been developed to address this issue. In this paper, we comprehensively compare the performance of state-of-the-art techniques to deal with class imbalance in the context of churn prediction. A recently developed expected maximum profit criterion is used as one of the main performance measures to offer more insights from the perspective of cost-benefit. The experimental results show that the applied evaluation metric has a great impact on the performance of techniques. An in-depth exploration of reaction patterns to different measures is conducted by intra-family comparison within each solution group and global comparison among the representative techniques from different groups. The results also indicate there is much space to improve solutions’ performance in terms of profit-based measure. Our study offers valuable insights for academics and professionals and it also provides a baseline to develop new methods for dealing with class imbalance in churn prediction.},
	urldate = {2018-01-03},
	journal = {Information Sciences},
	author = {Zhu, Bing and Baesens, Bart and vanden Broucke, Seppe K. L. M.},
	month = oct,
	year = {2017},
	keywords = {Class imbalance, Benchmark experiment, Churn prediction, Expected maximum profit measure},
	pages = {84--99},
	file = {ScienceDirect Snapshot:/Users/felix/Zotero/storage/4NUDXXHE/S0020025517306618.html:text/html;Zhu et al. - 2017 - An empirical comparison of techniques for the clas.pdf:/Users/felix/Zotero/storage/LFRBS3QH/Zhu et al. - 2017 - An empirical comparison of techniques for the clas.pdf:application/pdf;zhu2017.pdf:/Users/felix/Zotero/storage/T6ENE6Q7/zhu2017.pdf:application/pdf}
}
