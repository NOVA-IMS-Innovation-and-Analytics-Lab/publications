\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{G-SOMO \\ \LARGE{An Oversampling Approach based on Self-Organized Maps and Geometric SMOTE}}

\author{
	Georgios Douzas\(^{* 1, a}\), Rene Rauch\(^{1, b}\), Fernando Bacao\(^{1, c}\)
	\\
	\small{Affiliation: \(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
	\\
	\small{Email: \(^{a}\)gdouzas@novaims.unl.pt, \(^{b}\)rene.rauch@outlook.de}, \(^{c}\)bacao@novaims.unl.pt
	\\
	\small{*Corresponding Author}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Traditional supervised machine learning classifiers are challenged to learn highly skewed data distributions as they are designed to expect classes to equally contribute to the minimization of the classifiers cost function. Moreover, the classifiers design expects equal misclassification costs, causing a bias for overrepresented classes. Different strategies have been proposed to correct this issue. The modification of the data set has become a common practice since the procedure is generalizable to all classifiers. Various algorithms to rebalance the data distribution through the creation of synthetic instances were proposed in the past. In this paper, we propose a new oversampling algorithm named G-SOMO. The algorithm identifies optimal areas to create artificial data instances in an informed manner and utilizes a geometric region during the data generation process to increase their variability. Our empirical results on 69 datasets, validated with different classifiers and metrics against a benchmark of commonly used oversampling methods show that G-SOMO consistently outperforms competing oversampling methods. Additionally, the statistical significance of our results is established.
\\
\\
Keywords: Machine Learning, Classification, Imbalanced Learning, Oversampling, G-SMOTE, SOM.
\end{abstract}

\section{Introduction}

Learning from imbalanced datasets is a pervasive and challenging problem in supervised machine learning. Imbalanced datasets are common in many application domains such as fraud detection, medical diagnosis, risk management, airborne imagery, face recognition and forecasting of ozone levels and can be seen as a form of data scarcity \cite{Vong2014}, \cite{He2009}. The recent growth in the number of research papers on the topic \cite{Haixiang2017}, \cite{Fernandez2018}, suggests that it continues to be a relevant and important topic for the research community.

The imbalanced learning problem can be defined as a classification problem where there is a substantial asymmetry between the number of instances of the different classes. The dominant class is called the majority class while the other classes are called the minority classes \cite{Chawla2003}. The Imbalance Ratio (IR) is the measure commonly used to assess the imbalance severity in datasets. The IR of each minority class is defined as the ratio between the majority and the minority class number of samples. It is a frequent case to observe IR values between 100 and 100.000 \cite{Chawla2002}, \cite{Barua2014}.

Imbalanced datasets constitute a problem since standard machine learning methods induce a bias in favor of the majority class during training. This happens because the minority classes contribute less to the minimization of accuracy, the commonly used objective function. It is well known that accuracy is heavily biased towards the majority class, and additionally can be a misleading choice when assessing the performance of a classifier. For instance, assuming a dataset with $ 99\% $ majority class instances and only $ 1\% $ minority class instances, the model accuracy would still be $ 99\% $ without classifying a single minority instance correctly.

It is also important to note that most methods assume equal costs in the misclassification of minority and majority instances. However it is often the case, that the misclassification costs associated with the misclassification of minority instances (false negatives) is much higher than the misclassification of majority instances (false positives) \cite{Domingos1999}, \cite{Ting2002}. Diseases screening tests are just an example of the situation in which false negatives involve a much higher cost than the false positives \cite{Wan2014}.

In general, there are three different options to handle imbalanced datasets \cite{Fernandez2013}. One is to adjust the cost function of the algorithm, which means that during the training process, the model will be heavily penalized for misclassifying minority instances (false negatives). This can be a laborious task as it involves the modification of the training process for every algorithm the user applies. Another option is to focus on the modification of algorithms, reinforcing the learning towards the minority class. Again, this can be a complex and lengthy procedure, possibly restricting the algorithm options available to the user. Finally, the last option is to modify the data. This can be done by generating new synthetic instances of the minority classes, known as oversampling, or by removing instances from the majority class, known as undersampling. The objective of both is to balance the dataset, minimizing the skewness of the data distribution.

In this paper, we adopt the last option and tackle imbalanced learning problem by rebalancing class distribution through oversampling. This is a more general approach because, once the distribution is balanced, any supervised machine learning algorithm can be used, without any further modification. Additionally, by using oversampling, contrary to undersampling, no data is wasted, as none of the majority instances needs to be removed.

The oversampling process can be divided into two steps: An initial process of choosing where to generate the synthetic minority instances and the actual process of generating them. Both of these steps are essential for an effective oversampling algorithm and should prevent the generation of duplicate as well as noisy instances.

The method presented in this paper, G-SOMO, extends the Self-Organizing Map Oversampling algorithm (SOMO) \cite{Douzas2017}, shown to be particularly efficient in identifying areas where the synthetic minority instances should be generated, by coupling it with the Geometric SMOTE algorithm (G-SMOTE) \cite{Douzas2019}, used as the data generation mechanism of the minority instances.

SOMO is a clustering based oversampling algorithm which uses the Self-Organizing Map (SOM) and has been shown to outperform related algorithms over various datasets. It preserves the topology of the input space, while reducing the dimensionality to a two-dimensional representation. The emerging clusters are then used as a map to guide the generation of minority instances within, but also between neighboring minority clusters. In the original SOMO paper the generation process was done through the application of the SMOTE mechanism \cite{Chawla2002}.

The new algorithm presented in this paper, replaces SMOTE with G-SMOTE as the data generation mechanism. G-SMOTE generates synthetic samples in a geometric region of the input space, around each selected minority instance. It has been shown that G-SMOTE produces significant improvements in the generated data quality. We evaluate G-SOMO performance on binary classification problems using 69 datasets, 4 different classifiers and 3 different performance measures and compare it with Random Oversampling (ROS), SMOTE, K-Means SMOTE \cite{Douzas2018a}, SOMO and G-SMOTE.

In the next section we will outline related methods that have been shown to be efficient. In section 3, we explain some of the shortcomings of oversampling methods and explain the motivation for using G-SOMO. Next, we present in detail the G-SOMO algorithm, followed by a description of the experimental setup. In section 6 the results and their statistical analysis is presented. The last section summarizes our findings and provides some ideas for further research.

\section{Related work}

The section outlines the current state of the art oversampling methods. Oversampling algorithms generate synthetic examples for the minority class that are added to the training set. In contrast to oversampling, undersampling methods remove samples from the majority class to establish a class balance. This implies that information is excluded, which might affect the learning process negatively, especially when the data set is small. Both methods have shown to be effective, depending on the problem addressed \cite{Chawla2002}. More information on undersampling methods can be found in \cite{Ganganwar2012} and \cite{Yen2006}. Synthetic data instances can be created uninformed, by randomly duplicating minority instances or informed, by identifying areas where oversampling is deemed to be most effective \cite{Douzas2018a}.

The simplest form of oversampling is ROS, an uninformed approach, which randomly selects minority samples and duplicates them. The method stands out through its simplicity, but it increases the risk of overfitting, since the same information is used multiple times during the training process.

The most popular approach among practitioners in the domain of oversampling is SMOTE \cite{Chawla2002}. The algorithm chooses a random minority instance and identifies its $ k $ nearest neighboring minority instances. The parameter $ k $ is chosen beforehand. A synthetic sample is created along the line segment joining the selected minority instance and a randomly selected minority class neighbor. Figure \ref{fig:Schubach} shows the SMOTE data generation mechanism.

\begin{figure}[H]
	\centering
	\includegraphics[width=16.5cm, height=5cm, keepaspectratio]{../../analysis/gsomo/fig1.png}
	\captionbelow{SMOTE data generation machanism, adopted from \cite{Schubach2017}}.
	\label{fig:Schubach}
\end{figure}

Compared to ROS, the synthetic samples are not copies of the original samples and therefore the risk of overfitting is reduced. However, SMOTE has several drawbacks. First, the algorithm randomly selects a minority instance for oversampling with uniform probability. Therefore it tackles between-class imbalance, while within-class imbalance is ignored \cite{Nekooeimehr2016}. This is because areas that were highly populated by minority samples will expand, while smaller areas of minority samples will remain sparse \cite{Prati2004}. Second, the algorithm is prone to the generation of noisy samples. An example of it is when initially a noisy minority class sample is selected. This scenario frequently results in the generation of additional noisy samples.

To tackle the problems of SMOTE several modifications were created. SMOTE + Edited Nearest Neighbor \cite{Batista2004} removes any misclassified instances after the creation of synthetic samples, by applying the edited nearest neighbor rule. Safe-Level SMOTE \cite{Bunkhumpornpat2009} applies a weight degree to differentiate between noisy and safe instances. Furthermore, G-SMOTE \cite{Douzas2019} extends the linear interpolation mechanism of SMOTE by introducing a geometric region where the data generation process occurs. Thereby, the algorithm increases the data variability and prevents additional correlation between the created samples. Further algorithms like Borderline-SMOTE \cite{Han2005}, MWMOTE (Majority Weighted Minority Oversampling Technique for Imbalanced Data Set Learning) \cite{Barua2014}, ADASYN and its variation KernelADASYN \cite{Tang2015} are trying to avoid noisy samples and focus on hard to learn instances. Hereby, the borderline samples of the classes are identified to use the informative minority class instances.

The prior algorithms focus exclusively on between-class imbalance. On the other hand, within-class-imbalance refers to identifying sparse or dense clusters of minority or majority instances. To tackle this challenge, different clustering based oversampling methods have been proposed. These methods are segmenting the instances and then apply traditional oversampling algorithms to each segment.

Cluster SMOTE \cite{Cieslak2006} utilizes the K-means clustering algorithm, to identify clusters with a specific threshold of minority instances, before applying SMOTE within these clusters. In addition, K-means SMOTE \cite{Douzas2018a} method applies a similar approach, but also considers the density within the clusters. Then sparse clusters receive a higher number of generated samples compared to dense clusters. DBSMOTE \cite{Bunkhumpornpat2012} provides another approach to cluster based oversampling, by applying the density-based DBSCAN algorithm to discover arbitrarily shaped clusters and create artificial data instances along the shortest path from each minority class instance to a pseudo-centroid of the cluster. A-SUWO \cite{Nekooeimehr2016} uses a hierarchical clustering approach and adaptively determines the size to oversample each sub-cluster using its classification complexity and cross-validation procedure.

The SOMO algorithm \cite{Douzas2017} applies a self-organizing map to create a two-dimensional representation of the multidimensional input space and creates inter-cluster and intra-cluster synthetic samples based on the underlying manifold structure. Specifically, the algorithm maps the input space in a two-dimensional space where clusters are identified. The next step is to apply SMOTE in order to generate synthetic instances within selected clusters as well as between them. Besides clustering based algorithms, further approaches have been proposed, that are based on ensemble methods like SMOTEBoost \cite{Chawla2003} and DataBoost-IM \cite{Guo2004}.

\section{Motivation}

The section briefly outlines the challenges and shortcomings of competitive
oversampling methods and clarifies the motivation for our proposed algorithm
G-SOMO.

A classifiers performance is significantly influenced by the positioning of the instances in the dataset as presented in Figure \ref{fig:Sez}. A common challenge in real-world problems, is that the instances of a class can be spread over the boundaries of the majority class \cite{Tang2007}. Furthermore, the different types of instances can be labeled as safe and unsafe, while the latter are further divided as borderline and outlier instances \cite{Sez2016}.

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm,height=7cm, keepaspectratio]{../../analysis/gsomo/fig2.png}
	\captionbelow{Challenging data formations, adopted from  \cite{Sez2016}.}
	\label{fig:Sez}
\end{figure}

Safe instances are located in the homogeneous regions and populated by the examples from one class only \cite{rodriguez2012}. They are clearly separated from examples of other classes. Most classifiers are able to correctly identify those instances \cite{Prati2004B}. Instances that are not clearly separated are considered unsafe ones, like borderline instances and outliers \cite{Kubat1997}. Borderline instances are placed in the boundary region between classes, where instances of multiple classes overlap \cite{Sez2016}. Outliers are isolated distinct instances, also termed as noise.

Learning algorithms are challenged when they are confronted with overlapping areas between classes, especially unsafe instances. Therefore, oversampling algorithms should be able to carefully consider in which areas to create artificial instances and which areas to ignore. Inefficiencies that were observed with SMOTE and similar algorithms are the generation of noise penetrating the area of majority class, as well as the generation of nearly duplicated examples. Noisy examples can be created when applying $k$-nearest neighbor approaches, since they can either choose a noisy instance as a starting point or select a noisy instance as nearest neighbor, as seen in Figure \ref{fig:Douzas}a and \ref{fig:Douzas}b. Similar or nearly duplicated instances are created while generating new artificial instances within the borders of a minority cluster since they do not help to strengthen the decision boundary and might lead to overfitting, as it can be seen in Figure \ref{fig:Douzas}c. Figure \ref{fig:Douzas}d outlines the case, in which the parameter $k$ is too high and observations from another cluster are selected.

\begin{figure}[H]
	\centering
	\includegraphics[width=16.5cm, height=14cm, keepaspectratio]{../../analysis/gsomo/fig3.png}
	\captionbelow{Shortcomings of SMOTE, adopted from \cite{Douzas2017}}
	\label{fig:Douzas}
\end{figure}

SOMO algorithm improves the selection criteria of the minority class samples, which are used to generate synthetic examples. Through the synthetic data generation in and between neighboring minority clusters, as well as the consideration of the density, the algorithm also manages to generate the synthetic instances in more productive areas of the data space. Therefore, the generation of noisy examples is avoided. Although SOMO manages to identify efficient oversampling areas, the generation of synthetic instances handled by the SMOTE algorithm still provides several drawbacks. Challenges, as outlined in Figure \ref{fig:Douzas}d, are prevented through the above identification of clusters, nevertheless, the SMOTE algorithm introduces a high correlation between samples, since it only creates synthetic instances that are on the line segment between two minority class instances.

G-SMOTE extends the linear interpolation mechanism by introducing a geometric region, in which the data generation process occurs. Figure \ref{fig:GSMOTE} illustrates this idea.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm, height=9.5cm, keepaspectratio]{../../analysis/gsomo/fig4.png}
	\captionbelow{Synthetic data generation based on a geometric region, adopted from \cite{Douzas2019}}
	\label{fig:GSMOTE}
\end{figure}

Besides utilizing a geometric region for the data generation mechanism, G-SMOTE also introduced the \textit{majority} and \textit{combined} selection strategies, which avoid the scenarios shown in Figures \ref{fig:Douzas}a, \ref{fig:Douzas}b and \ref{fig:Douzas}d. More specifically, the synthetic sample is created between the initially selected minority instance and the closest majority instance, as it can be seen in Figure \ref{fig:GSMOTE}. It has been empirically shown that G-SMOTE \cite{Douzas2019} significantly increased the performance of SMOTE.

SOMO provides an efficient process to identify areas that should be populated
with minority instances, but lacks the improvements provided by G-SMOTE. On the
other hand, G-SMOTE exploits an intelligent and efficient approach to generate
synthetic instances, but lacks the ability to identify attractive regions for their generation. In this paper we propose a combination of both techniques,
leveraging the benefits of both algorithms.

\section{Proposed method}

The previous section outlined the inefficiencies of the SMOTE algorithm, as well as of SOMO and G-SMOTE. The proposed method, G-SOMO, manages to tackle them, by utilizing the best characteristics of both algorithms. The method considers the density, based on the average Euclidean distances, in and between clusters when creating artificial data instances. Also, G-SOMO creates synthetic instances within a geometric region, increasing their variety. Finally, through the \textit{majority} or \textit{combined} selection strategies, the creation of noisy examples is avoided. This should already be prevented through the identification of clusters, but still provides an advantage, in the case where the number of clusters were not correctly selected.

The proposed method consists of three stages:

\begin{enumerate}

	\item Initially, the SOM algorithm is applied to the normalized input data set. Consequently, a mapping of the high dimensional data space into a low-dimensional space is created in such a way that the topological relations of the input patterns are preserved \cite{Kker2007}. To train the SOM, the Euclidean distance between the input vector and all neural weights has to be calculated. The neuron that has the shortest distance to the input vector (the winner) is chosen and its weights are slightly modified to the direction represented by the input vector. Afterward, the neighboring neurons weights are modified in the same direction \cite{Brocki2007}. Due to SOMs ability to preserve topological
	relations from high dimensional input spaces, insights in the underlying data structure can be retrieved by analyzing the two-dimensional output map.

	\item In the second stage of the algorithm, filtered clusters are identified, i.e. clusters where the minority class dominates the majority class on the number of samples in the cluster. A weighted approach is used to create synthetic instances in the filtered clusters and also between neighboring filtered clusters.

	\item The last stage of the algorithm is marked by the creation of synthetic instances within the identified filtered clusters as well as between them. The synthetic data generation is based on a geometric region, that is formed between the current minority instance and its selected neighbor. The shape of the geometric region varies, depending on various hyper-parameters.

\end{enumerate}

\subsection{G-SOMO Algorithm}

The complete G-SOMO algorithm is listed below:

\begin{algorithm}[H]

	\SetAlgoLined
	\caption{Pseudo code for G-SOMO implementation}

	\BlankLine

	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}

	\Input{$S_{maj}$, $S_{min}$, $N_{intra}$, $N_{inter}$, $IR_{f}$, $SOM_{parameters}$, $\alpha_{trunc}$, $\alpha_{def}$, $\alpha_{sel}$}

	\BlankLine

	\textbf{Algorithm}
	\SetAlgoLined

	\begin{enumerate}

		\item Train a SOM with $SOM_{parameters}$ on the normalized input data set. Define as $cl$ the collection of randomly assigned cluster labels on the SOM nodes that contain at least one minority class instance.

		\item Define $cl_{f} =\{i \in cl: IR^{i} < IR_{f} \} $ and $cl_{f, n} =\{(i, j) \in cl_{f} \times cl_{f}: i, j \text{ topological neighbors } \} $ where $IR^{i}$ is the $IR$ of  $i$ cluster.

		\item Calculate the density factor for each $i \in cl_{f}$  as $den^{i}= \dfrac{n^{i}_{+}}{(dist^{i})^2}$ and for each $(i, j) \in cl_{f, n}$ as $den^{i, j} = den^{i} + den^{j}$ where $n^{i}_{+}$ is the number of minority class samples in $i$ cluster and $dist^{i}$ is the sum of Euclidean distances between all pairs  of minority class samples in $i$ cluster.

		\item Calculate the sampling weights for the intra-cluster case as $w^{i}_{intra} = \dfrac{1 / den^{i}}{\sum_{i \in cl_{f}} 1/den^{i}}$ and for the inter-cluster case as $w^{(i, j)}_{inter} = \dfrac{1 / den^{(i, j)}}{\sum_{i,j \in cl_{f}} 1/den^{(i,j)}}$.

		\item Calculate the number of artificial instances to be created for each $i \in cl_{f}$ from $[w^{i}_{intra} \cdot N_{intra}]$ and for each $(i, j) \in cl_{f, n}$ from $[w^{(i, j)}_{inter} \cdot N_{inter}]$.

		\item Define as $S_{min, i}$ and $S_{maj, i}$ the sets of minority and majority class instances for $i$ cluster, respectively. Set $S_{synthetic} = \emptyset$.

		\item For each $i \in cl_{f}$ and each $(i, j) \in cl_{f, n}$ repeat the following:
		
		\begin{enumerate}[label*=\arabic*.]
			
			\item Shuffle $S_{min, i}$.
			
			\item Repeat until the calculated number of instances from step 6 is created:
			
			\begin{enumerate}[label*=\arabic*.]
				
				\item Select $x_{center}$ from $S_{min, i}$.
				
				\item Apply the $\alpha_{sel}$ selection strategy and calculate a radius $R$. For the case of neighboring filtered clusters, use $S_{min, j}$ and $S_{maj, j}$ to search for the nearest neighbors.
				
				\item Generate a random point inside the unit-hypersphere centered at the origin of the input space.
				
				\item Truncate the unit hyper sphere using $\alpha_{trunc}$ hyper-parameter.
				
				\item Deform the unit hyper sphere to a hyper spheroid using
				$\alpha_{def}$ hyper-parameter.
				
				\item Rescale the created sample $x_{gen}$ using the radius $R$.
				
				\item Add the sample $x_{gen}$ to the set $S_{synthetic}$ of generated instances.
			
			\end{enumerate}
		
		\end{enumerate}
	
	\end{enumerate}
	
	\Output{ $S' = S_{maj} \cup S_{min} \cup S_{synthetic}$}

\end{algorithm}

\subsection{Explanation of G-SOMO}

The G-SOMO algorithm relies on filtered clusters, which are areas where the number of minority class instances dominates over the number of majority class instances. These regions can be considered as safe areas for the generation of minority samples. G-SOMO assumes that noisy examples belong to non-filtered clusters and are ignored. The density and weights of each filtered cluster are calculated to create new instances in each filtered cluster (intra-cluster) and between filtered clusters that are neighbors on the topological output map of the SOM algorithm (inter-cluster). The geometric region used for the data generation process increases the variety of the generated instances.

\textbf{Input}

The G-SOMO algorithm requires the following input parameters:

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item $S_{maj}$ and $S_{min}$ represent the set of instances of the majority and minority classes, respectively.

	\item $N_{intra}$, $N_{inter}$ are the number of artificial instances that will be created within and between neighboring filtered clusters.

	\item $IR_{f}$ represents the maximum imbalance ratio of any filtered cluster and takes positive values. Higher values allow more clusters to be included in the set of filtered clusters.

	\item $SOM_{parameters}$ represents the set of all SOM hyper-parameters. One notable SOM hyper-parameter is the size of the grid $N_{grid}$, which determines the number of nodes and therefore clusters.

	\item $\alpha_{trunc}$, $\alpha_{def}$ and $\alpha_{sel}$, known as geometric hyper-parameters, control the geometric characteristics of the G-SMOTE data generation mechanism.

\end{itemize}

\textbf{Steps 1-5}

These steps correspond to the SOMO algorithm minus the data generation phase.
Initially the input data are normalized and SOM algorithm is applied with the
provided hyper-parameters $SOM_{parameters}$. The set of filtered clusters $cl_{f}$ is identified by comparing the imbalance ratio $IR_{i} = \frac{n_{-i}}{n_{+i}}$ of each cluster $i \in cl_{f}$, where $n_{-i}$ and $n_{+i}$ are the number of instances belonging to the minority and majority classes respectively, to a threshold called $IR_{f}$. Filtered clusters are areas of the input space where the number of minority instances dominate over the number of majority instances and therefore can be considered as safe areas to apply any local data generation mechanism. An important point is that the G-SOMO algorithm will not create any artificial instances if no filtered clusters are found. Additionally the set $cl_{f, n}$ of neighboring filtered clusters pairs is identified by including only pairs of filtered clusters that are topological neighbors.

For each identified filtered cluster $i$, the average Euclidean distance $dist_{i}$ is calculated between the minority instances. Using the average Euclidean distance, we define a density factor to each filtered cluster as $den_{i} = \frac{n_{+i}}{dist_{i}^2}$. This measure provides information about the distribution of the instances within each cluster and is used to assign the correct amount of artificial samples to each cluster. Also for each topological neighboring combination of filtered clusters $(i, j)$, the density $den^{i, j}$ is defined as the sum of both individual density factors. Figure \ref{fig:Somo_Overview}a illustrates the SOM Grid with all identified filtered clusters and Figure \ref{fig:Somo_Overview}b outlines the topological neighbor structure.

Utilizing the density information, a relative weight $w^{i}_{intra}$ is calculated for each filtered cluster, that represents the normalized inverse density. $N_{intra}$ and $N_{inter}$ provide the guideline of the total synthetic instances to be created in the intra-cluster and inter-cluster processes, respectively. $N_{intra}$ times the weight for each specific filtered cluster results in the amount of artificial data that is generated for the intra-cluster case. Similarly, the weights $w^{(i, j)}_{inter}$ of two neighboring filtered clusters $(i, j)$ are calculated. Once each neighboring pair of filtered clusters has a relative weight assigned, $N_{inter}$ times the relative weight results in the number of synthetic instances that are generated for the inter-cluster case. If no neighborhood relations exist then the artificial samples are created through the intra-cluster process.

\begin{figure}[H]
	\centering
	\includegraphics[width=18cm,height=13cm, keepaspectratio]{../../analysis/gsomo/fig5.png}
	\captionbelow{Overview of identified filtered clusters and neighboring relations, adopted from \cite{Douzas2017}}
	\label{fig:Somo_Overview}
\end{figure}

\textbf{Step 7:}

In previous steps, the numbers of required synthetic instances for each filtered cluster and each neighboring pair of filtered clusters were calculated. The following process, that corresponds to the G-SMOTE data generation mechanism, is applied to each one of them individually.

The set of minority instances is shuffled, minority instances are repetitively selected, where each minority instance can be selected multiple times if the number of required synthetic instances is higher than the number of minority instances. The current selected minority sample is named $x_{center}$. Based on $x_{center}$ a selection strategy given by $a_{sel}$ is applied and identifies $x_{surface}$, the final neighbor of $x_{center}$. Both are used to define the geometric area where new instances are generated and a radius $R$ is defined as their Euclidean distance.

The data generation process starts by creating a unit hypersphere around the origin, as it can be seen in Figure \ref{fig:Hypersphere}a. This hypersphere is going to be modified within the next steps. Initially, a random point $e_{sphere}$ is created on the surface of the unit hypersphere. Subsequent, $e_{sphere}$ is transformed to $x_{gen}$ a random point along the radius, uniformly distributed. Next, a truncation is applied, a partition of the hypersphere, as illustrated in \ref{fig:Hypersphere}b with $\alpha_{trunc}$ determining the degree of the truncation. Another transformation deforms the truncated hypersphere to a hyper spheroid, as it can be seen in \ref{fig:Hypersphere}c. The parameter $\alpha_{def}$ controls the degree of the deformation.  As a final transformation we translate and rescale the point $x_{gen}$ based on $x_{center}$ and the radius $R$.

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm,height=10cm, keepaspectratio]{../../analysis/gsomo/fig6.png}
	\captionbelow{Constructing the geometric region, based on the unit hyper sphere, truncation and
		deformation, adopted from \cite{Douzas2017}}
	\label{fig:Hypersphere}
\end{figure}

The process is repeated until $N_{intra} + N_{inter}$ are generated. The final output of the G-SOMO algorithm is the oversampled matrix $S'$, consisting of the input data set and the set $S_{synthetic}$ of synthetic minority instances.

\subsection{Time complexity of G-SOMO}

As it was explained above, G-SOMO algorithm consists of three stages. Below we present the time complexity of G-SOMO, focusing on parameters that are related to the characteristics of the dataset. 

The first stage is the application of the SOM algorithm.  The time complexity of SOM is \( \mathcal{O}(N_{samples} \cdot N_{grid}^2 ) \) where \( N_{samples} = |S_{maj} \cup S_{min}| \) is the number of the dataset's observations. 

The second and third stages include the identification of the filtered clusters and the creation of the synthetic instances using the G-SMOTE algorithm. Both of them require the calculation of the distance matrix between samples that has a time complexity \( \mathcal{O}(N_{samples}^2 \cdot N_{features}) \) where \( N_{features} \) is the number of the dataset's features.

Since the number of SOM nodes \( N_{grid}^2 \) is less than the number of samples, the final time complexity of G-SOMO is \( \mathcal{O}(N_{samples}^2 \cdot N_{features}) \). While this is identical to the time complexity of the underlying G-SMOTE oversampler when the $ majority $ or $ combined $ selection strategies are used, it can be considered as a limitation of G-SOMO since it is higher than the time complexity of the SMOTE algorithm and some of its variations that calculate the nearest neighbors only on the minority class samples.

\section{Research methodology}

This section describes the evaluation process of G-SOMO using multiple classifiers, datasets and metrics. The experimental data as well as the experimental procedure are presented in details, while a description of the evaluation measures, the classification algorithms and the software implementation is provided.

\subsection{Experimental data}

To ensure meaningful and significant insights we evaluated our models on binary classification problems for a total of \href{https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/publications/blob/master/imbalanced-learning/data/imbalanced.db}{69 datasets}, mainly from the UCI Machine Learning repository. In order to reach this high number of datasets, we randomly undersampled the minority class of some of them so that their IR is increased. Using this approach, we managed to create additional and more challenging datasets. Table 1 provides an overview of our datasets, the number of features, instances and their IR:

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},
	col sep=comma,
	header=true,
	columns={Dataset name,Features,Instances,Minority instances,Majority instances,Imbalance Ratio},
	columns/Dataset name/.style={column type=l,string type},
	columns/Features/.style={fixed,fixed zerofill,precision=0,column type=r},
	columns/Instances/.style={fixed,fixed zerofill,precision=0,column type=r},
	columns/Minority instances/.style={fixed,fixed zerofill,precision=0,column type=r},
	columns/Majority instances/.style={fixed,fixed zerofill,precision=0,column type=r},
	columns/Imbalance Ratio/.style={fixed,fixed zerofill,precision=2,column type=r},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Overview of experimental data.}}
]
{../../analysis/gsomo/datasets_summary.csv}

\subsection{Evaluation measures}

The model evaluation is based on different evaluation measures. Traditional metrics, such as accuracy, show a strong bias towards the majority class and should not be used on imbalanced datasets. The accuracy would seem to be precise, even though the model might not perform well on the minority class. To retrieve more accurate insights the following evaluation measures are used:

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item Area Under The ROC Curve (AUC)
	
	The ROC Curve is created by plotting the True Positive Rate against the False Positive Rate as the decision threshold is changing \cite{Hand2009}.

	\item F-score
	
	It is defined as the harmonic mean between precision and recall, assuming that both metrics are of equal importance \cite{Guo2018}.

	\item Geometric Mean Score (G-mean)
	
	The geometric mean score, as the name implies, is defined as the geometric mean between sensitivity and specificity.

\end{itemize}

\subsection{Machine learning algorithms}

The goal of the paper is to validate the effectiveness of G-SOMO as oversampling method. Therefore G-SOMO was evaluated on the aforementioned 69 datasets and compared to ROS, SMOTE, K-Means SMOTE, SOMO and G-SMOTE. K-Means SMOTE was included in order to verify the assumption that replacing K-Means and SMOTE with SOM and G-SMOTE respectively results into a performance gain. Similarly, SOMO and G-SMOTE are included since G-SOMO, as a minimum requirement, is expected to improve the performance compared to its component algorithms. Additionally, the performance of the classifiers without applying any type of oversampling is also included in the experimental results (NO OVER).

Finally, it is crucial to ensure that the obtained results are generalizable to different classifiers and not only to specific ones, due to their different characteristics. Therefore, different classifiers are selected to evaluate the performance of the oversampling methods. These classifiers are Logistic Regression (LR) \cite{McCullagh1989}, K-Nearest Neighbors (KNN) \cite{Cover1967}, Decision Tree (DT) \cite{Salzberg1994} and Gradient Boosting Classifier (GBC) \cite{Friedman2001}.

\subsection{Experimental procedure}

In order to evaluate the performance of each combination of oversampler and classifier, $n$-fold cross-validation was applied with $n = 5$. The data set was splitted into $n$ different subsets (also called folds). $n-1$ folds were used to train the classifier and generate artificial data by utilizing the oversampler while the last fold remained unseen to validate. The process was applied in an iterative manner, such that each fold was used for validation once. The validation scores of each model were averaged afterwards. The process was repeated three times to avoid bias due to random effects \cite{Japkowicz2013}.

A variety of hyper-parameters were used for the oversamplers. Below we group the common hyper-parameters and present their range of values:

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item For SMOTE, K-Means SMOTE, SOMO, G-SMOTE, G-SOMO
	
	$k \in \{3, 5 \}$ where $k$ is the value of $k$-nearest neighbors.

	\item For K-Means SMOTE
	
	$ N_{clusters} \in \{0\%, 25\%, 50\%, 75\% \} $ where $N_{clusters}$ is the number of K-Means clusters in units that correspond to the percentage of samples for each dataset (particulalrly $ 0\% $ does not cluster the input space). The threshold $ IR_{f} $ was set equal to $ 1.0 $.

	\item For SOMO, G-SOMO

	$ N_{grid} \in \{0\%, 25\%, 50\%, 75\% \} $ where $N_{grid}$ is the dimension of the SOM grid in units that correspond to the percentage of samples for each dataset (particulalrly $ 0\% $ does not cluster the input space). Also, instead of selecting $N_{intra}$ and $N_{inter}$ seperately, the ratio $ \frac{N_{intra}}{N_{intra} + N_{inter}} \in \{0.75, 1.0 \} $ is selected with the constrain that $N_{intra} + N_{inter} = N$, where $N$ is the number of generated instances that make the initial dataset perfectly balanced. Finally, the threshold $ IR_{f} $ was set equal to $ 1.0 $.

	\item For G-SMOTE, G-SOMO
	
	$ a_{trunc} \in \{-1.0, 0.0, 0.25, 1.0\} $, $ a_{def} \in \{0.0, 0.25, 1.0\} $,  $ a_{sel} \in \{ minority, majority, combined \} $ where $a_{trunc}$, $a_{def}$ and $a_{sel}$ are the truncation factor, deformation factor and selection strategy respectively.

\end{itemize}

Using the above hyper-parameters, the highest cross validation score for each combination of datasets, classifiers, oversamplers and evaluation metrics was reported. A ranking score was applied to compare the performance of the oversampling methods, aggregated over all datasets. The ranking score for the best performing method is 1, while for the worst performing method is 6. The Friedman test was applied to confirm the statistical significance of the ranking results \cite{Guyon2003}. The Friedman test is used to detect differences for a set of experimental attempts when normality assumption may not hold. In this case the null hypothesis represents the situation in which the classifiers show an identical performance, in terms of their mean ranking, independently of the oversampling method and performance metric used. Additionally, the Holms test was applied, using G-SOMO as the control method \cite{Guyon2003}. The Holms test is a non-parametric version of the t-test, where the null hypothesis is whether the proposed G-SOMO algorithm performs similarly to the other methods as the control method.

\subsection{Software implementation}

The implementation of the classifiers and oversamplers are based on the Python libraries \href{https://scikit-learn.org/stable/}{Scikit-Learn} \cite{Pedregosa2012} and \href{https://imbalanced-learn.org/en/stable/}{Imbalanced-Learn} \cite{Lemaitre2016}. All functions, algorithms, experiments and results reported are provided at the GitHub repository of the \href{https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/publications}{project}, while the \href{https://github.com/AlgoWit/cluster-over-sampling}{ G-SOMO implementation} is also available as an open-source library. Additionally, \href{https://research-learn.readthedocs.io/en/latest/?badge=latest}{Research-Learn} library provides a framework to implement comparative experiments, being also fully integrated with the Scikit-Learn ecosystem.

\section{Results and discussion}

In this section the performance of the different oversamplers is presented. The results are analyzed, by applying various statistical tests, to establish their significance. Additionally, a taxonomy of G-SOMO hyper-parameters is presented as well as guidelines for their tuning relative to the characteristics of the imbalanced datasets.

\subsection{Comparative presentation}

The mean and standard error of cross validation scores across datasets for each combination of classifiers, metrics and oversamplers are presented in Table 2:

\pgfplotstabletypeset[
	begin table=\bgroup\small\setlength{\tabcolsep}{3pt}\begin{longtable},
	end table=\end{longtable}\egroup,
	col sep=comma,
	header=true,
	columns={Classifier,Metric,NO OVER,ROS,SMOTE,K-MEANS SMOTE,SOMO,G-SMOTE,G-SOMO},
	columns/Classifier/.style={column type=c,string type},
	columns/Metric/.style={column type=c,string type},
	columns/NO OVER/.style={column type=c,string type},
	columns/ROS/.style={column type=c,string type},
	columns/SMOTE/.style={column type=c,string type},
	columns/K-MEANS SMOTE/.style={column type=c,string type},
	columns/SOMO/.style={column type=c,string type},
	columns/G-SMOTE/.style={column type=c,string type},
	columns/G-SOMO/.style={column type=c,string type},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Results for mean cross validation scores of oversamplers across the datasets.}}
]
{../../analysis/gsomo/mean_sem_scores.csv}

Table 2 shows that G-SOMO performs better on average than the rest of the methods. Particularly, the mean and standard error of percentage difference between G-SOMO and SMOTE, K-Means SMOTE, SOMO and G-SMOTE is presented in Table 3:

\pgfplotstabletypeset[
	begin table=\bgroup\small\setlength{\tabcolsep}{3pt}\begin{longtable},
	end table=\end{longtable}\egroup,
	col sep=comma,
	header=true,
	columns={Classifier,Metric,SMOTE,K-MEANS SMOTE,SOMO,G-SMOTE},
	columns/Classifier/.style={column type=c,string type},
	columns/Metric/.style={column type=c,string type},
	columns/SMOTE/.style={column type=c,string type},
	columns/K-MEANS SMOTE/.style={column type=c,string type},
	columns/SOMO/.style={column type=c,string type},
	columns/G-SMOTE/.style={column type=c,string type},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Results for percentage difference between G-SOMO and the rest of main oversamplers.}}
]
{../../analysis/gsomo/mean_sem_perc_diff_scores.csv}

Table 4 below illustrates the mean ranking of each oversampler on each metric and each classifier averaged across all datasets:

\pgfplotstabletypeset[
	begin table=\bgroup\small\setlength{\tabcolsep}{3pt}\begin{longtable},
	end table=\end{longtable}\egroup,
	col sep=comma,
	header=true,
	columns={Classifier,Metric,NO OVER,ROS,SMOTE,K-MEANS SMOTE,SOMO,G-SMOTE,G-SOMO},
	columns/Classifier/.style={column type=c,string type},
	columns/Metric/.style={column type=c,string type},
	columns/NO OVER/.style={column type=c,string type},
	columns/ROS/.style={column type=c,string type},
	columns/SMOTE/.style={column type=c,string type},
	columns/K-MEANS SMOTE/.style={column type=c,string type},
	columns/SOMO/.style={column type=c,string type},
	columns/G-SMOTE/.style={column type=c,string type},
	columns/G-SOMO/.style={column type=c,string type},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Results for mean ranking of oversamplers across the datasets.}}
]
{../../analysis/gsomo/mean_sem_ranking.csv}

Based on the insights from the above tables we can conclude, that G-SOMO
successfully outperforms the rest of oversampling methods, for any combination of metrics and classifiers.

\subsection{Statistical analysis}

The results of the application of the Friedman test are shown in Table 5:

\pgfplotstabletypeset[
	begin table=\bgroup\small\setlength{\tabcolsep}{3pt}\begin{longtable},
	end table=\end{longtable}\egroup,
	col sep=comma,
	header=true,
	columns={Classifier,Metric,p-value,Significance},
	columns/Classifier/.style={column type=c,string type},
	columns/Metric/.style={column type=c,string type},
	columns/p-value/.style={column type=c, zerofill, precision=1, dec sep align},
	columns/Significance/.style={column type=c,string type},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Results for Friedman test.}}
]
{../../analysis/gsomo/friedman_test.csv}

Therefore at a significance level of $a = 0.05$ the null hypothesis is rejected,
i.e. the classifiers do not perform similarly in the mean rankings across the
oversampling methods and evaluation metrics. Following the Friedman test, the
Holm's method is applied to adjust the p-values of the paired difference test
with G-SOMO algorithm as the control method. The adjusted $\text{p-values}$ are
presented in Table 6:

\pgfplotstabletypeset[
	begin table=\bgroup\small\setlength{\tabcolsep}{3pt}\begin{longtable},
	end table=\end{longtable}\egroup,
	col sep=comma,
	header=true,
	columns={Classifier,Metric,NO OVER,ROS,SMOTE,K-MEANS SMOTE, SOMO,G-SMOTE},
	columns/Classifier/.style={column type=c,string type},
	columns/Metric/.style={column type=c,string type},
	columns/NO OVER/.style={ zerofill,precision=1,column type=c, dec sep align},
	columns/ROS/.style={zerofill,precision=1,column type=c, dec sep align},
	columns/SMOTE/.style={zerofill,precision=1,column type=c, dec sep align},
	columns/K-MEANS SMOTE/.style={zerofill,precision=1,column type=c, dec sep align},
	columns/SOMO/.style={zerofill,precision=1,column type=c, dec sep align},
	columns/G-SMOTE/.style={zerofill,precision=1,column type=c, dec sep align},
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{Adjusted p-values using the Holm's method.}}
]
{../../analysis/gsomo/holms_test.csv}

Therefore, the null hypothesis of the Holm's test is rejected at a significance
level of $a = 0.05$ for 69 out of 72 cases, i.e. the hypothesis that G-SOMO's performance is similar to the rest of the methods is rejected. The 3 remaining cases correspond to G-SOMO and G-SMOTE performance comparison.

\subsection{Analysis and tuning of optimal hyper-parameters}

G-SOMO's hyper-parameters can be divided into two different categories. The first category is related to the SOMO parametrization while the second category are the $\alpha_{trunc}$, $\alpha_{def}$, $\alpha_{sel}$ hyper-parameters that control the G-SMOTE data generation mechanism.

The most crucial hyper-parameter of the first category is the optimal size $N_{grid}$ of the SOM grid. After SOM training is applied, the high dimensional input space will be transformed into a two-dimensional grid that consists of $N_{grid}^2$ clusters, which are used to identify safe areas for the data generation process. The challenge hereby is to select a value allowing to discriminate between sparse and dense minority class areas.  A very small value will not be able to identify sub-clusters, since the identified clusters will have a very large size of instances. Assuming a uniform distribution on the SOM map, one can set the threshold to $\sqrt{|S_{min}|}$ to ensure that each cluster contains on average one minority class. On the other hand, high values of $N_{grid}$ will result in smaller sized clusters, which might lead to filtered clusters in areas that we would usually like to ignore, because they are considered outliers. $\sqrt{|S_{maj}|}$ provides a reasonable upper bound for $N_{grid}$. The optimal value for $N_{grid}$ is dependent on the characteristics of the data set and can only be approximated in an experimental approach.

An analysis of the second category of G-SOMO hyper-parameters shows a dependence on Imbalance Ratio (IR) as well as the number of samples over number of features ratio (R). Specifically we can conclude the following:

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item High IR or low R.
	
	The $ majority $ or $ combined $ selection strategy and lower absolute values of the truncation and deformation factors dominate as optimal hyper-parameters.

	\item Low IR or high R.
	
	The $ minority $ selection strategy and higher absolute values of the truncation and deformation factors dominate as optimal hyper-parameters.

\end{itemize}

\section{Conclusion}

In this paper, we proposed a new oversampling algorithm G-SOMO. The algorithm examines the characteristics of the multi-dimensional input space while grouping the data to identify filtered clusters where minority instances dominate. Within these filtered clusters, as well as between neighboring filtered clusters, synthetic minority class instances are created inside the geometric region of a hyper-ellipsis.

G-SOMO was evaluated on binary classification problems using 69 different datasets and compared to popular oversampling methods that included also G-SOMO components i.e. SOMO and G-SMOTE algorithms. The performance was evaluated using the AUC, F-score and G-mean metrics. In order to avoid dependence of the results to a specific classifier, multiple ones were selected that differ in their characteristics. Each experiment was repeated 3 times with a 5-fold cross-validation procedure. The statistical significance of the results was established by appropriate statistical tests.

In particular, our empirical results highlight the need for oversampling algorithms. Utilizing no oversampling method produced the worst results in our mean ranking, while simple methods like ROS increased the performance when compared with it. The popular method SMOTE performed better than ROS, whereas it was confirmed that K-Means SMOTE, SOMO and G-SMOTE performed better than SMOTE. G-SOMO dominated the ranking and outperformed all the other oversamplers. Therefore, we are confident that G-SOMO is a new appealing approach for researchers and practitioners working with imbalanced datasets.

Future work may include the behavior of the algorithm on multi-class imbalanced datasets, since research about oversampling on these type of classification problems is limited compared to the binary case. Additionally, the data we used include only numerical features, therefore the underlying G-SMOTE data generation mechanism may be extended to generate also categorical data. While the proposed algorithm outperforms the rest state-of-the-art oversampling methods, a shift from the nearest-neighbors paradigm, using Conditional Generative Adversarial Network (CGAN) as an oversampler, may be worth to explore. An initial exploration of this approach has already been done in \cite{Douzas2018b} with encouraging results. Finally, another important future work is the theoretical analysis of various oversampling methods, including ROS, SMOTE and G-SMOTE.

\section*{Funding}

This research was funded by Fundação para a Ciência e Tecnologia (Portugal) \\
Grant number DSAIPA/DS/0116/2019 – MapIntel.

\bibliography{references}
\bibliographystyle{apalike}

\end{document}