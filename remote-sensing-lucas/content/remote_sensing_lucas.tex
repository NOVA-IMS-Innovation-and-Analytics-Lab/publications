\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority classes' prediction accuracy using the Geometric SMOTE algorithm}}

\author{Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Joao Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\) 
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
In spite of its importance in sustainable resource management, the automatic
production of Land Use/Land Cover maps continues to be a challenging problem.
The ability to build robust automatic classifiers able to produce accurate maps
can have a significant impact in the way we manage and optimize natural
resources. The difficulty in achieving these results comes from many different
factors, such as data quality, uncertainty, among others. In this paper, we
address the imbalanced learning problem, a common and difficult problem in
remote sensing that significantly affects the quality of classifiers. Having
very asymmetric distributions of the different classes constitutes a significant
hurdle for any classifier. In this work, we propose Geometric SMOTE (G-SMOTE) as
a means of addressing the imbalanced learning problem in remote sensing. G-SMOTE
is a sophisticated oversampler which significantly increases the quality of the
generated instances over previous methods, such as Synthetic Minority
Oversampling Technique (SMOTE). In this paper, we test the performance of
G-SMOTE, in the the LUCAS dataset, against other oversamplers with different
classifiers. The results show that G-SMOTE significantly outperforms all the
other oversamplers and improves the robustness of the classifiers. These results
indicate that, when using imbalanced datasets, remote sensing researchers should
consider the use of these new generation oversamplers to increase the quality of
the classification results.
\end{abstract}

\section{Introduction}
The production of accurate Land Use/Land Cover (LULC) maps offers unique
monitoring capabilities within the remote-sensing domain \cite{Mellor2015}. LULC
maps are being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. In order to build accurate and
robust supervised classifiers it is crucial to have a large enough training
dataset. Often, the problem is that different land cover types have very
different levels of area coverage, which causes some of them to be frequent in
the training dataset, while others are limited \cite{Feng2019}. In
remote-sensing classification problems, the number of instances of some classes
is often smaller when compared to the rest of the classes. This is especially
true in LULC data \cite{Williams2009, Cenggoro2018}.

The above mentioned asymmetry in class distribution affects negatively the
performance of classifiers. In the machine learning community, the problem is
referred as the imbalanced learning problem \cite{Chawla2004}. Generally, the
imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class problems \cite{Abdi2016}. Particularly,
the later appears to be an even more challenging task \cite{Garcia2018}. In both
cases, during the learning phase, the minority class(es) contribute less to the
minimization of accuracy, the typical objective function, inducing a bias
towards the majority class. Consequently, as typical classification algorithms
are designed to work with reasonably balanced datasets, defining the proper
decision boundaries between the different classes becomes a very difficult task
\cite{Saez2016}.

The possible approaches to deal with the class imbalance problem can be divided into three main
groups \cite{Fernandez2013}:

\begin{enumerate}

	\item Cost-sensitive solutions. They introduce a cost matrix that applies
	higher misclassification costs for the examples of the minority class.

	\item Algorithmic level solutions. They modify the algorithmic procedure to
	reinforce the learning of the minority class.

	\item Data level solutions. They rebalance class distribution by generating
	artificial data for the minority class(es).

\end{enumerate}

The later method constitutes a more general approach since it can be used for
any classification algorithm and it does not require any type of domain
knowledge in order to construct a cost matrix.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups:

\begin{enumerate}

	\item Undersampling algorithms reduce the size of the majority classes.
	
	\item Oversampling algorithms attempt to even the distributions by
	generating artificial data for the minority class(es).

	\item Hybrid approaches use both oversampling and undersampling techniques
	to ensure a balanced dataset.

\end{enumerate}

Studies suggest that the usage of data level solutions in remote sensing
problems, to balance class distribution, seem to outperform models trained by
randomly resampling from the training set, an approach know as Random
Oversampling \cite{Wang2019, Mellor2015}.  Studies employing informed
oversampling methods seem to consistently yield better results
\cite{Johnson2013, Geib2015} when compared to no oversampling. Other studies
employ active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}. In
spite of the potential benefits that oversampling techniques can bring to the
improvement of automatic classification, there are not many studies on the
subject in the remote sensing domain.

In this paper, we compare the performance of various oversampling algorithms on
an the publicly available EUROSTAT's Land Use/Cover Area Statistical Survey
(LUCAS) dataset \cite{LUCAS2015} with Landsat 8 dense time-series. The
experimental procedure includes a comparison of five oversamplers using five
classifiers and three evaluation metrics. More specifically the oversampling
algorithms are G-SMOTE \cite{Douzas2019}, SMOTE \cite{Chawla2002}, Borderline
SMOTE \cite{Han2005}, Adaptive Synthetic Sampling Technique (ADASYN)
\cite{HaiboHe2008} and Random Oversampling, while no oversampling is included as
a baseline method. Results show that G-SMOTE outperforms every other
oversampling technique, for the selected evaluation metrics.

This paper is organized in 5 sections: Section 2 analyses the resampling methods,
section 3 describes the proposed methodology, section 4 shows the results and
discussion and section 5 presents the conclusions drawn from this study.

\section{Resampling methods}

Data modification through resampling has been the most popular approach to deal
with the imbalanced learning problem, in machine learning in general and remote
sensing in particular \cite{Feng2019}. As it was mentioned above, by decoupling
the imbalance problem from the classification algorithms, resampling allows the
users to apply any standard algorithm once the resampling preprocessing step is
done. This is especially convenient for users that are not machine learning
experts and want to use several classifiers. Additionally, resampling methods
can be easily adapted to multi-class imbalanced data, which is relevant for LULC
classification. In this section we present the most relevant applications of
resampling methods for remote sensing imbalanced data classification.

\section{Non-informed resampling}

Some of the existing studies implement the Random Undersampling method, which
randomly reduces the number of the majority class training samples.
However, this method has the disadvantage of information loss as it discards
samples from the majority class \cite{Feng2019}. \cite{Feng2018} carries out a
comparative analysis of undersamplers and oversamplers performance for land
cover classification with Rotation Forest (RoF) ensemble classifier. The
selected dataset consists of 16 imbalanced classes from hypercritical imagery.
The reported results show that oversampling methods outperform undersampling methods.

Contrary to Random Undersampling, the Random Oversampling avoids information
loss. However, this method simply replicates randomly selected instances of the
minority class, consequently, increasing the risk of over-fitting
\cite{Krawczyk2016}. \cite{Maxwell2018} reports that balancing data with Random
Oversampling affects the classification performance differently for various
supervised models. In their paper, land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of Random Oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand, it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), K-Nearest Neighbors (KNN)
and Boosted DT.

\section{Informed resampling}

SMOTE is the most popular informed oversampling method, and it has been used to
successfully deal with the class imbalance problem in land cover classification.
In this approach the minority class is oversampled by selecting each minority 
class instance and generating synthetic examples along the line segments 
joining $k$ minority class neighbors. For this $k$-nearest neighbors selection 
rule is used, and the number of nearest neighbors is setup to five. Depending 
on the amount of required data generation, samples from five nearest 
neighbors are randomly selected.

The variational semi-supervised learning (VSSL) proposed by \cite{Cenggoro2018}
aims to fix the imbalance problem in LULC mapping. VSSL is a semi-supervised
learning framework consisting of a deep generative model allowing learning from
both labeled and unlabeled samples while using SMOTE to balance the data. The
results show significant improvements in performance on the producerâ€™s accuracy
of the minority class, when compared to the results without any imbalanced
learning.

Ensemble learning method together with oversampling algorithms have been
successfully implemented for remote sensing image classification in the resent
years. \cite{Feng2018, Feng2019} apply the Rotation Forest ensemble learning
algorithm with SMOTE to deal with class imbalance problem. The idea is to
iteratively balance the class distribution using SMOTE before creating each
rotation decision tree. This approach yielded improved classification accuracy
in the minority classes, as well as for the whole dataset.

A number of studies report significant improvements in LULC mapping accuracy
with the use of SMOTE oversampling along with standard algorithms to handle data
imbalance for multi-source remote sensing data. \cite{Johnson2016} uses
OpenStreetMap crowdsourced data and Landsat time series for LULC classification.
A simple implementation of the SMOTE oversampler shows improved classification
results for all the supervised classifiers (Naive Bayes, DT and RF).
\cite{Bogner2018} used a RF classifier to detect rare land use classes on MODIS
time series. They use SMOTE to decrease the imbalance ratio in the original
dataset. The comparison of different classification scenarios shows the minority
classes to be better classified when SMOTE is used. \cite{Panda2018} carried out
a simple implementation of SMOTE with five different classifiers (Naive Bayes,
DT, RF, KNN, SVM) to predict a land cover maps with 4, 5 and 6 classes. The
results show the relevance of using oversampling to improve the classifiers'
performance, as 5 of the classifiers improve their results.

Although recent studies demonstrate the usefulness of informed oversampling
methods for remote sensing applications, they still present some drawbacks.
SMOTE method has the disadvantage of generating noisy data \cite{He2008}. In
order to mitigate this problem many variations of SMOTE have been developed with
the objective of preventing noisy data generation. 

Borderline-SMOTE is one of the most popular SMOTE-based oversamplers. It has two
variations, Borderline-SMOTE1 and Borderline-SMOTE2 \cite{Han2005}. Similarly to
SMOTE, it uses $k$-nearest neighbors selection rule. The main difference to
SMOTE is that it modifies the data generation mechanism by generating samples
closer to the decision boundary. Borderline-SMOTE has also been reported to
perform better than SMOTE in number of studies \cite{Nguyen2009, Ramentol2012}.

ADASYN is another well known variation of SMOTE. Adaptive Synthetic sampling is
based on the idea of adaptively generating minority class instances according to
their weighted distribution: more instances are generated for those minority
class instances that are harder to learn compared to ones that are easier to
learn \cite{HaiboHe2008}. With this characteristics ADASYN differs from SMOTE
method, which gives equal chances for all the minority instances to be selected
for synthetic data generation. The synthetic instances are created based on the
majority nearest neighbors using $k$ nearest neighbor method. ADASYN
oversampling method has the advantage of reducing the learning bias introduced
by the original imbalance data distribution. One drawback of this approach is
that it does not identify noisy instances, and thus it does not prevent from
adding outlier values in the dataset.

G-SMOTE algorithm is an extension of the SMOTE SMOTE, the size of the area for
data generation for Geometric-SMOTE is derived from the distance of the selected
minority sample to the nearest neighbor using $k$-neighbors. The main difference
to SMOTE is that instead of using line segment between minority samples,
Geometric-SMOTE defines a flexible geometric region (hyper-spheroid) around each
minority class instance for synthetic data generation. The shape of geometric
regions are defined with $deformation factor$ and $truncation factor$
hyper-parameters. This intensively increases the options for more informative
data generation. Furthermore, Geometric-SMOTE is designed to avoid noisy sample
generation by some restrictions in the neighbor selection strategy. This
oversampling method has already proven to outperform SMOTE and random
oversampling algorithms across 69 imbalanced datasets and number of classifiers
such as logistic regression, K-Nearest Neighbors, Decision Trees and Gradient
Boosting Classifier.

In this study we use a set of
more recent oversamplers, most notably G-SMOTE, that is yet to be adopted within
the remote sensing community.

To compare the performance of the different oversamplers we use the LUCAS
dataset and 5 different supervised classifiers: Logistic Regression, K-Nearest
Neighbors, Decision Trees, Gradient Boosting Classifier and Random Forest.

\section{Methodology}

This section reports the evaluation process of G-SMOTE's performance. A
description of the baseline methods, classification algorithms, evaluation
methods, dataset used and experimental procedure is provided.

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well as the analysis of the results
are reproducible using the scripts available at \url{https://github.com/AlgoWit/
publications}.

\subsection{Oversamplers}

Four oversampling techniques were used in this experiment as baseline methods
along with G-SMOTE. Random oversampling was chosen for its simplicity. SMOTE was
selected for being a classic technique and the most popular oversampling
algorithm \cite{Douzas2019}. ADASYN \cite{HaiboHe2008} and Borderline SMOTE
\cite{Han2005} were selected for representing popular modifications of the
original SMOTE technique in the selection phase and data generation mechanism,
respectively. Finally, we use no oversampling as an additional baseline method.

\subsection{Classifiers}

The selection of classification algorithms was done according to 3 criteria:
learning type, training time and popularity within the remote sensing community.
We further divide classification algorithms into 4 different learning types:
neighbors-based, rule-based, ensemble and generalized linear methods.

For each combination of oversampler and classifier (represented as $x_a$ and
$y_b$, respectively), a grid search will be run in order to find the optimal
parameter settings. The number of fits for a single combination  is defined as:

\begin{equation}
fits_{x_a,y_b}=\prod\limits_{p=1} \textrm{count}(x_a^p).\prod\limits_{q=1}
\textrm{count}(y_b^q)
\end{equation}

Where $\textrm{count}(x_a^p)$ corresponds to the search grid size of
hyperparameter $p$ of an oversampler $x_a$. Finally, the total number of fits is
defined as:

\begin{equation}
Total\ Fits=k\sum\limits_{a=1} \sum\limits_{b=1} fits_{x_a,y_b}
\end{equation}

Where $k$ is the number of folds defined for cross-validation. Thus, adding new
classifiers, oversamplers or grid search values may exponentially increase the
time necessary to run the experiment. Therefore, in order to setup an extensive
grid search and run the full experiment within feasible time, selecting
computationally efficient classifiers becomes vital.

\cite{Khatami2016} developed a meta-analysis of classification processes out of
266 manuscripts published from 1998 to 2012. Additionally, \cite{Maxwell2018}
also describe mature classification methods used in remote sensing. Both
manuscripts were used to determine the most commonly used classifiers for LULC
classification problems: K-Nearest Neighbors, Support Vector Machines, Decision
Trees, Boosted Decision Trees, Random Forests and Neural Networks. None of the
manuscripts mention the use of generalized linear models.

Amongst the neighbors-based learning methods the KNN algorithm was selected. Out
of the rule-based learning methods, the decision tree classifier (DT) was
chosen. Two ensemble learning methods were chosen: Gradient boosting classifier
and Random Forest classifier. Finally, the logistic regression classifier was
selected amongst the generalized linear models, since it is a commonly used
baseline algorithm. All these algorithms were found to be computationally
efficient and commonly used classifiers for the proposed task (with exception
for the logistic regression).

Despite their common use, both artificial neural networks and Support Vector
Machines were not included in this experiment due to their long training times.

\subsection{Evaluation metrics} \label{evaluation methods}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several indices that are considered to be preliminary and
are widely used for LULC classification accuracy assessment. Those measures are
classification overall accuracy, user's accuracy or precision and producer's
accuracy or recall,  and are calculated using the classification confusion
matrix \cite{Liu2007}. However, the LUCAS dataset has high inter-class
variability. Hence, the minority classes can be difficult to account for by the
classifiers that are designed to maximize the overall accuracy, as the majority
classes are receiving more weight \cite{Inglada2017}. On the other hand,
\cite{He2008} states that the use of user's and producer's accuracy for class
imbalanced data is not straightforward and producer's accuracy is not sensitive
to the data distribution at all. Considering these, class specific metrics are
needed to better describe classifier's performance. For this purpose, F-score
and G-mean metrics are used to evaluate and compare the performance of the
oversampling methods. Classification overall accuracy (OA) is provided for
discussion.

\begin{itemize}

	\renewcommand\labelitemi{--}
	
	\item Overall Accuracy

	Classification Overall Accuracy is the number of correctly classified
	samples divided by the sum of all the samples on the confusion matrix.

	In case when the training data is imbalanced and the accuracy metric is
	utilized, the result of the grid search for the best parameters will
	indicate that the particular parameterization is the one with highest
	classification overall accuracy, which is biased towards the majority class
	and can disregard the classification of minority classes.

	\item F-score

	F-score (or F-measure) is the harmonic mean of user's accuracy (precision)
	and	producer's accuracy (recall), where user's accuracy is the fraction of
	correctly classified instances with regard to all instances classified as
	this certain class in the confusion matrix, and producer's accuracy is the
	fraction of  correctly classified instances with regard to all instances of
	that reference class. The F-score value for multi-class imbalanced data
	classification can be calculated considering the user's accuracy and
	producer's accuracy as a unweighted average of the respective values from
	all the classes.

	\begin{equation}
	F{-}score=2\frac{\text{E(UA)} \times \text{E(PA)}}{\text{E(UA)} +
	\text{E(PA)}}
	\end{equation}

	Where E(UA) and E(PA) are unweighted mean values of user's and producer's
	accuracy of all the classes, respectively.

	\item G-measure

	The geometric mean score (G-mean) is the root of the product of class-wise
	sensitivity and specificity. For the multi-class case sensitivity and
	specificity are calculated for each class, then averaged with simple mean
	value. Here the sensitivity (true positive rate) is the same with producer's
	accuracy (recall), and specificity (true negative rate) is defined as the
	proportion of actual negatives classified as such. Adequately, specificity
	can be defined as producer's accuracy of the negative class
	\cite{Silva2017}.

	\begin{equation}
	G{-}mean = \sqrt{E(Sensitivity) \times E(Specificity)}
	\end{equation}

	Where E(Sensitivity) and E(Specificity) are the unweighted mean values of
	sensitivity and specificity of all the classes, respectively. The geometric
	mean indicates the balance between classification performances on both
	majority and minority classes. It means, that high misclassification in the
	minority class will lead to a lower geometric mean value. This is a very
	valuable feature for evaluation of the classifier's performance with class
	imbalance problem, which is the case of this study.

\end{itemize}

\subsection{Experiment settings}

As it is stated in the section 3.2, machine-learning classifiers usually have
parameters that need to be set up by users for a better classification
performance. Parameter tuning to define the optimal values of the
hyper-parameters was performed with 3-fold cross-validation (CV) for each of the
model (combining classifiers with oversampler). The best parameters are later
selected to validate the classifier's performance.

In order to avoid over-optimistic classification results, implementation of CV
with oversampling technique should be carried out as following \cite{Lusa2015}.
In $k$-fold CV dataset is divided into $k$ parts, one part is excluded and used
as only test set, while the rest $k$-1 part is used to train the model. This
process is iterated and each of the $k$ folds are used only once as a test set
to evaluate the classifier's performance. Later, results are averaged across all
$k$ iterations. It is crucial to remember that all the steps for building the
prediction model should be applied only on the training data. From this respect,
the oversampling (or any kind of data resampling) technique should not be
applied on the whole dataset. Instead, it is used only on the training dataset
generated in each step of the CV procedure.

A range of hyper-parameters used for the grid search for each of the classifier
and oversampler are presented in the table \ref{tab:grid}.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier       & Hyper-Parameters & Values\\
		\hline
		LR               & maximum iterations   & 10000   \\
		KNN              & number of neighbors  & {3, 5} \\
		DT               & maximum depth        & {3, 6} \\
		GBC              & maximum depth        & {3, 6} \\
    			 		 & number of estimators & {50, 100} \\
		RF               & maximum depth        & {None, 3, 6} \\
				 		 & number of estimators & {50, 100} \\
		\toprule
		Oversampler      &                      & \\
		\hline
		G-SMOTE          & number of neighbors  & {3, 5} \\
				 		 & selection strategy   & combined, minority, majority\\
				 		 & truncation factor    & {-1.0, -0.5, .0, 0.25, 0.5,
				 		 0.75, 1.0} \\
				 		 & deformation factor   & {0, 0.2, 0.4, 0.5, 0.6, 0.8,
				 		 1.0} \\
 		SMOTE            & number of neighbors & {3, 5} \\
		BORDERLINE SMOTE & number of neighbors & {3, 5} \\
		ADASYN           & number of neighbors & {2, 3} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:grid}User-defined grid values}
\end{table}


\subsection{Dataset}

For this research LUCAS dataset and Landsat-8 time series from 2015 were used
for training and validation of supervised classifiers. 1694 ground-truth points
from LUCAS 2015 dataset are used as reference data. This dataset is grouped into
8 classes and represents the main land cover types for the study area. The area
of study is in the continental part of north-western Portugal, corresponding to
the area covered by Landsat image from the track 204 and row 32, with
coordinates of center lat: 40.336, long: -8.468.

The dataset's class distribution and imbalance ratio is presented in Table
\ref{tab:dataset_classes}. The data imbalance becomes visible with a minority
class of 4 instances (wetlands) and majority class of 761 instances (woodland).

\begin{table}[H]
	\centering
	\begin{tabular}{llrrrr}
		\toprule
		\textbf{LUCAS Category} & \textbf{Land cover type} & \textbf{Instances}
		& \textbf{Imbalance Ratio} \\
		\hline
		A & Artificial land & 131 & 5.81 \\
		B & Cropland        & 270 & 2.81 \\
		C & Woodland        & 761 & 1.00 \\
		D & Shrubland       & 296 & 2.61 \\
		E & Grassland       & 185 & 4.11 \\
		F & Bareland        & 37  & 20.56 \\
		G & Water           & 10  & 76.10 \\
		H & Wetlands        & 4   & 190.25\\
		\hline
		\textbf{Total} & & \textbf{1694} &  \textbf{190.25} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:dataset_classes}LUCAS nomenclature and sample distribution}
\end{table}

The remote sensing dataset presents 8 images from Landsat 8ETM+ multi-spectral
sensor. The images are Level-1 terrain corrected products (L1TP) and are
acquired in 2015 from February to September, 1 image each month. The acquisition
mode is Descending. Only bands 2, 3, 4, 5, 6 and 7 are used from each image.
Thus, each reference point from LUCAS dataset has 48 features (Table
\ref{tab:datasets}). Those features have the value of digital numbers of the
pixels from each spectral bands of the image time series representing LUCAS
point location.

\pgfplotstabletypeset[
begin table=\begin{longtable}, 
end table=\end{longtable},
col sep=comma, 
header=true, 
columns={Dataset name , LUCAS},
columns/Dataset name /.style={column type=l,string type},
columns/LUCAS/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead}, 
every last row/.style={after row=\bottomrule 
\caption{\label{tab:datasets}Description of the dataset}}
]
{../analysis/dataset_descrip.csv}

\section{Results and discussion}

This section presents the final results of classifiers performance with
different oversamplers over LUCAS dataset. A simple ranking of oversamplers
performance is carried out for comparative analysis.

The cross-validation scores for each combination of classifiers, evaluation
metrics and oversamplers are presented in Table \ref{tab:mean_cross_val}. From
this table we can observe that Geometric-SMOTE over-performs nearly all other
oversampling methods for both G-mean and F-score metrics on all classifiers. The
absolute best results are achieved when Geometric-SMOTE is combined with Fandom
Forest.

\pgfplotstabletypeset[
begin table=\begin{longtable}, 
end table=\end{longtable},
col sep=comma, 
header=true, 
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type}, 
columns/Metric/.style={column type=l,string type}, 
columns/NO OS/.style={column type=l,string type}, 
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type}, 
columns/B-SMOTE/.style={column type=l,string type},  
columns/ADASYN/.style={column type=l,string type}, 
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead}, 
every last row/.style={after row=\bottomrule 
\caption{\label{tab:mean_cross_val}Results for cross validation scores of 
oversamplers (NO OS: No Oversampling, RAND OS: Random Oversampling, 
B-SMOTE: Borderline SMOTE, G-SMOTE: Geometric SMOTE)}}
] 
{../analysis/mean_scores.csv}

Amongst the classifiers Random Forest shows best performance for all three
measures. It recorded constantly improved results for both G-mean and F-score
metrics while combined with oversamplers. Second best classification results are
obtained with Gradient Boosting Classifier, and the worst performance is with
single Decision Tree Classifier.

Somewhat contrary to expectations, for some of the data balancing methods
slightly decreased the accuracy of imbalance-specific metrics while compared to
the baseline methods. For instance, the KNN classifier recorded higher G- mean
value (0.50) with unchanged data compared to the results of Random Oversampling
(0.48), ADASYN (0.48) and SMOTE (0.49). Similarly, Logistic Regression
classifier recorded higher F-score value (0.30) with No Oversampling compared to
the ones with Random Oversampling (0.29), SMOTE (0.29) and ADASYN (0.28).

A ranking score was assigned to each oversampling method with the best and worst
performing methods receiving scores equal to 1 and 6, respectively. This
comparison is presented in Table \ref{tab:ranking}. From this table we can
observe that G-SMOTE systematically outperforms any other oversampling algorithm
according to the G-mean measure. Using F-score it performs best on all
classifiers except the Decision Tree classifier, where Borderline-SMOTE
outperforms the remaining oversamplers. Regardless, its discrepancy with
Geometric-SMOTE is negligible.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type}, 
columns/Metric/.style={column type=l,string type}, 
columns/NO OS/.style={column type=l,string type}, 
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type}, 
columns/B-SMOTE/.style={column type=l,string type},  
columns/ADASYN/.style={column type=l,string type}, 
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule 
\caption{\label{tab:ranking}Results for ranking of oversamplers}}
]
{../analysis/mean_ranking.csv}

The mean of ranking scores for each of oversampling method across the
classification algorithms is presented in the Table \ref{tab:mean_rankings}.
From this table it is easier to interpret that Geometric-SMOTE has been the best
method almost all the time for G-mean and F-score measures. The same way
Borderline-SMOTE has been the second best method almost all the times. In
contrast to those two methods, in average ADASYN could not improve the
classification results for any of measures compared to SMOTE method.

Accuracy scores' results depict the bias discussed in section \ref{evaluation
methods}. This metric, by attributing proportional importance to the predictive
power of each class according to the dataset's class distribution, hinders the
importance of minority classes. Therefore, this metric can be misleading for
imbalanced data classification problems. Accordingly, models with no
oversampling present the highest accuracy score. In a multi-class classification
problem with an imbalanced dataset, where the prediction of all the classes are
of equal importance, overall accuracy should not be implemented to evaluate the
model performance, as is commonly utilized in remote sensing. However, even for
accuracy metric, Geometric-SMOTE shows the best performance amongst the data
modification algorithms and exceeds the rest of the oversampling methods.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Oversampler,F1 macro,G-mean macro,Accuracy},
columns/Oversampler/.style={column type=l,string type}, 
columns/F1 macro/.style={column type=l,string type}, 
columns/G-mean macro/.style={column type=l,string type}, 
columns/Accuracy/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule 
\caption{\label{tab:mean_rankings}Results for mean ranking of oversamplers}}
]
{../analysis/model_mean_ranking.csv}

Table \ref{tab:smote_gsmote} presents the percentage difference between
Geometric-SMOTE and SMOTE. For this comparison SMOTE algorithm was selected due
to its popularity and wide use within the remote sensing society (see section
2). The table shows that Geometric-SMOTE outperforms SMOTE regardless of the
classifier or metric used.

\pgfplotstabletypeset[
begin table=\begin{longtable}, 
end table=\end{longtable},
col sep=comma, 
header=true, 
columns={Classifier,Metric,Difference},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/Difference/.style={column type=l,string type}, 
every head row/.style={before row=\toprule, after row=\midrule\endhead}, 
every last row/.style={after row=\bottomrule 
\caption{\label{tab:smote_gsmote}Results for percentage difference 
between G-SMOTE and SMOTE}}
]
{../analysis/mean_perc_diff_scores.csv}

\section{Conclusions}

In this paper we presented the performance of Geometric-SMOTE oversampling
algorithm as a novel approach to deal with class imbalance problem in naturally
imbalanced remote sensing datasets  with an aim to improve the LULC
classification. Geometric-SMOTE's performance was evaluated and compared with
other oversampling methods, specifically Random Oversampling, SMOTE and its
variations Borderline-SMOTE and ADASYN, through the use of a number of
classifiers: Logistic Regression, K-Nearest Neighbors, Decision Trees, Gradient
Boosting Classifier and Random Forests.

For this purpose, an imbalanced LUCAS dataset was used. For model validation and
comparison Overall Accuracy was used, as well as G-mean and F-score as they
represent imbalance-specific metrics. The models were trained using the 3-fold
cross-validation method. The experiment results show the importance of
oversampling methods for multi-class imbalanced data classification as in most
of the cases balancing data with oversampling resulted for higher G-mean and F-
score values.

Geometric-SMOTE shows the best results and dominated the ranking across the
oversamplers. However, table \ref{tab:ranking} reports Decision Trees to achieve
a higher F-Score when using the Borderline SMOTE algorithm. Although this
algorithm systematically performs better than any other oversampling method
using G-mean and F-score metrics, it may be useful to also consider Borderline
SMOTE. Geometric-SMOTE proves to be a useful tool for remote sensing researchers
and can effectively replace the two widely accepted oversamplers Random
Oversampling and SMOTE methods.

\bibliography{references}
\bibliographystyle{apalike}

\end{document}
