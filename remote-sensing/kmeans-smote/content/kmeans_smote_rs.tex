\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Improving Imbalanced Learning in Land Cover Classification \\ 
	\LARGE{A Heuristic Oversampling Method Based on K-Means and SMOTE}}
\author{
	Joao Fonseca\(^{1}\), Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\) 
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{
	a4paper,
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}

    Land cover maps are an important resource to make informed policy,
    development, planning and resource management decisions. The primary
    challenge for the development of accurate, timely and automated Land
    Use/Land Cover maps are technical skills. Specifically, remotely sensed data
    is often imbalanced, where the number of samples of a few classes is
    significantly greater the number of samples of the remaining classes. This
    asymmetric class distribution, impacts negatively the performance of
    classifiers and adds a new source of inaccuracy to the production of these
    maps. In this paper, we address this problem, known as the imbalanced
    learning problem, by using K-Means SMOTE, a recently proposed oversampling
    method. K-Means SMOTE is an oversampling algorithm that attempts to improve
    the quality of newly created artificial data by avoiding the generation of
    noisy data and effectively overcome data imbalance. The performance of
    K-Means SMOTE is compared to other popular oversampling methods using seven
    well known datasets and a variety of classifiers and evaluation metrics. The
    results show that the proposed method consistently outperforms the remaining
    oversamplers and produces higher quality land cover classifications.

\end{abstract}

\section{Introduction}

% context

The increasing amount of remote sensing missions granted the access to dense
time series (TS) data at a global level and provides up-to-date, accurate land
cover information \cite{Drusch2012}. This information is often materialized
through Land Use/Land Cover (LULC) maps, which constitute an essential asset for
various purposes, such as land cover change detection, urban planning,
environmental monitoring and natural hazard assessment \cite{Khatami2016}.
However, the timely production of accurate and updated LULC maps is still a
challenge within the remote sensing community \cite{Wulder2018}. LULC maps are
produced based on two main approaches: photo-interpreted by the human eye, or
automatic mapping using remotely sensed data and classification algorithms.

While photo-interpreted LULC maps rely on human operators and can be more
reliable, they also present some significant disadvantages. The most important
disadvantage are the production costs, in fact photo-interpretation consumes
significant resources, both money and time. Because of that, they are not
frequently updated and not suitable for operational mapping over large areas.
Finally, there is also the issue of overlooking rare or small-area classes, due
to factors such as the minimum mapping unit being used.

Automatic mapping with classification algorithms based on machine-learning (ML)
have been extensively researched and used to speed up and reduce the costs of
the production process. Improvements in classification algorithms are sure to
have significant impact in the efficiency with which remote sensing imagery is
used. Several challenges need to be tackled in order to improve automatic
classification:

\begin{enumerate}
    \item Improve the ability to handle high-dimensional datasets, in cases such
        as Multi-spectral TS composites high-dimensionality increases the
        complexity of the problem and creates a strain on computational power
        \cite{Stromann2020}.
    \item Improve class separability, as the production of an accurate LULC map
        can be hindered by the existence of classes with similar spectral
        signatures, making these classes difficult to distinguish
        \cite{Alonso-Sarria2019}.
    \item Resilience to mislabelled LULC patches, as the use of
        photo-interpreted training data poses a threat to the quality of any
        LULC map produced with this strategy, since factors such as the minimum
        mapping unit tend to cause the overlooking of small-area LULC patches
        and generates noisy training data that may reduce the prediction
        accuracy of a classifier \cite{Pelletier2017}.
	\item Dealing with rare land cover classes, due to the varying levels of
	      area coverage for each class. In this case using a purely random sampling strategy
	      will amount to a dataset with a roughly proportional class
	      distribution as the one on the landscape. On the other hand, the
	      acquisition of training datasets containing balanced class frequencies
	      is often unfeasible. This causes an asymmetry in class distribution,
	      where some classes are frequent in the training dataset, while others
	      have little expression \cite{Wang2019, Feng2019}.
\end{enumerate}

The latter challenge is known as the imbalanced learning problem
\cite{Chawla2004}. It is defined as a skewed distribution of observations found
in a dataset among classes in both binary and multi-class problems
\cite{Abdi2016}. This asymmetry in class distribution negatively impacts the
performance of classifiers, especially in multi-class problems. During the
learning phase, classifiers are optimized to maximize an objective function,
with overall accuracy being the most common one \cite{Maxwell2018}. This means
that observations belonging to minority classes contribute less to the
optimization process, translating into a bias towards majority classes.  As an
example, a trivial classifier can achieve 99\% overall accuracy on a binary
dataset where 1\% of the observations belong to the minority class if it
classifies all observations as belonging to the majority class. This is an
especially significant issue in the automatic classification of LULC maps, as
the distribution of the different land-use classes tends to be highly
imbalanced. Therefore, improvements in the ability to deal with imbalanced
datasets will translate into important progress in the automatic classification
of LULC maps.

There are three different types of approaches to deal with the class imbalance
problem \cite{Fernandez2013,Kaur2019}:

\begin{enumerate}
    \item Cost-sensitive solutions. Introduces a cost matrix to the learning
        phase with misclassification costs attributed to each class. Minority
        classes will have a higher cost than majority classes, forcing the
        algorithm to be more flexible and adapt better to predict minority
        classes.
    \item Algorithmic level solutions. Specific classifiers are modified to
        reinforce the learning on minority classes. Consists on the creation or
        adaptation of classifiers.
    \item Resampling solutions. Rebalances the dataset's class distribution by
        removing majority class instances and/or generating artificial minority
        instances. This is considered an external approach, where the
        intervention occurs before the learning phase, benefitting from
        versatility and independency from the classifier used.
\end{enumerate}

It is important to note that resampling strategies have a significant advantage
over the other approaches. By operating at the data level, resampling strategies
allow the use of any off the shelf algorithm, without the need for any type of
changes or adaptions to the algorithm. This is a significant advantage
especially considering that most users in remote sensing are not expert machine
learning engineers. 

Within resampling approaches there are three subgroups of approaches
\cite{Fernandez2013,Kaur2019,Luengo2020}:

\begin{enumerate}
    \item Undersampling methods, which rebalance class distribution by removing
        instances from the majority classes.
    \item Oversampling methods, which rebalance datasets by generating new
        artificial instances belonging to the minority classes.
    \item Hybrid methods, which are a combination of both oversampling and
        undersampling, resulting in the removal of instances in the majority
        classes and the generation of artificial instances in the minority
        classes.
\end{enumerate}

Resampling methods can be further distinguished between non-informed and
heuristic (i.e., informed) resampling techniques
\cite{Fernandez2013,Luengo2020,Garcia2016}. The former consist of methods that
duplicate/remove a random selection of data points to set class distributions to
user-specified levels, and are therefore a simpler approach to the problem. The
latter consists of more sophisticated approaches that aim to perform
over/undersampling based on the points' contextual information within their data
space.

The imbalanced learning problem is not new in machine learning but its relevancy
has been growing, as attested by \cite{Haixiang2017}. The problem has also been
addressed in the context of remote sensing \cite{Douzas2019rs}. In this paper,
we propose the K-means SMOTE (K-SMOTE) \cite{Douzas2018} oversampler to address
the imbalanced learning problem in a multiclass context for LULC classification
using various remote sensing datasets. The K-SMOTE algorithm presents
significant advantages over other oversamplers, by coupling two different
procedures in the generation of artificial data. The algorithm starts by
clustering the observations; next, the generation of the artificial observations
is done taking into consideration the distribution of majority/minority cases in
each individual cluster. The efficacy of K-SMOTE is tested using
different types of classifiers. To do so, we employ both commonly used and
state-of-the-art oversamplers as benchmarking methods: Random oversampling
(ROS), Synthetic Minority Oversampling Technique (SMOTE) \cite{Chawla2002} and
Borderline-SMOTE (B-SMOTE) \cite{Han2005}. Also as a baseline score we include
classification results without the use of any resampling method.

This paper is organized in 5 sections: section \ref{sec:sota} provides an
overview of the state-of-art, section \ref{sec:methodology} describes the
proposed methodology, section \ref{sec:results} covers the results and
discussion and section \ref{sec:conclusion} presents the conclusions taken from
this study.

\section{Imbalanced Learning Approaches} \label{sec:sota}

Imbalanced learning has been addressed in three different ways:
over/undersampling, cost-sensitive training and changes/adaptations in the
learning algorithms \cite{Kaur2019}. These approaches impact different
phases of the learning process, while over/undersampling can be seen as a
pre-processing step, cost-sensitive and changes in the algorithm imply a more
customized and complex intervention in the algorithms. In this
section, we focus on previous work related with resampling methods, while
providing a brief explanation of cost-sensitive and algorithmic level solutions.

All of the most common classifiers used for LULC classification tasks
\cite{Khatami2016, Gavade2019} are sensitive to class imbalance
\cite{Blagus2010}. Algorithm-based approaches typically focus on adaptations
based on ensemble classification methods \cite{Mellor2015} or common
non-ensemble based classifiers such as Support Vector Machines \cite{Shao2014}.
In \cite{Lee2016}, the reported results show that algorithm-based methods have
comparable performance to resampling methods.

Cost-sensitive solutions refer to changes in the importance attributed to each
instance through a cost matrix \cite{Huang2016,Cui2019,Dong2017}. A common cost
sensitive solution is found in \cite{Huang2016}. The authors use the inverse
class frequency (i.e., $1/|C_i|$) to give higher weight to minority classes. Cui
et al. \cite{Cui2019} extended this method by adding a hyperparameter $\beta$ to
class weights as $(1-\beta)/(1-\beta^{|C_i|})$. When $\beta=0$, no re-weighting
is done. When $\beta\rightarrow 1$, weights are the inverse of the frequency
class matrix. Another method \cite{Dong2017} explores adaptations of
Cross-entropy classification loss by adding different formulations of class
rectification loss.

Resampling (over/undersampling) is the most common approach to imbalanced
Learning is most commonly addressed through data resampling in machine learning
in general and remote sensing in particular \cite{Feng2019}. The generation of
artificial instances (i.e., augmenting the dataset), based on rare examples, is
done independently of any other step in the learning process. Once the procedure
is applied, any standard machine learning algorithm can be used. Its
simplicity makes resampling strategies particularly appealing for any user
(especially the non-sophisticated user) interested in applying several
classifiers, while maintaining a simple approach.
It is also important to notice that over/undersampling methods can also be
easily applied to multiclass problems, common in LULC classification tasks.

\subsection{Non-informed resampling methods}

There are two main non-informed resampling methods. Random Oversampling (ROS)
generates artificial observations through random duplication of minority
instances.  This method is used in remote sensing \cite{Sharififar2019,
Hounkpatin2018} for its simplicity, even though its mechanism makes the
classifier prone to overfitting \cite{Krawczyk2016}. Hounkpatin et al.
\cite{Hounkpatin2018} found that using ROS returned worse results than keeping
the original imbalance in their dataset.

A few of the recent remote sensing studies employed Random Undersampling (RUS)
\cite{Ferreira2019}, which randomly removes
observations belonging to majority classes. Although it's not as prone to
overfitting as ROS, it incurs into information loss by eliminating observations
from the majority class \cite{Feng2019}.

Another disadvantage of non-informed resampling methods is their performance-wise
inconsistency across classifiers. ROS' impact on the Indian Pines dataset was
found inconsistent between Random Forest Classifiers (RFC) and Support Vector
Machines (SVM) and lowered the predictive power of an artificial neural network
(ANN) \cite{Maxwell2018}. Similarly, RUS is found to generally lead to a lower
overall accuracy due to the associated information loss \cite{Maxwell2018}.

\subsection{Heuristic methods}

The methods presented in this section appear as a means to overcome the
insufficiencies found in non-informed resampling. They use either local or
global information to generate new, relevant, non-duplicated instances to
populate the minority classes and/or remove irrelevant instances from majority
classes. In a comparative analysis between over- and undersamplers' performance
for LULC classification \cite{Feng2018} using the rotation forest ensemble
classifier, authors found that oversampling methods consistently outperform
undersampling methods. This result led us to exclude undersampling from our
study.

SMOTE \cite{Chawla2002} was the first heuristic oversampling algorithm to be
proposed and has been the most popular one since then, likely due to its fair
degree of simplicity and quality of generated data. It takes a random minority
class sample and introduces synthetic examples along the line segment that join
a random $k$ minority class nearest neighbor to the selected sample.
Specifically, a single synthetic sample $\overrightarrow{z}$ is generated within
the line segment of a randomly selected minority class observation
$\overrightarrow{x}$ and one of its $k$ nearest neighbors $\overrightarrow{y}$
such that $\overrightarrow{z} =
\alpha\overrightarrow{x}+(1-\alpha)\overrightarrow{y}$, where $\alpha$ is a
random floating point between 0 and 1, as shown in Figure
\ref{fig:smote_example}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/smote_example}
	\caption{Example of SMOTE's data generation process.}
	\label{fig:smote_example}
\end{figure}

A number of studies implement SMOTE within the LULC classification context and
reported improvements on the quality of the trained predictors
\cite{Jozdani2019, Bogner2018}. Another study proposes an adaptation of SMOTE on
an algorithmic level for deep learning applications \cite{Zhu2020}. This method
combines both typical computer vision data augmentation techniques, such as
image rotation, scaling and flipping on the generated instances to populate
minority classes. Another algorithmic implementation is the variational
semi-supervised learning model \cite{Cenggoro2018}. It consists of a generative
model that allows learning from both labeled and unlabeled instances while
using SMOTE to balance the data.

Despite SMOTE's popularity, its limitations have motivated the development of more
sophisticated oversampling algorithms \cite{Douzas2019, Han2005, Ma2017,
Douzas2017, Douzas2018, HaiboHe2008}. \cite{Douzas2019} identify four major
weaknesses of the SMOTE algorithm, which can be summarized as:
\begin{enumerate}
    \item Generation of noisy instances due to random selection of a minority
        observation to oversample. The random selection of a minority
        observation makes SMOTE oversampling prone to the amplification of
        existing noisy data. This has been addressed by variants such as B-SMOTE
        \cite{Han2005} and ADASYN \cite{HaiboHe2008}. 

    \item Generation of noisy instances due to the selection of the $k$ nearest
        neighbors. In the event an observation (or a small number thereof) is
        not noisy but is isolated from the remaining clusters, known as the
        "small disjuncts problem" \cite{holte1989}, much like sample
        $\overrightarrow{b}$ from Figure \ref{fig:smote_example}, the selection
        of any nearest neighbor of the same class will have a high likelihood of
        producing a noisy sample.

    \item Generation of nearly duplicated instances. Whenever the linear
        interpolation is done between two observations that are close to each
        other, the generated instance becomes very similar to its parents and
        increases the risk of overfitting. G-SMOTE \cite{Douzas2019} attempts to
        address both the $k$ nearest neighbor selection mechanism problem as
        well as the generation of nearly duplicated instances problem. 

    \item Generation of noisy instances due to the use of observations from two
        different minority class clusters. Although an increased $k$ could
        potentially avoid the previous problem, it can also lead to the
        generation of artificial data between different minority clusters, as
        depicted Figure \ref{fig:smote_example} with the generation of point
        $\overrightarrow{r}$ using minority class observations
        $\overrightarrow{p}$ and $\overrightarrow{q}$. Cluster-based
        oversampling methods attempt to address this problem. 
\end{enumerate}

This last issue, the generation of noisy instances due to the existence of
several minority class clusters, is particularly relevant in remote sensing. It
is frequent that instances belonging to the same minority class can have
different spectral signatures, meaning that they will be clustered in different
parts of the input space. In this context, the use of SMOTE will lead to the
generation of noisy instances of the minority class. This problem can be
efficiently mitigated through the use of a cluster-based oversampling. According
to our literature review cluster-based oversampling approaches have never been
applied in the context of remote sensing. On the other hand, while there are
several references of the application of cluster-based oversampling in the
context of machine learning, there is no such application for the multiclass
case, a fundamental requirement for the application of oversampling in the
context of LULC. 

Cluster-based oversampling approaches introduce an additional layer to
SMOTE's selection mechanism, which is done through the inclusion of a clustering
process. This ensures that both between-class data balance and within each class
balance is preserved. The self-organizing map
oversampling (SOMO) \cite{Douzas2017} algorithm transforms the dataset into a
2-dimensional input, where the areas with the highest density of minority
samples are identified. SMOTE is then used to oversample each of the identified
areas separately. CURE-SMOTE \cite{Ma2017} applies a hierarchical clustering
algorithm (CURE) to discard isolated minority instances before applying SMOTE.
Although it avoids noise generation problems, it ignores within-class data
distribution. Another method \cite{Santos2015} uses K-means to cluster the
entire input space and applies SMOTE to clusters with the fewest observations,
regardless of their class label. The label of the generated observation is
copied from one of its parents. This method cannot ensure a balanced dataset
since class imbalance is not specifically addressed, but rather dataset
imbalance.

K-SMOTE~\cite{Douzas2018} avoids noisy data generation by modifying the data
selection mechanism. It employs $k$-means clustering to identify safe areas
using cluster-specific Imbalance Ratio (defined by
$\frac{count(C_{majority})}{count(C_{minority})}$) and determine the quantity of
generated samples per cluster based on a density measure. These samples are
finally generated using the SMOTE algorithm. The K-SMOTE's data generation
process is depicted in Figure~\ref{fig:kmeans_smote_example}. Note that the
number of samples generated for each cluster varies according to the sparsity of
each cluster (the sparser the cluster is, the more samples will be generated)
and a cluster is rejected if the cluster's IR surpasses the threshold.
Therefore, this method can be combined with any data generation mechanism, such
as G-SMOTE. Also K-SMOTE includes the SMOTE algorithm as a special case when the
number of clusters is set to one. Consequently, K-SMOTE is always guaranteed to
return results as good as or better than SMOTE.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/kmeans_smote_example}
	\caption{Example of K-SMOTE's data generation process. Clusters $A$,
		$B$ and $C$ are selected for
		oversampling, whereas cluster $D$ was rejected due to its
		high imbalance ratio. The oversampling is done using the SMOTE algorithm and
		the $k$ nearest neighbors selection only considers
		observations within the same cluster.}
	\label{fig:kmeans_smote_example}
\end{figure}

Although no other study was found to implement cluster-based oversampling,
another study \cite{Douzas2019rs} compared the performance of SMOTE, ROS,
ADASYN, B-SMOTE and G-SMOTE in a highly imbalanced LULC classification dataset.
The authors found that G-SMOTE consistently outperformed the remaining
oversampling algorithms regardless of the classifier used.

This paper's main contributions are:
\begin{itemize}
    \item Propose a cluster-based multiclass oversampling method appropriate
        for LULC classification;
    \item Compare its performance with the remaining oversamplers in a
        multiclass context with a set of widely used LULC classification
        datasets. Allows us to check for oversamplers' performance statistical
        significance across datasets and report K-SMOTE's performance in
        benchmark LULC datasets.
    \item Introducing a cluster-based oversampling algorithm within the remote
        sensing domain, as well as comparing its performance with the remaining
        oversamplers in a multiclass context.
\end{itemize}

\section{Methodology}\label{sec:methodology}

The purpose of this work is to understand the performance of K-SMOTE as opposed
to other popular and/or state-of-the-art oversamplers for LULC classification.
To do so, we employ 7 LULC datasets along with 3 evaluation metrics and 5
classifiers to evaluate the performance of oversamplers. In this section we
describe the datasets, evaluation metrics, oversamplers, classifiers and
software used as well as the procedure developed.

\subsection{Datasets}

The datasets used were extracted from publicly available hyperspectral scenes.
Information regarding each of these scenes is provided in this subsection. A
similar data preprocessing procedure was used for each scene: 1) Conversion of
each hyperspectral scene to a structured dataset and removal of instances with
no associated LULC class, 2) random sampling to maintain similar class
proportions on a sample of 10\% of each dataset and 3) removal of instances
belonging to a class with frequency lower than 20 or higher than 1000. This is
done to maintain the datasets to a practicable size due to computational
constraints, while conserving the relative LULC class frequencies and data
distribution. Table~\ref{tab:datasets_description} provides a description of the
final datasets used for this work.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=comma, header=true,
	columns={Dataset,Features,Instances,Min. Instances,Maj. Instances,IR, Classes}, string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:datasets_description}
				Description of the datasets used for this experiment.}}
]{../analysis/datasets_description.csv}

\subsubsection*{Indian Pines} 

The Indian Pines scene~\cite{Baumgardner2015} was collected on June 12, 1992 and
consists of AVIRIS hyperspectral image data covering the Indian Pine Test Site
3, located in North-western Indiana, USA. As a subset of a larger scene, it is
composed of $145 \times 145$ pixels (see Figure~\ref{fig:indian_pines}) and 220
spectral reflectance bands in the wavelength range 400 to 2500 nanometers.
Approximately two thirds of this scene is composed by agriculture and the other
third is composed of forest and other natural perennial vegetation.
Additionally, the scene also contains low density buildup areas.

\subsubsection*{Pavia Centre and University}

Both Pavia Centre and University scenes were acquired by the ROSIS sensor. These
scenes are located in Pavia, northern Italy. Pavia Centre is a $1096 \times
1096$ pixels image with 102 spectral bands, whereas Pavia University is a $610
\times 610$ pixels image with 103 spectral bands.  Both images have a
geometrical resolution of 1.3 meters and their ground truths are composed of 9
classes each (see Figures~\ref{fig:pavia_centre}
and~\ref{fig:pavia_university}).

\subsubsection*{Salinas and Salinas-A}

These scenes were collected by the AVIRIS sensor over Salinas Valley, California
and contain at-sensor radiance data. Salinas is a $512 \times 217$ pixels image
with 224 bands and 16 classes regarding vegetables, bare soil and vineyard
fields (see Figure~\ref{fig:salinas}). Salinas-A, a subscene of Salinas,
comprises $86 \times 83$ pixels and contains 6 classes regarding vegetables (see
Figure~\ref{fig:salinas_a}). These scenes have a geometrical resolution of 3.7
meters.

\subsubsection*{Botswana}

The Botswana scene was acquired by the Hyperion sensor on the NASA EO-1
satellite over the Okavango Delta, Botswana in 2001-2004 at a 30m spatial
resolution. Data preprocessing was performed by the UT Center for Space
Research. The scene comprises a $1476 \times 256$ pixels with 145 bands and 14
classes regarding land cover types in seasonal and occasional swamps, as well as
drier woodlands (see figure~\ref{fig:botswana}).

\subsubsection*{Kennedy Space Center}

The Kennedy Space Center scene was acquired by the AVIRIS sensor over the
Kennedy Space Center, Florida, on March 23, 1996. Out of the original 224 bands,
water absorption and low SNR bands were removed and a total of 176 bands at a
spatial resolution of 18m are used. The scene is a $512 \times 614$ pixel image
and contains a total of 16 classes (see figure~\ref{fig:kennedy_space_center}).

\begin{figure}[H]
	\centering
	\begin{subfigure}{.24\textwidth}
		\centering
		\captionsetup{skip=12pt}
		\includegraphics[height=1.5\linewidth]{../analysis/indian_pines}
		\subcaption{{\medbreak}}\label{fig:indian_pines}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/pavia_centre}
		\subcaption{{\medbreak}}\label{fig:pavia_centre}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/pavia_university}
		\subcaption{{\medbreak}}\label{fig:pavia_university}
	\end{subfigure}

	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/salinas}
		\subcaption{{\medbreak}}\label{fig:salinas}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/salinas_a}
		\subcaption{{\medbreak}}\label{fig:salinas_a}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/botswana}
		\subcaption{{\medbreak}}\label{fig:botswana}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/kennedy_space_center}
		\subcaption{{\medbreak}}\label{fig:kennedy_space_center}
	\end{subfigure}
	\caption{Gray scale visualization of a band (top row) and ground truth (bottom row) of
		each scene used in this study. (a) Indian Pines, (b) Pavia Centre, (c) Pavia
		University, (d) Salinas, (e) Salinas A, (f) Botswana, (g) Kennedy Space Center
    }\label{fig:scenes}
\end{figure}

\subsection{Evaluation Metrics}
Most of the satellite-based LULC classification studies (nearly 80\%) employ
\textit{Overall Accuracy} (OA) and the \textit{Kappa Coefficient}
\cite{Gavade2019}. Although, some authors argue that both evaluation metrics,
even when used simultaneously, are insufficient to fully address the area
estimation and uncertainty information needs \cite{Olofsson2013,Pontius2011}.
Other metrics like User's Accuracy (or \textit{Precision}) and Producer's
Accuracy (or \textit{Recall}) are also common metrics to evaluate per-class
prediction power. These metrics consist of ratios employing the True and False
Positives (\textit{TP} and \textit{FP}, number of correctly/incorrectly
classified observations of a given class) and True and False Negatives
(\textit{TN} and \textit{FN}, number of correctly/incorrectly classified
observations as not belonging to a given class). These metrics are formulated as
$Precision = \frac{TP}{TP+FP}$ and $Recall = \frac{TP}{TP+FN}$. While metrics
like OA and \textit{Kappa Coefficient} are significantly affected by imbalanced
class distributions, \textit{F-Score} is less sensitive to data imbalance and a
more appropriate choice for performance evaluation \cite{Jeni2013}.

The datasets used present significantly high IRs (see Table
\ref{tab:datasets_description}). Therefore, it is especially important to
attribute equal importance to the predictive power of all classes, which does
not happen with OA and \textit{Kappa Coefficient}. In this study, we employ 3
evaluation metrics: 1) \textit{G-mean}, since it is not affected by skewed class
distributions, 2) \textit{F-Score}, as it proved to be a more appropriate metric
for this problem when compared to other commonly used metrics \cite{Jeni2013},
and 3) \textit{Overall Accuracy}, for discussion purposes.

\begin{itemize}
	\item The \textit{G-mean} consists of the geometric mean of
	      $Specificity = \frac{TN}{TN + FP}$ and \textit{Sensitivity} (also known as \textit{Recall}). For multiclass problems, The
	      \textit{G-mean} is expressed as:

	      $$\textit{G-mean} = \sqrt{ \overline{Sensitivity} \times
			      \overline{Specificity}}$$

	\item \textit{F-score} is the harmonic mean of \textit{Precision} and
	      \textit{Recall}. The \textit{F-score} for the multi-class case can
	      be calculated using their average per class values \cite{He2009}:

	      $$\textit{F-score}=2\frac{\overline{Precision} \times \overline{Recall}}{\overline{Precision} +
			      \overline{Recall}}$$

	\item \textit{Overall Accuracy} is the number of correctly classified observations
	      divided by the total amount of observations. Having \( c \) as the label of the
	      various classes, \textit{Accuracy} is given by the following formula:

	      $$\textit{Accuracy} = \frac{ \sum\limits_{c}{ \text{TP}_{c} } }{
			      \sum\limits_{c}{ (\text{TP}_{c}  + \text{FP}_{c}) } } $$

\end{itemize}

\subsection{Machine Learning Algorithms}
The assess the quality of the K-SMOTE algorithm, five other oversampling
algorithms were used for benchmarking. ROS and SMOTE were chosen for their
simplicity and popularity. ADASYN and B-SMOTE were chosen for their popular
variations of the SMOTE algorithm. Finally, G-SMOTE was chosen for being a
state-of-the-art oversampler as it was found to outperform all of the other
oversamplers in \cite{Douzas2019rs}. Additionally we include the classification
results of no oversampling (NONE) as a baseline.

To assess the performance of each oversampler, we use the classifiers Logistic
Regression (LR) \cite{Nelder1972}, K-Nearest Neighbors (KNN)
\cite{Cover1967}, Decision Tree (DT) \cite{Salzberg1994}, Gradient Boosting
Classifier (GBC) \cite{Friedman2001} and Random Forest (RF) \cite{Liaw2002}.
This choice was based on the classifiers' popularity for LULC classification,
learning type and training time \cite{Maxwell2018,Gavade2019}.

\subsection{Experimental Procedure}

The procedure for the experiment reported in this study is similar to the one
proposed in \cite{Douzas2019rs}. We start by defining a hyperparameter search
grid, where a list of possible values for each relevant hyperparameter in both
classifiers and oversamplers is stored. Based on this search grid, all possible
combinations of oversamplers, classifiers and hyperparameters are formed.
Finally, for each dataset, hyperparameter combination and initialization we use
the evaluation strategy shown in Figure \ref{fig:experiment_pipeline}: $k$-fold
cross-validation strategy where $k=5$ to train each model defined and save the
averaged scores of each split.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/experiment_pipeline}
	\caption{Experimental procedure. The performance metrics are averaged over
    the 5 folds across each of the 3 different initializations of this procedure
    for a given combination of oversampler, classifier and hyperparameter
    definition.}
	\label{fig:experiment_pipeline}
\end{figure}

Each combination of oversampler, classifier and parameters definition is fit 5
times (once for each fold) per dataset. Each time, an oversampler will use the
training set ($80\%$ of the dataset) to generate a set with artificial data,
which is appended to the original training set in order to generate a training
dataset with the exact same number of observations for each class. The newly
formed training dataset is used to train the classifier and the test set ($20\%$
of the dataset, the remaining fold) is used to evaluate the performance of the
classifier. The evaluation scores are then averaged over the 5 times the process
is repeated. The range of hyperparameters used are shown in table
\ref{tab:grid}.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier       & Hyperparameters      & Values                            \\
		\midrule
		LR               & maximum iterations   & 10000                             \\
		KNN              & \# neighbors  & {3, 5, 8}                            \\
		RF               & maximum depth        & {None, 3, 6}                      \\
		                 & \# estimators & {50, 100, 200}                         \\
		\toprule
		Oversampler      &                      &                                   \\
		\midrule
		K-SMOTE          & \# neighbors  & {3, 5}                            \\
		                 & \# clusters (as \% of number of observations)   & {1$^*$, 0.1, 0.3, 0.5, 0.7, 0.9}      \\
                         & Exponent of mean distance & {auto, 2, 5, 7}       \\
                         & IR threshold  & {auto, 0.5, 0.75, 1.0}            \\
		SMOTE            & \# neighbors  & {3, 5}                            \\
		BORDERLINE SMOTE & \# neighbors  & {3, 5}                            \\
		\bottomrule
	\end{tabular}
    \caption{\label{tab:grid}Hyper-parameters grid. $^*$~One cluster is generated in total, a corner
        case that mimics the behavior of SMOTE}
\end{table}

\subsection{Software Implementation}~\label{sec:implementation}

The experiment was implemented using the Python programming language, using the
\href{https://scikit-learn.org/stable/}{Scikit-Learn}~\cite{Pedregosa2011},
\href{https://imbalanced-learn.org/en/stable/}{Imbalanced-Learn}~\cite{JMLR:v18:16-365},
\href{https://geometric-smote.readthedocs.io/en/latest/?badge=latest}{Geometric-SMOTE},
\href{https://cluster-over-sampling.readthedocs.io/en/latest/?badge=latest}{Cluster-Over-Sampling}
and \href{https://research-learn.readthedocs.io/en/latest/?badge=latest}{Research-Learn} libraries.
All functions, algorithms, experiments and results are provided at the
\href{https://github.com/joaopfonseca/publications/tree/master/remote-sensing-kmeans-smote}{GitHub
repository of the project}.

\section{Results}~\label{sec:results}
When evaluating the performance of an algorithm across multiple datasets, it is
generally recommended to avoid direct score comparisons and use classification
rankings instead~\cite{demvsar2006}. This is done by assigning a ranking to
oversamplers based on the different combinations of classifier, metric and
dataset used. These rankings are also used for the statistical analyses
presented in Section~\ref{sec:statistical_analysis}.

The rank values are assigned based on the mean validation scores resulting from
the experiment described in Section~\ref{sec:methodology}. The averaged ranking
results are computed over 3 different initialization seeds and a 5 fold cross
validation scheme, returning a float value within the interval $[1,5]$. The mean
rankings are presented in Table~\ref{tab:mean_sem_ranking} and
Figure~\ref{fig:mean_rankings_bar_chart}.

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\linewidth]{../analysis/mean_rankings_bar_chart}
	\caption{Mean ranking of oversamplers across datasets.
    }\label{fig:mean_rankings_bar_chart}
\end{figure}

The mean ranking results show that K-SMOTE consistently presents the best
results for every classifier and performance metric used. This is visually
depicted in Figure~\ref{fig:mean_rankings_bar_chart}. The quantitative results
of this analysis is presented in Table~\ref{tab:mean_sem_ranking}. In addition
to its better performance, in most cases K-SMOTE's mean ranking has a lower
standard deviation than any of the remaining methods, and particularly when
opposed to SMOTE (the best performing benchmark method).

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable}, col sep=comma, header=true,
	columns={Classifier,Metric,NONE,ROS,SMOTE,B-SMOTE,K-SMOTE}, string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:mean_sem_ranking}
				Results for mean ranking of oversamplers across datasets.}}
]{../analysis/mean_sem_ranking.csv}

The mean percentage difference among K-SMOTE and SMOTE is presented in
Figure~\ref{fig:mean_score_improvement_heatmap}. It is calculated as the score
difference among the test (K-SMOTE) and control (SMOTE) oversampler, divided by
the control oversampler's score. K-SMOTE's average performance improves
classification performance of up to $1.9\%$ and outperforms all other methods,
with the exception of two situations when using the G-mean evaluation metric.

\begin{figure}[H]
	\centering
	\includegraphics[height=.4\linewidth]{../analysis/mean_score_improvement_heatmap}
    \caption{Mean score improvement (percentage difference) of the proposed method versus
        SMOTE across datasets.
        }\label{fig:mean_score_improvement_heatmap}
\end{figure}

The mean cross-validation scores are shown in Table~\ref{tab:mean_sem_scores}.
Considering the disparity of performance scores across datasets, the results
presented in this table may not be as informative as the scores for each
dataset, presented in Table~\ref{tab:cross_validation_scores}. K-SMOTE's
performance is the highest in most classifier/metric combinations and datasets,
showing more inconsistency on the Indian Pines and Kennedy Space Center
datasets.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=comma, header=true,
	columns={Classifier,Metric,NONE,ROS,SMOTE,B-SMOTE,K-SMOTE}, 
    string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:mean_sem_scores}
				Mean cross-validation scores of oversamplers.}}
]{../analysis/mean_sem_scores.csv}

The performance of both oversamplers and classifiers is generally dependent on
the dataset being used. Although both absolute and relative scores between the
different oversamplers are dependent on the choice of metric and classifier,
K-SMOTE's relative performance is consistent across datasets and generally
outperforms the remaining oversampling methods.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},
	col sep=comma,
	header=true,
	columns={Dataset, Classifier,Metric,NONE,ROS,SMOTE,B-SMOTE,K-SMOTE}, 
    string type,
	every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule
			\caption{\label{tab:cross_validation_scores}Mean cross-validation scores of oversamplers
        for each dataset. Legend: IP \- Indian Pines, KSC \- Kennedy Space Center, PC \- Pavia Center,
        PU \- Pavia University, SA \- Salinas A.}}
]
{../analysis/wide_optimal.csv}

\subsection{Statistical Analysis}~\label{sec:statistical_analysis}

The experiment's multi-dataset context was used to perform both a Friedman
test~\cite{friedman1937use}. Table~\ref{tab:friedman_test} shows the results
obtained in the Friedman test performed, where the null hypothesis is rejected
in all cases. Consequently, the Holm-Bonferroni comparison method (Holm's
method)~\cite{holm1979simple} is used for post-hoc analysis.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=comma, header=true,
	columns={Classifier,Metric,p-value,Significance}, string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:friedman_test}
				Results for Friedman test. Statistical significance is tested at a level of
				$\alpha = 0.05$. The null hypothesis is that there is no difference in the
                classification outcome across oversamplers.}}
]{../analysis/friedman_test.csv}

The results of the Holm's method are shown in Table~\ref{tab:holms_test}. Even though K-SMOTE
outperforms the remaining oversamplers, the datasets' inherent high prediction scores make the
rejection of this null hypothesis particularly difficult.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=comma, header=true,
    columns={Classifier,Metric,NONE,ROS,SMOTE,B-SMOTE}, 
    string type, 
    every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule 
        \caption{\label{tab:holms_test} Adjusted p-values using the Holm's method. Bold values are
            statistically significant at a level of $\alpha=0.05$. The null hypothesis is that the
    test method does not perform better than the control method.}}
]{../analysis/holms_test.csv}

\section{Conclusion}~\label{sec:conclusion} 
This research paper was motivated by the difficulty posed in classifying rare
classes in Land Use/Land Cover tasks. A number of existing methods to address
this problem (known as imbalanced learning) was identified and their caveats
were exposed.  Typically, these methods are not only difficult to implement,
they are also context dependent. We focused on oversampling methods due to their
widespread usage, easy implementation and flexibility.  Specifically, this paper
demonstrated the efficacy of a recent oversampler, K-Means SMOTE, applied in a
multi-class context for Land Cover Classification tasks. This was done with
sampled data from seven well known and naturally imbalanced datasets: Indian
Pines, Pavia Centre, Pavia University, Salinas, Salinas A, Botswana and Kennedy
Space Center. The experiment comprised a hyper-parameter search in order to tune
each algorithm to its specific use case. For each combination of dataset,
oversampler and classifier, the results of every classification task was
averaged across a 5 fold stratification strategy with 3 different initialization
seeds, resulting in a mean validation score of 15 classification tasks.  The
optimal mean validation score of each combination was then used to perform the
analyses presented in this report.

In most cases, classification tasks using K-SMOTE led to better results than
using the original, unmodified, imbalanced data. More importantly, we found that
K-Means SMOTE is always better or equal than the second best oversampling
method. K-SMOTE's performance was independent from both the classifier and
performance metric under analysis. In general, K-SMOTE shows a higher
performance among the non tree-based classifiers employed, when compared to the
remaining oversamplers. Although these findings are case dependent, they are
consistent with the results presented in~\cite{Douzas2018}. The proposed method
also had the most consistent results across datasets, since it had the lowest
standard deviations across datasets in most cases for both analyses, either
based on ranking or mean cross-validation scores.

The proposed algorithm is an extension of the original SMOTE algorithm. In fact,
the SMOTE algorithm represents a corner case of K-SMOTE i.e. when the number of
clusters equals to 1. Its data selection phase differs from the one used in
SMOTE and Borderline SMOTE, providing artificially augmented datasets with less
noisy data than the commonly used methods. This allows the training of
classifiers with better defined decision boundaries, especially in the most
important regions of the data space (the ones populated by a higher percentage
of minority class instances).

As stated previously, the usage of this oversampler is technically simple. It
can be applied to any classification problem relying on an imbalanced dataset,
alongside any classifier. K-SMOTE is available as an open source implementation
for the Python programming language (see Subsection~\ref{sec:implementation}).
Consequently, it can be a useful tool for both remote sensing researchers and
practitioners.

\bibliography{references}
\bibliographystyle{apalike}

\end{document}
