
@article{kabbani_deep_2022,
	title = {Deep {Reinforcement} {Learning} {Approach} for {Trading} {Automation} in the {Stock} {Market}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9877940/},
	doi = {10.1109/ACCESS.2022.3203697},
	urldate = {2023-02-28},
	journal = {IEEE Access},
	author = {Kabbani, Taylan and Duman, Ekrem},
	year = {2022},
	pages = {93564--93574},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/FU97F89E/Kabbani and Duman - 2022 - Deep Reinforcement Learning Approach for Trading A.pdf:application/pdf},
}

@article{yang_deep_2020,
	title = {Deep {Reinforcement} {Learning} for {Automated} {Stock} {Trading}: {An} {Ensemble} {Strategy}},
	issn = {1556-5068},
	shorttitle = {Deep {Reinforcement} {Learning} for {Automated} {Stock} {Trading}},
	url = {https://www.ssrn.com/abstract=3690996},
	doi = {10.2139/ssrn.3690996},
	language = {en},
	urldate = {2023-02-28},
	journal = {SSRN Electronic Journal},
	author = {Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
	year = {2020},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/3V5VAR6K/Yang et al. - 2020 - Deep Reinforcement Learning for Automated Stock Tr.pdf:application/pdf},
}

@article{gort_deep_2022,
	title = {Deep {Reinforcement} {Learning} for {Cryptocurrency} {Trading}: {Practical} {Approach} to {Address} {Backtest} {Overfitting}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Reinforcement} {Learning} for {Cryptocurrency} {Trading}},
	url = {https://arxiv.org/abs/2209.05559},
	doi = {10.48550/ARXIV.2209.05559},
	abstract = {Designing profitable and reliable trading strategies is challenging in the highly volatile cryptocurrency market. Existing works applied deep reinforcement learning methods and optimistically reported increased profits in backtesting, which may suffer from the false positive issue due to overfitting. In this paper, we propose a practical approach to address backtest overfitting for cryptocurrency trading using deep reinforcement learning. First, we formulate the detection of backtest overfitting as a hypothesis test. Then, we train the DRL agents, estimate the probability of overfitting, and reject the overfitted agents, increasing the chance of good trading performance. Finally, on 10 cryptocurrencies over a testing period from 05/01/2022 to 06/27/2022 (during which the crypto market crashed two times), we show that the less overfitted deep reinforcement learning agents have a higher return than that of more overfitted agents, an equal weight strategy, and the S\&amp;P DBM Index (market benchmark), offering confidence in possible deployment to a real market.},
	urldate = {2023-02-28},
	author = {Gort, Berend Jelmer Dirk and Liu, Xiao-Yang and Sun, Xinghang and Gao, Jiechao and Chen, Shuaiyu and Wang, Christina Dan},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 6},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), 68T07,, I.2.6, Artificial Intelligence (cs.AI), FOS: Economics and business, Statistical Finance (q-fin.ST)},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/XSZV7IAP/Gort et al. - 2022 - Deep Reinforcement Learning for Cryptocurrency Tra.pdf:application/pdf},
}

@article{qiu_qf-tradernet_2021,
	title = {{QF}-{TraderNet}: {Intraday} {Trading} via {Deep} {Reinforcement} {With} {Quantum} {Price} {Levels} {Based} {Profit}-{And}-{Loss} {Control}},
	volume = {4},
	issn = {2624-8212},
	shorttitle = {{QF}-{TraderNet}},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.749878/full},
	doi = {10.3389/frai.2021.749878},
	abstract = {Reinforcement Learning (RL) based machine trading attracts a rich profusion of interest. However, in the existing research, RL in the day-trade task suffers from the noisy financial movement in the short time scale, difficulty in order settlement, and expensive action search in a continuous-value space. This paper introduced an end-to-end RL intraday trading agent, namely QF-TraderNet, based on the quantum finance theory (QFT) and deep reinforcement learning. We proposed a novel design for the intraday RL traderâ€™s action space, inspired by the Quantum Price Levels (QPLs). Our action space design also brings the model a learnable profit-and-loss control strategy. QF-TraderNet composes two neural networks: 1) A long short term memory networks for the feature learning of financial time series; 2) a policy generator network (PGN) for generating the distribution of actions. The profitability and robustness of QF-TraderNet have been verified in multi-type financial datasets, including FOREX, metals, crude oil, and financial indices. The experimental results demonstrate that QF-TraderNet outperforms other baselines in terms of cumulative price returns and Sharpe Ratio, and the robustness in the acceidential market shift.},
	urldate = {2023-02-28},
	journal = {Frontiers in Artificial Intelligence},
	author = {Qiu, Yifu and Qiu, Yitao and Yuan, Yicong and Chen, Zheng and Lee, Raymond},
	month = oct,
	year = {2021},
	pages = {749878},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/3EAXBCXI/Qiu et al. - 2021 - QF-TraderNet Intraday Trading via Deep Reinforcem.pdf:application/pdf},
}

@article{seno_d3rlpy_2021,
	title = {d3rlpy: {An} {Offline} {Deep} {Reinforcement} {Learning} {Library}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {d3rlpy},
	url = {https://arxiv.org/abs/2111.03788},
	doi = {10.48550/ARXIV.2111.03788},
	abstract = {In this paper, we introduce d3rlpy, an open-sourced offline deep reinforcement learning (RL) library for Python. d3rlpy supports a set of offline deep RL algorithms as well as off-policy online algorithms via a fully documented plug-and-play API. To address a reproducibility issue, we conduct a large-scale benchmark with D4RL and Atari 2600 dataset to ensure implementation quality and provide experimental scripts and full tables of results. The d3rlpy source code can be found on GitHub: {\textbackslash}url\{https://github.com/takuseno/d3rlpy\}.},
	urldate = {2023-03-01},
	author = {Seno, Takuma and Imai, Michita},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI)},
}

@article{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/2005.01643},
	doi = {10.48550/ARXIV.2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2023-03-01},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML)},
}

@article{patel_predicting_2015,
	title = {Predicting stock and stock price index movement using {Trend} {Deterministic} {Data} {Preparation} and machine learning techniques},
	volume = {42},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417414004473},
	doi = {10.1016/j.eswa.2014.07.040},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {Expert Systems with Applications},
	author = {Patel, Jigar and Shah, Sahil and Thakkar, Priyank and Kotecha, K},
	month = jan,
	year = {2015},
	pages = {259--268},
}

@inproceedings{tsantekidis_forecasting_2017,
	address = {Thessaloniki, Greece},
	title = {Forecasting {Stock} {Prices} from the {Limit} {Order} {Book} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-3035-8},
	url = {http://ieeexplore.ieee.org/document/8010701/},
	doi = {10.1109/CBI.2017.23},
	urldate = {2023-03-06},
	booktitle = {2017 {IEEE} 19th {Conference} on {Business} {Informatics} ({CBI})},
	publisher = {IEEE},
	author = {Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
	month = jul,
	year = {2017},
	pages = {7--12},
}

@article{ntakaris_mid-price_2020,
	title = {Mid-price prediction based on machine learning methods with technical and quantitative indicators},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0234107},
	doi = {10.1371/journal.pone.0234107},
	language = {en},
	number = {6},
	urldate = {2023-03-06},
	journal = {PLOS ONE},
	author = {Ntakaris, Adamantios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
	editor = {Hernandez Montoya, Alejandro Raul},
	month = jun,
	year = {2020},
	pages = {e0234107},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/M69NHIJK/Ntakaris et al. - 2020 - Mid-price prediction based on machine learning met.pdf:application/pdf},
}

@article{hao_predicting_2020,
	title = {Predicting the {Trend} of {Stock} {Market} {Index} {Using} the {Hybrid} {Neural} {Network} {Based} on {Multiple} {Time} {Scale} {Feature} {Learning}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/11/3961},
	doi = {10.3390/app10113961},
	abstract = {In the stock market, predicting the trend of price series is one of the most widely investigated and challenging problems for investors and researchers. There are multiple time scale features in financial time series due to different durations of impact factors and tradersâ€™ trading behaviors. In this paper, we propose a novel end-to-end hybrid neural network, a model based on multiple time scale feature learning to predict the price trend of the stock market index. Firstly, the hybrid neural network extracts two types of features on different time scales through the first and second layers of the convolutional neural network (CNN), together with the raw daily price series, reflect relatively short-, medium- and long-term features in the price sequence. Secondly, considering time dependencies existing in the three kinds of features, the proposed hybrid neural network leverages three long short-term memory (LSTM) recurrent neural networks to capture such dependencies, respectively. Finally, fully connected layers are used to learn joint representations for predicting the price trend. The proposed hybrid neural network demonstrates its effectiveness by outperforming benchmark models on the real dataset.},
	language = {en},
	number = {11},
	urldate = {2023-03-06},
	journal = {Applied Sciences},
	author = {Hao, Yaping and Gao, Qiang},
	month = jun,
	year = {2020},
	pages = {3961},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/3ZLWXF24/Hao and Gao - 2020 - Predicting the Trend of Stock Market Index Using t.pdf:application/pdf},
}

@article{lopez_de_prado_10_2018,
	title = {The 10 {Reasons} {Most} {Machine} {Learning} {Funds} {Fail}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3104816},
	doi = {10.2139/ssrn.3104816},
	language = {en},
	urldate = {2023-03-06},
	journal = {SSRN Electronic Journal},
	author = {Lopez de Prado, Marcos},
	year = {2018},
}

@article{meng_reinforcement_2019,
	title = {Reinforcement {Learning} in {Financial} {Markets}},
	volume = {4},
	issn = {2306-5729},
	url = {https://www.mdpi.com/2306-5729/4/3/110},
	doi = {10.3390/data4030110},
	abstract = {Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {Data},
	author = {Meng, Terry Lingze and Khushi, Matloob},
	month = jul,
	year = {2019},
	pages = {110},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/8UQYXHKQ/Meng and Khushi - 2019 - Reinforcement Learning in Financial Markets.pdf:application/pdf},
}

@book{puterman_markov_1994,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	isbn = {978-0-471-61977-2 978-0-470-31688-7},
	shorttitle = {Markov {Decision} {Processes}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
	language = {en},
	urldate = {2023-03-06},
	publisher = {Wiley},
	author = {Puterman, Martin L.},
	month = apr,
	year = {1994},
	doi = {10.1002/9780470316887},
}

@article{chakraborty_capturing_2019,
	title = {Capturing {Financial} markets to apply {Deep} {Reinforcement} {Learning}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/1907.04373},
	doi = {10.48550/ARXIV.1907.04373},
	abstract = {In this paper we explore the usage of deep reinforcement learning algorithms to automatically generate consistently profitable, robust, uncorrelated trading signals in any general financial market. In order to do this, we present a novel Markov decision process (MDP) model to capture the financial trading markets. We review and propose various modifications to existing approaches and explore different techniques like the usage of technical indicators, to succinctly capture the market dynamics to model the markets. We then go on to use deep reinforcement learning to enable the agent (the algorithm) to learn how to take profitable trades in any market on its own, while suggesting various methodology changes and leveraging the unique representation of the FMDP (financial MDP) to tackle the primary challenges faced in similar works. Through our experimentation results, we go on to show that our model could be easily extended to two very different financial markets and generates a positively robust performance in all conducted experiments.},
	urldate = {2023-03-06},
	author = {Chakraborty, Souradeep},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), FOS: Economics and business, Computational Finance (q-fin.CP)},
}

@inproceedings{vargas_deep_2018,
	address = {Rio de Janeiro},
	title = {Deep {Leaming} for {Stock} {Market} {Prediction} {Using} {Technical} {Indicators} and {Financial} {News} {Articles}},
	isbn = {978-1-5090-6014-6},
	url = {https://ieeexplore.ieee.org/document/8489208/},
	doi = {10.1109/IJCNN.2018.8489208},
	urldate = {2023-03-06},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Vargas, Manuel R. and dos Anjos, Carlos E. M. and Bichara, Gustavo L. G. and Evsukoff, Alexandre G.},
	month = jul,
	year = {2018},
	pages = {1--8},
}

@article{corazza_q-learning-based_2014,
	title = {Q-{Learning}-{Based} {Financial} {Trading} {Systems} with {Applications}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2507826},
	doi = {10.2139/ssrn.2507826},
	language = {en},
	urldate = {2023-03-06},
	journal = {SSRN Electronic Journal},
	author = {Corazza, Marco and Bertoluzzo, Francesco},
	year = {2014},
	file = {Submitted Version:/Users/georgiosdouzas/Zotero/storage/ZMR2QKX4/Corazza and Bertoluzzo - 2014 - Q-Learning-Based Financial Trading Systems with Ap.pdf:application/pdf},
}

@article{tan_stock_2011,
	title = {Stock trading with cycles: {A} financial application of {ANFIS} and reinforcement learning},
	volume = {38},
	issn = {09574174},
	shorttitle = {Stock trading with cycles},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741741000905X},
	doi = {10.1016/j.eswa.2010.09.001},
	language = {en},
	number = {5},
	urldate = {2023-03-06},
	journal = {Expert Systems with Applications},
	author = {Tan, Zhiyong and Quek, Chai and Cheng, Philip Y.K.},
	month = may,
	year = {2011},
	pages = {4741--4755},
}

@article{deng_deep_2017,
	title = {Deep {Direct} {Reinforcement} {Learning} for {Financial} {Signal} {Representation} and {Trading}},
	volume = {28},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/7407387/},
	doi = {10.1109/TNNLS.2016.2522401},
	number = {3},
	urldate = {2023-03-06},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
	month = mar,
	year = {2017},
	pages = {653--664},
}

@article{alagoz_markov_2010,
	title = {Markov {Decision} {Processes}: {A} {Tool} for {Sequential} {Decision} {Making} under {Uncertainty}},
	volume = {30},
	issn = {0272-989X, 1552-681X},
	shorttitle = {Markov {Decision} {Processes}},
	url = {http://journals.sagepub.com/doi/10.1177/0272989X09353194},
	doi = {10.1177/0272989X09353194},
	abstract = {We provide a tutorial on the construction and evaluation of Markov decision processes (MDPs), which are powerful analytical tools used for sequential decision making under uncertainty that have been widely used in many industrial and manufacturing applications but are underutilized in medical decision making (MDM). We demonstrate the use of an MDP to solve a sequential clinical treatment problem under uncertainty. Markov decision processes generalize standard Markov models in that a decision process is embedded in the model and multiple decisions are made over time. Furthermore, they have significant advantages over standard decision analysis. We compare MDPs to standard Markov-based simulation models by solving the problem of the optimal timing of living-donor liver transplantation using both methods. Both models result in the same optimal transplantation policy and the same total life expectancies for the same patient and living donor. The computation time for solving the MDP model is significantly smaller than that for solving the Markov model. We briefly describe the growing literature of MDPs applied to medical decisions.},
	language = {en},
	number = {4},
	urldate = {2023-03-06},
	journal = {Medical Decision Making},
	author = {Alagoz, Oguzhan and Hsu, Heather and Schaefer, Andrew J. and Roberts, Mark S.},
	month = jul,
	year = {2010},
	pages = {474--483},
	file = {Accepted Version:/Users/georgiosdouzas/Zotero/storage/S646QC92/Alagoz et al. - 2010 - Markov Decision Processes A Tool for Sequential D.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@book{bellman_dynamic_2010,
	address = {Princeton, NJ},
	edition = {1. Princeton Landmarks in Mathematics ed., with a new introduction},
	series = {Princeton {Landmarks} in mathematics},
	title = {Dynamic programming},
	isbn = {978-0-691-14668-3},
	language = {eng},
	publisher = {Princeton University Press},
	author = {Bellman, Richard and Dreyfus, Stuart},
	year = {2010},
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1312.5602},
	doi = {10.48550/ARXIV.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2023-03-06},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2023-03-06},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1509.06461},
	doi = {10.48550/ARXIV.1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2023-03-06},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1509.02971},
	doi = {10.48550/ARXIV.1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2023-03-06},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 6},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{fujimoto_addressing_2018,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1802.09477},
	doi = {10.48550/ARXIV.1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
	urldate = {2023-03-06},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML)},
}

@article{bertoluzzo_testing_2012,
	title = {Testing {Different} {Reinforcement} {Learning} {Configurations} for {Financial} {Trading}: {Introduction} and {Applications}},
	volume = {3},
	issn = {22125671},
	shorttitle = {Testing {Different} {Reinforcement} {Learning} {Configurations} for {Financial} {Trading}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212567112001220},
	doi = {10.1016/S2212-5671(12)00122-0},
	language = {en},
	urldate = {2023-03-06},
	journal = {Procedia Economics and Finance},
	author = {Bertoluzzo, Francesco and Corazza, Marco},
	year = {2012},
	pages = {68--77},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/AY58U6PU/Bertoluzzo and Corazza - 2012 - Testing Different Reinforcement Learning Configura.pdf:application/pdf},
}

@inproceedings{conegundes_beating_2020,
	address = {Glasgow, United Kingdom},
	title = {Beating the {Stock} {Market} with a {Deep} {Reinforcement} {Learning} {Day} {Trading} {System}},
	isbn = {978-1-72816-926-2},
	url = {https://ieeexplore.ieee.org/document/9206938/},
	doi = {10.1109/IJCNN48605.2020.9206938},
	urldate = {2023-03-06},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Conegundes, Leonardo and Pereira, Adriano C. Machado},
	month = jul,
	year = {2020},
	pages = {1--8},
}

@book{kirkpatrick_technical_2011,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	title = {Technical analysis: the complete resource for financial market technicians},
	isbn = {978-0-13-705944-7},
	shorttitle = {Technical analysis},
	publisher = {FT Press},
	author = {Kirkpatrick, Charles D. and Dahlquist, Julie R.},
	year = {2011},
	keywords = {Investment analysis, Technical analysis (Investment analysis)},
}

@article{mitchell_impact_1994,
	title = {The {Impact} of {Public} {Information} on the {Stock} {Market}},
	volume = {49},
	issn = {00221082},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1994.tb00083.x},
	doi = {10.1111/j.1540-6261.1994.tb00083.x},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {The Journal of Finance},
	author = {Mitchell, Mark L. and Mulherin, J. Harold},
	month = jul,
	year = {1994},
	pages = {923--950},
}

@article{liu_practical_2018,
	title = {Practical {Deep} {Reinforcement} {Learning} {Approach} for {Stock} {Trading}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1811.07522},
	doi = {10.48550/ARXIV.1811.07522},
	abstract = {Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
	urldate = {2023-03-06},
	author = {Liu, Xiao-Yang and Xiong, Zhuoran and Zhong, Shan and Yang, Hongyang and Walid, Anwar},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), FOS: Economics and business, Machine Learning (stat.ML), Trading and Market Microstructure (q-fin.TR)},
}

@article{jiang_deep_2017,
	title = {A {Deep} {Reinforcement} {Learning} {Framework} for the {Financial} {Portfolio} {Management} {Problem}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.10059},
	doi = {10.48550/ARXIV.1706.10059},
	abstract = {Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25\% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.},
	urldate = {2023-03-06},
	author = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), FOS: Economics and business, Computational Finance (q-fin.CP), Portfolio Management (q-fin.PM)},
}

@incollection{ahram_continuous_2020,
	address = {Cham},
	title = {Continuous {Control} in {Deep} {Reinforcement} {Learning} with {Direct} {Policy} {Derivation} from {Q} {Network}},
	volume = {1152},
	isbn = {978-3-030-44266-8 978-3-030-44267-5},
	url = {http://link.springer.com/10.1007/978-3-030-44267-5_25},
	language = {en},
	urldate = {2023-03-06},
	booktitle = {Human {Interaction}, {Emerging} {Technologies} and {Future} {Applications} {II}},
	publisher = {Springer International Publishing},
	author = {Akhmetzyanov, Aydar and Yagfarov, Rauf and Gafurov, Salimzhan and Ostanin, Mikhail and Klimchik, Alexandr},
	editor = {Ahram, Tareq and Taiar, Redha and Gremeaux-Bader, Vincent and Aminian, Kamiar},
	year = {2020},
	doi = {10.1007/978-3-030-44267-5_25},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	pages = {168--174},
}

@article{chong_revisiting_2014,
	title = {Revisiting the {Performance} of {MACD} and {RSI} {Oscillators}},
	volume = {7},
	issn = {1911-8074},
	url = {http://www.mdpi.com/1911-8074/7/1/1},
	doi = {10.3390/jrfm7010001},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {Journal of Risk and Financial Management},
	author = {Chong, Terence and Ng, Wing-Kam and Liew, Venus},
	month = feb,
	year = {2014},
	pages = {1--12},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/WSX6NX88/Chong et al. - 2014 - Revisiting the Performance of MACD and RSI Oscilla.pdf:application/pdf},
}

@inproceedings{ding_using_2014,
	address = {Doha, Qatar},
	title = {Using {Structured} {Events} to {Predict} {Stock} {Price} {Movement}: {An} {Empirical} {Investigation}},
	shorttitle = {Using {Structured} {Events} to {Predict} {Stock} {Price} {Movement}},
	url = {http://aclweb.org/anthology/D14-1148},
	doi = {10.3115/v1/D14-1148},
	language = {en},
	urldate = {2023-03-06},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	year = {2014},
	pages = {1415--1425},
	file = {Submitted Version:/Users/georgiosdouzas/Zotero/storage/I5LRWKPR/Ding et al. - 2014 - Using Structured Events to Predict Stock Price Mov.pdf:application/pdf},
}

@article{henderson_deep_2017,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1709.06560},
	doi = {10.48550/ARXIV.1709.06560},
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	urldate = {2023-03-06},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{sharpe_sharpe_1994,
	title = {The {Sharpe} {Ratio}},
	volume = {21},
	issn = {0095-4918, 2168-8656},
	url = {http://jpm.pm-research.com/lookup/doi/10.3905/jpm.1994.409501},
	doi = {10.3905/jpm.1994.409501},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {The Journal of Portfolio Management},
	author = {Sharpe, William F.},
	month = oct,
	year = {1994},
	pages = {49--58},
}

@article{moody_learning_2001,
	title = {Learning to trade via direct reinforcement},
	volume = {12},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/935097/},
	doi = {10.1109/72.935097},
	number = {4},
	urldate = {2023-03-06},
	journal = {IEEE Transactions on Neural Networks},
	author = {Moody, J. and Saffell, M.},
	month = jul,
	year = {2001},
	pages = {875--889},
}

@article{dempster_automated_2006,
	title = {An automated {FX} trading system using adaptive reinforcement learning},
	volume = {30},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417405003015},
	doi = {10.1016/j.eswa.2005.10.012},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {Expert Systems with Applications},
	author = {Dempster, M.A.H. and Leemans, V.},
	month = apr,
	year = {2006},
	pages = {543--552},
}

@article{giudici_network_2020,
	title = {Network {Models} to {Enhance} {Automated} {Cryptocurrency} {Portfolio} {Management}},
	volume = {3},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/article/10.3389/frai.2020.00022/full},
	doi = {10.3389/frai.2020.00022},
	urldate = {2023-03-06},
	journal = {Frontiers in Artificial Intelligence},
	author = {Giudici, Paolo and Pagnottoni, Paolo and Polinesi, Gloria},
	month = apr,
	year = {2020},
	pages = {22},
	file = {Full Text:/Users/georgiosdouzas/Zotero/storage/DFJJSPBU/Giudici et al. - 2020 - Network Models to Enhance Automated Cryptocurrency.pdf:application/pdf},
}

@incollection{hutchison_maximus-ai_2010,
	address = {Berlin, Heidelberg},
	title = {Maximus-{AI}: {Using} {Elman} {Neural} {Networks} for {Implementing} a {SLMR} {Trading} {Strategy}},
	volume = {6291},
	isbn = {978-3-642-15279-5 978-3-642-15280-1},
	shorttitle = {Maximus-{AI}},
	url = {http://link.springer.com/10.1007/978-3-642-15280-1_55},
	urldate = {2023-03-06},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Marques, Nuno C. and Gomes, Carlos},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bi, Yaxin and Williams, Mary-Anne},
	year = {2010},
	doi = {10.1007/978-3-642-15280-1_55},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {579--584},
}

@article{vella_dynamic_2015,
	title = {A {Dynamic} {Fuzzy} {Money} {Management} {Approach} for {Controlling} the {Intraday} {Risk}-{Adjusted} {Performance} of {AI} {Trading} {Algorithms}: {DYNAMIC} {FUZZY} {MONEY} {MANAGEMENT} {APPROACH} {FOR} {AI} {TRADING} {ALGORITHMS}},
	volume = {22},
	issn = {1055615X},
	shorttitle = {A {Dynamic} {Fuzzy} {Money} {Management} {Approach} for {Controlling} the {Intraday} {Risk}-{Adjusted} {Performance} of {AI} {Trading} {Algorithms}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/isaf.1359},
	doi = {10.1002/isaf.1359},
	language = {en},
	number = {2},
	urldate = {2023-03-06},
	journal = {Intelligent Systems in Accounting, Finance and Management},
	author = {Vella, Vince and Ng, Wing Lon},
	month = apr,
	year = {2015},
	pages = {153--178},
}

@article{chen_trading_2021,
	title = {Trading strategy of structured mutual fund based on deep learning network},
	volume = {183},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421008149},
	doi = {10.1016/j.eswa.2021.115390},
	language = {en},
	urldate = {2023-03-06},
	journal = {Expert Systems with Applications},
	author = {Chen, Jiao and Luo, Changqing and Pan, Lurun and Jia, Yun},
	month = nov,
	year = {2021},
	pages = {115390},
}

@article{huang_robust_2016,
	title = {Robust {Median} {Reversion} {Strategy} for {Online} {Portfolio} {Selection}},
	volume = {28},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/7465840/},
	doi = {10.1109/TKDE.2016.2563433},
	number = {9},
	urldate = {2023-03-06},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Huang, Ding-jiang and Zhou, Junlong and Li, Bin and Hoi, Steven C. H. and Zhou, Shuigeng},
	month = sep,
	year = {2016},
	pages = {2480--2493},
}
